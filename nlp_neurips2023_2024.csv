"payload","id"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""Authors claimed that their work suggests potential improvements from using Neural Comprehension in other tasks, but they did not mention how to get CoNNs for these tasks? In general, the discussion about CoNN design and implementation is somewhat skipped in the paper while it seem to be a crucial factor in this paper's impact."", ""Target claims"": [6], ""Reasoning"": ""This span questions the implementation details and scalability of the Neural Comprehension framework, as it highlights the lack of discussion on designing and implementing CoNNs for other tasks. Claim 6 asserts the potential to transform symbolic reasoning and language understanding capabilities, which naturally requires effective CoNN design and implementation. Thus, the unspecified nature of obtaining CoNNs directly targets the truthfulness of Claim 6 by challenging the assertion of broad impact without concrete evidence of how this crucial aspect is addressed.""}, {""Weakness span"": ""In general I like this idea of plug-in CoNNs, but they need to be more seamless and fluent without requiring such hard coding in the task level."", ""Target claims"": [11, 20], ""Reasoning"": ""The reviewer argues for the lack of seamlessness and fluidity in integrating CoNNs, pointing out the hard coding required at the task level. Claim 11 discusses CoNNs providing optimal gradients for rule-based tasks, and Claim 20 mentions that LMs with CoNNs require fewer computational steps. The reviewer's critique directly questions these claims by emphasizing the lack of seamless integration and suggesting that the current implementation might be overly rigid or task-specific, which contradicts the claims of fluid adaptability and efficiency.""}]}, ""meta"": {""id"": ""tpIUgkq0xa"", ""review_idx"": 0, ""title"": ""Title: 2020 Egyptian protests\nAbstract: The 2020 Egyptian protests were a series of decentralized street protests in Egypt that started on September 20, the anniversary of the 2019 Egyptian protests, and were aimed at the resignation of Egyptian president Abdel Fattah el-Sisi"", ""claims"": [""Claim1: Our experiments on symbolic reasoning tasks and real-world arithmetic reasoning tasks demonstrate the superior performance of our method compared to existing techniques."", ""Claim2: Furthermore, our LM achieves flawless execution on symbolic operations tasks, highlighting the potential of our method in enabling LMs to possess true symbolic comprehension capabilities."", ""Claim3: The implementation of the Neural Comprehension framework facilitates the integration of rule-intensive abilities and reasoning capabilities into LMs endowing them with genuine symbolic comprehension skills."", ""Claim4: Our experimental results demonstrate the effectiveness of our approach in comparison with existing state-of-the-art techniques, such as vanilla fine-tuning, few-shot learning, and Chain-of-Thought reasoning."", ""Claim5: Specifically, Neural Comprehension outperforms these methods in terms of accuracy showcasing its superiority in handling rule-intensive tasks."", ""Claim6: Our study presents a strong case for the deployment of Neural Comprehension in language models highlighting its potential to transform the landscape of symbolic reasoning and language understanding capabilities."", ""Claim7: For example, when calculating 364425-216582 , the pre-trained language model output 148843 , which is incorrect. However, the Subtraction CoNN can correct the result to 147843 in the neural comprehension framework."", ""Claim8: This process encoded into context dynamically, improving intermediate results interpretability and final result accuracy."", ""Claim9: To better appreciate the benefits of our method in handling rule-intensive tasks and improving accuracy, it is crucial to understand the gradient perspective of ICL."", ""Claim10: Therefore, our proposed method introduces an explicit learning component to provide more appropriate gradient updates for such tasks, ultimately leading to enhanced overall performance."", ""Claim11: It is crucial to note that CoNNs are designed to minimize the loss associated with rule-based tasks, essentially providing an optimal gradient for tasks involving rule-intensive operations."", ""Claim12: This leads to a substantial improvement in the model\u2019s accuracy for rule-based tasks, as the gradient updates provided by CoNNs are more suitable for rule learning compared to the initially available gradients from the LM."", ""Claim13: By amalgamating the both of gradient sources, the Neural Comprehension model achieves a more refined optimization of in-context learning."", ""Claim14: Additionally, from the perspective of gradients, our approach surpasses conventional data-driven implicit learning techniques as it integrates explicit rule-based learning mechanisms that exhibit more suitable gradient updates for rule-intensive questions."", ""Claim15: The Neural Comprehension model effectively balances the need for implicit and explicit learning within the ICL framework, leading to an enhanced overall performance in terms of accuracy and interpretability."", ""Claim16: In this study, we primarily explore the capacity of language models to address symbolic reasoning tasks, concentrating on three areas: symbolic operations, symbolic reasoning, and arithmetic reasoning."", ""Claim17: The results of our experiment demonstrate that the Vanilla Fine-tune (red lines) method performs optimally on the in-domain (10-20 digit) training set, while its performance deteriorates for both more simplistic and more intricate."", ""Claim18: Our study reveals that Neural Comprehension attains remarkably high accuracy in symbolic operations."", ""Claim19: This implies that Neural Comprehension, unlike conventional methods, does not rely on training data and remains unaffected by discrepancies in input lengths for in-distribution and out-of-distribution data."", ""Claim20: Consequently, it alleviates the requirement for step-by-step work tracking, and language models with CoNNs only need relatively fewer computational steps to execute sequence operations directly."", ""Claim21: Encoding rules into neural network modules endows us with greater interpretability, enabling language models to flawlessly perform purely symbolic operation tasks."", ""Claim22: We observed that neural comprehension improves the symbolic reasoning capabilities of pre-trained language models in most cases (Neural Comprehension almost always outperforms Vanilla Fine-tune in Figure 4), and can fit faster."", ""Claim23: This observation suggests that the introduction of compiled neural networks has a positive impact on pretrained LMs, addressing rule comprehension limitations in symbolic reasoning tasks."", ""Claim24: As demonstrated in Figure 5, the vanilla CoT model struggles to extrapolate and solve arithmetic problems involving longer digit lengths. However, integrating Neural Comprehension significantly improves the performance of language models on such complex arithmetic tasks."", ""Claim25: For further evidence, we present additional experimental results on widely-used arithmetic reasoning datasets in Appendix D.2 , which reinforce the benefits of using Neural Comprehension over the vanilla CoT model."", ""Claim26: In comparison to language models employing external tools like PAL [Gao et al., 2022], our findings suggest that generating accurate code for the less code-trained GLM-130B model might be challenging for PAL, resulting in performance levels inferior to those of the vanilla CoT."", ""Claim27: This outcome indicates that language models offer greater flexibility, whereas external tools may have difficulties in more complex or unique situations."", ""Claim28: The integration of compiled neural networks appears to be a more promising approach, as evidenced by the performance improvements observed in our experiments."", ""Claim29: Specifically, when language models encounter intricate arithmetic tasks that involve nested operations or multi-step calculations, the integrated CoNNs can efficiently handle these operations, allowing the language model to focus on higher-level reasoning."", ""Claim30: In contrast, the use of external tools often requires explicit coding and may not generalize effectively to more complicated scenarios."", ""Claim31: In conclusion, our results demonstrate that incorporating compiled neural networks into language models provides a more robust and versatile solution for arithmetic reasoning and related challenges, underlining the superiority of this approach over external tools such as PAL."", ""Claim32: Efficiently deploying multiple CoNNs is crucial for achieving exceptional Neural Comprehension performance."", ""Claim33: It is observed that integrating pertinent CoNNs bolsters the performance of the initial language model, whereas the inclusion of unrelated language models rarely causes detrimental effects, regardless of whether single or multiple CoNNs are combined."", ""Claim34: This can be ascribed to the refined design of the Neural Comprehension framework, which ensures the precise execution of assigned tasks by CoNNs without interference from irrelevant modules."", ""Claim35: Importantly, as seen in Appendix B.3 , the parameter count for each CoNN module ranges from 1/1000 to 1/1000000 of that for GPT-3, and the experiments in Appendix D.3 show that the inference latency in the neural understanding framework only increases by 1%-3% compared to Vanilla."", ""Claim36: This observation underscores the remarkable scalability of the Neural Comprehension framework, which possesses the capability to not only accommodate existing knowledge concepts but also assimilate novel ones as the number of CoNNs expands."", ""Claim37: Theoretically, the integration of tens of thousands of CoNN modules within language models holds the potential to foster a comprehensive understanding of concepts."", ""Claim38: We have observed that pretrained language models lack an intrinsic comprehension of rule-based concepts and explored how Neural Comprehension can integrate compiled neural networks into the language model framework in a simple and generic manner."", ""Claim39: We demonstrated the superiority of our approach over existing learning-based method, Without external tools, our approach enables language models to perform nearly perfect symbolic operations and can be applied to more realistic arithmetic reasoning tasks.""], ""review"": ""Review: Weakness: Sentence from the source text. (1) claim 1, (2) claim 2 (3) claim 3""}, ""review"": ""Title: Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks\nAbstract: Language models (LMs) proficiency in handling deterministic symbolic reasoning and rule-based tasks remains limited due to their dependency implicit learning on textual data. To enable fully rule comprehension ability, we explore how to incorporate compiled neural networks (CoNNs) which weight is specially designed into the architecture of LMs, to achieve high accuracy and robust performance. CoNNs are transformer-based neural networks that execute rules through artificially generated attention weights. Our method, which call \""Neural Comprehension\"", by incorporating CoNN modules into the LM, the framework effectively tackles rule-intensive challenges. Our experiments on symbolic reasoning tasks and real-world arithmetic reasoning tasks demonstrate the superior performance of our method compared to existing techniques. Furthermore, our LM achieves flawless execution on symbolic operations tasks, highlighting the potential of our method in enabling LMs to possess true symbolic comprehension capabilities.\n\nClaim1: Our experiments on symbolic reasoning tasks and real-world arithmetic reasoning tasks demonstrate the superior performance of our method compared to existing techniques.\nClaim2: Furthermore, our LM achieves flawless execution on symbolic operations tasks, highlighting the potential of our method in enabling LMs to possess true symbolic comprehension capabilities.\nClaim3: The implementation of the Neural Comprehension framework facilitates the integration of rule-intensive abilities and reasoning capabilities into LMs endowing them with genuine symbolic comprehension skills.\nClaim4: Our experimental results demonstrate the effectiveness of our approach in comparison with existing state-of-the-art techniques, such as vanilla fine-tuning, few-shot learning, and Chain-of-Thought reasoning.\nClaim5: Specifically, Neural Comprehension outperforms these methods in terms of accuracy showcasing its superiority in handling rule-intensive tasks.\nClaim6: Our study presents a strong case for the deployment of Neural Comprehension in language models highlighting its potential to transform the landscape of symbolic reasoning and language understanding capabilities.\nClaim7: For example, when calculating 364425-216582 , the pre-trained language model output 148843 , which is incorrect. However, the Subtraction CoNN can correct the result to 147843 in the neural comprehension framework.\nClaim8: This process encoded into context dynamically, improving intermediate results interpretability and final result accuracy.\nClaim9: To better appreciate the benefits of our method in handling rule-intensive tasks and improving accuracy, it is crucial to understand the gradient perspective of ICL.\nClaim10: Therefore, our proposed method introduces an explicit learning component to provide more appropriate gradient updates for such tasks, ultimately leading to enhanced overall performance.\nClaim11: It is crucial to note that CoNNs are designed to minimize the loss associated with rule-based tasks, essentially providing an optimal gradient for tasks involving rule-intensive operations.\nClaim12: This leads to a substantial improvement in the model\u2019s accuracy for rule-based tasks, as the gradient updates provided by CoNNs are more suitable for rule learning compared to the initially available gradients from the LM.\nClaim13: By amalgamating the both of gradient sources, the Neural Comprehension model achieves a more refined optimization of in-context learning.\nClaim14: Additionally, from the perspective of gradients, our approach surpasses conventional data-driven implicit learning techniques as it integrates explicit rule-based learning mechanisms that exhibit more suitable gradient updates for rule-intensive questions.\nClaim15: The Neural Comprehension model effectively balances the need for implicit and explicit learning within the ICL framework, leading to an enhanced overall performance in terms of accuracy and interpretability.\nClaim16: In this study, we primarily explore the capacity of language models to address symbolic reasoning tasks, concentrating on three areas: symbolic operations, symbolic reasoning, and arithmetic reasoning.\nClaim17: The results of our experiment demonstrate that the Vanilla Fine-tune (red lines) method performs optimally on the in-domain (10-20 digit) training set, while its performance deteriorates for both more simplistic and more intricate.\nClaim18: Our study reveals that Neural Comprehension attains remarkably high accuracy in symbolic operations.\nClaim19: This implies that Neural Comprehension, unlike conventional methods, does not rely on training data and remains unaffected by discrepancies in input lengths for in-distribution and out-of-distribution data.\nClaim20: Consequently, it alleviates the requirement for step-by-step work tracking, and language models with CoNNs only need relatively fewer computational steps to execute sequence operations directly.\nClaim21: Encoding rules into neural network modules endows us with greater interpretability, enabling language models to flawlessly perform purely symbolic operation tasks.\nClaim22: We observed that neural comprehension improves the symbolic reasoning capabilities of pre-trained language models in most cases (Neural Comprehension almost always outperforms Vanilla Fine-tune in Figure 4), and can fit faster.\nClaim23: This observation suggests that the introduction of compiled neural networks has a positive impact on pretrained LMs, addressing rule comprehension limitations in symbolic reasoning tasks.\nClaim24: As demonstrated in Figure 5, the vanilla CoT model struggles to extrapolate and solve arithmetic problems involving longer digit lengths. However, integrating Neural Comprehension significantly improves the performance of language models on such complex arithmetic tasks.\nClaim25: For further evidence, we present additional experimental results on widely-used arithmetic reasoning datasets in Appendix D.2 , which reinforce the benefits of using Neural Comprehension over the vanilla CoT model.\nClaim26: In comparison to language models employing external tools like PAL [Gao et al., 2022], our findings suggest that generating accurate code for the less code-trained GLM-130B model might be challenging for PAL, resulting in performance levels inferior to those of the vanilla CoT.\nClaim27: This outcome indicates that language models offer greater flexibility, whereas external tools may have difficulties in more complex or unique situations.\nClaim28: The integration of compiled neural networks appears to be a more promising approach, as evidenced by the performance improvements observed in our experiments.\nClaim29: Specifically, when language models encounter intricate arithmetic tasks that involve nested operations or multi-step calculations, the integrated CoNNs can efficiently handle these operations, allowing the language model to focus on higher-level reasoning.\nClaim30: In contrast, the use of external tools often requires explicit coding and may not generalize effectively to more complicated scenarios.\nClaim31: In conclusion, our results demonstrate that incorporating compiled neural networks into language models provides a more robust and versatile solution for arithmetic reasoning and related challenges, underlining the superiority of this approach over external tools such as PAL.\nClaim32: Efficiently deploying multiple CoNNs is crucial for achieving exceptional Neural Comprehension performance.\nClaim33: It is observed that integrating pertinent CoNNs bolsters the performance of the initial language model, whereas the inclusion of unrelated language models rarely causes detrimental effects, regardless of whether single or multiple CoNNs are combined.\nClaim34: This can be ascribed to the refined design of the Neural Comprehension framework, which ensures the precise execution of assigned tasks by CoNNs without interference from irrelevant modules.\nClaim35: Importantly, as seen in Appendix B.3 , the parameter count for each CoNN module ranges from 1/1000 to 1/1000000 of that for GPT-3, and the experiments in Appendix D.3 show that the inference latency in the neural understanding framework only increases by 1%-3% compared to Vanilla.\nClaim36: This observation underscores the remarkable scalability of the Neural Comprehension framework, which possesses the capability to not only accommodate existing knowledge concepts but also assimilate novel ones as the number of CoNNs expands.\nClaim37: Theoretically, the integration of tens of thousands of CoNN modules within language models holds the potential to foster a comprehensive understanding of concepts.\nClaim38: We have observed that pretrained language models lack an intrinsic comprehension of rule-based concepts and explored how Neural Comprehension can integrate compiled neural networks into the language model framework in a simple and generic manner.\nClaim39: We demonstrated the superiority of our approach over existing learning-based method, Without external tools, our approach enables language models to perform nearly perfect symbolic operations and can be applied to more realistic arithmetic reasoning tasks.\n\nReview: Weakness: I think major weaknesses of this work is (1) a hardcoded structure of CoNNs under consideration and (2) hard coded policy of choosing the LM vs CoNN component by connecting that to task-based properties. I believe the most interesting part would be to learn the beta factor which also seems to be very challenging. \n\nAuthors claimed that their work suggests potential improvements from using Neural Comprehension in other tasks, but they did not mention how to get CoNNs for these tasks? In general, the discussion about CoNN design and implementation is somewhat skipped in the paper while it seem to be a crucial factor in this paper's impact.\nQuestions: There are some grammar and syntax typos in the paper, even in the section titles, please fix that.\n\nA clarification question: from my understanding you have done some hard coding of the beta policy such that i.e. it only triggers ICL gradient when it sees \""=\"" token. Does this mean that this approach will not work at all if you use sequence \""4 + 4 equals 8\""? If so, do you have any ideas in mind how to make it work? In general I like this idea of plug-in CoNNs, but they need to be more seamless and fluent without requiring such hard coding in the task level.\n\n\n\n"", ""id"": ""tpIUgkq0xa0"", ""pdf"": ""openreview.net/pdf?id=tpIUgkq0xa""}","0"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""Firstly, I would argue that random-LTD has almost nothing to do with data efficiency. It looks to me that random-LTD is actually closer to some regularization, particularly dropout [1]. Just because one can achieve the same performance with 2x less data with some regularization techniques (e.g. weight decay), calling them as a \""data efficiency trick\"" cannot be justified in my opinion."", ""Target claims"": [6, 37, 38], ""Reasoning"": ""The reviewer argues that random-LTD should not be classified as a data efficiency technique, which directly challenges claims associated with this technique being categorized under data efficiency. Claim 6 states that random-LTD provides better data/training efficiency than TokenBypass. Claim 37 highlights random-LTD's performance in terms of data saving which is directly linked to data efficiency. Claim 38 again emphasizes its data/time efficiency. The weakness targets the validity of categorizing random-LTD under data efficiency, thus questioning these claims.""}, {""Weakness span"": ""From the systems perspective, however, random-LTD consistently hurts the overall throughput as shown in Table 3 & 4 somehow."", ""Target claims"": [6], ""Reasoning"": ""The reviewer points out that random-LTD negatively impacts system throughput, which could undermine Claim 6's assertion of random-LTD providing better efficiency than TokenBypass. If throughput is affected, the overall 'efficiency' in a practical deployment could be compromised, questioning the claim made about its efficiency.""}, {""Weakness span"": ""XYZ Data Efficiency framework seems to lack the flexibility and/or modularity, a highly important aspect in the framework. For example, the use of certain CL techniques require specific LR schedules to enjoy the maximal improvement. This essentially means that users get a reduced flexibility in choosing their own LR schedulers. Such entanglement between LR schedulers and data sampling strategies can further harm the user experience when they want to implement their custom data sampling strategies. Overall, my impression is that XYZ Data Efficiency doesn't allow much flexibility for users to try out different things, but rather enforces users to follow their predefined pipeline, in this case, composed of random-LTD and several CL strategies."", ""Target claims"": [3, 5, 48], ""Reasoning"": ""The reviewer criticizes the lack of flexibility and modularity in the XYZ Data Efficiency framework, which questions claims about its presumed advantages. Claim 3 underscores the ease of use and tuning of XYZ Data Efficiency, Claim 5 mentions its application benefits on additional workloads, and Claim 48 asserts the applicability and benefit on other tasks, suggesting flexibility and broader application. Thus, the reviewer's concern about flexibility relates directly to these claims, questioning the breadth of application and ease advertised.""}, {""Weakness span"": ""To summarize, I find two major framing issues in this paper. First, while CL can be approached from the data efficiency perspective, I believe random-LTD (or data routing) has little to do with data efficiency."", ""Target claims"": [6, 37, 38], ""Reasoning"": ""The reviewer reiterates that random-LTD should not be viewed from a data efficiency standpoint, directly challenging the categorization of this technique which is central to several claims discussing its efficiency advantages (Claims 6, 37, and 38). By framing random-LTD more akin to regularization than data efficiency, the reviewer disputes the paper's claims regarding efficiency.""}]}, ""meta"": {""id"": ""nkfSodI4ow"", ""review_idx"": 0, ""title"": ""Title: XYZ Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing\nAbstract: Recent advances on deep learning models come at the price of formidable training cost. The increasing model size is one of the root causes, but another less-emphasized fact is that data scale is actually increasing at a similar speed as model scale, and the training cost is proportional to both of them. Compared to the rapidly evolving model architecture, how to efficiently use the training data (especially for the expensive foundation model pretraining) is both less explored and difficult to realize due to the lack of a convenient framework that focus on data efficiency capabilities. To this end, we present XYZ Data Efficiency, a framework that makes better use of data, increases training efficiency, and improves model quality. Specifically, we propose and combine two data efficiency techniques: efficient data sampling via a general curriculum learning library, and efficient data routing via a novel random layerwise token dropping technique. For GPT-3 1.3B language model pretraining, our work achieves 12.5x less data/time/cost (`$`3.7K if rent on Azure), while still maintaining 95% of model quality compared to baseline with full data and cost (`$`46.3K). For GPT-3 1.3B and BERT-large pretraining, our work can also achieve the same model quality with up to 2x less data/time/cost, or achieve better model quality under same data/time/cost. XYZ Data Efficiency is easy to use and tune, enabling us to easily apply it and verify its benefit on additional tasks including GPT-3 MoE model pretraining and small-scale GPT-2/ViT finetuning.\n\nTLDR:We propose a framework that reduces GPT-3/BERT pretraining and GPT-2/ViT finetuning data and cost with boosted model quality via new innovations on data sampling and data routing.\n\nKeywords:data efficiency, training efficiency, foundation model, pretraining, finetuning, language model, GPT-3, BERT, ViT"", ""claims"": [""Claim1: For GPT-3 1.3B language model pretraining, our work achieves 12.5x less data/time/cost (`$`3.7K if rent on Azure), while still maintaining 95% of model quality compared to baseline with full data and cost (`$`46.3K)."", ""Claim2: For GPT-3 1.3B and BERT-large pretraining, our work can also achieve the same model quality with up to 2x less data/time/cost, or achieve better model quality under same data/time/cost."", ""Claim3: XYZ Data Efficiency is easy to use and tune, enabling us to easily apply it and verify its benefit on additional tasks including GPT-3 MoE model pretraining and small-scale GPT-2/ViT finetuning."", ""Claim4: For GPT-3 1.3B and BERT-large pretraining, we can also achieve up to 2x data and 2x time saving together with better or similar model quality as compared to the baseline training with full data, greatly surpassing state-of-the-art data efficiency solutions as summarized in Tab. 1."", ""Claim5: This enables us to easily apply proposed work and verify its benefits on additional workloads including GPT-3 Mixture-of-Experts ( ) model pretraining and small-scale GPT-2/ViT model finetuning."", ""Claim6: Compared to TokenBypass, our random-LTD (1) does not require importance score metric, special token treatment, or the sandwich token dropping rule, which dramatically reduces the manual design effort; (2) has been broadly tested on GPT-3/BERT pretraining tasks and GPT-2/ViT finetuning tasks, providing better data/training efficiency than TokenBypass."", ""Claim7: As a result, our experiments on BERT pretraining confirm that random-LTD does not require and won\u2019t benefit from special token treatment used by the TokenBypass work, further reducing the implementation complexity."", ""Claim8: While TokenBypass needs to keep half of the layers in full sequence length training, random-LTD has no such limitation."", ""Claim9: Our experiments show that keeping the first and last layers in full sequence length training usually leads to better performance since (1) the first layer directly connects to the embedding, and it can help refine the raw feature; (2) directly connected to the final prediction, the last layer provides a feature realignment for all tokens which could improve the model quality."", ""Claim10: Our experiments show that MSLG provides better model quality than constant drop schedule under similar data/compute savings."", ""Claim11: This token consumption calculation is also critical to the learning rate schedule: previous CL work [29] finds that if a CL technique reduces the number of tokens on certain steps, it is desirable to use a learning rate decay schedule based on consumed tokens instead of consumed steps."", ""Claim12: To our knowledge this is the first work to apply such LR schedule to token dropping/data routing techniques, and our experiments show that it does help improving random-LTD\u2019s performance."", ""Claim13: And the overall composibility of XYZ Data Efficiency enables us to leverage both data efficiency techniques and achieve even better data and training efficiency (Sec. 4)."", ""Claim14: For GPT-2 finetuning, given the low training cost we also perform full training of 16 different CL/random-LTD settings which confirm that (1) the low-cost tuning strategy is able to find very good hyperparameters; (2) both CL and random-LTD are not sensitive to hyperparameter choices."", ""Claim15: We evaluate XYZ Data Efficiency by GPT-3/GPT-3 MoE/BERT pretraining and GPT-2/ViT finetuning."", ""Claim16: Among the 5 CL difficulty metrics we have for GPT-3 model, to find out which metric provides the best model quality we pretrain the model (with 100% data) 5 times (each with 1 CL metric)."", ""Claim17: As presented in Tab. 3 case 1 to 6, results show that all 5 CL metrics provide better model quality than baseline (except (4)CL_voc\u2019s 0-shot accuracy), and the (5)CL_seqtru_voc provides the best quality."", ""Claim18: Results show that the average 0-shot evaluation accuracy drops from 42.5 to 41.9 when baseline use less data (Tab. 3 case 1, 9)."", ""Claim19: On the other hand, our CL solution (case 10) with 67% data is able to achieve better 0-shot and 10-shot accuracy than baseline with 100% data, achieving a 1.5x data and time saving."", ""Claim20: Results show similar benefit as CL: better model quality when using 100% data (Tab. 3 case 7), and 1.5x data/time saving while maintaining model quality (case 11)."", ""Claim21: Results (case 5, 7, 8) show that using both techniques together further improves the model quality, demonstrating the benefit of composability by our framework."", ""Claim22: Results (case 12 to 15) show that the baseline has worse 0-shot and 10-shot evaluation accuracy under 2x less data."", ""Claim23: Using CL or random-LTD can only recover part of the accuracy loss."", ""Claim24: On the other hand, the composed data efficiency solution is able to achieve the same or better accuracy results as baseline with 100% data, demonstrating a 2x data and 2x time saving."", ""Claim25: At the beginning of the training the proposed approach has slower convergence since we focus on easier/simpler data samples (CL) and drop more tokens (random-LTD) at the beginning."", ""Claim26: On the other hand, at the later stage of training the proposed approach is able to provide faster convergence speed than baseline."", ""Claim27: Our approach with 50% data is able to achieve similar final validation perplexity as baseline with 100% data (while baseline with 50% data cannot)."", ""Claim28: Our approach with 100% data is able to achieve even better final validation perplexity which leads to the highest model quality."", ""Claim29: Results show that our approach provides better model quality at all cost budgets, advancing the whole cost-quality Pareto frontier."", ""Claim30: In particular, we achieve up to 12.5x data/time/cost saving while still maintaining 95% of the model quality (zero-shot eval accuracy) compared to the baseline with full data."", ""Claim31: Based on measured training time, this would be a cost reduction from $46.3K to $3.7K if renting similar hardware on Azure [2], greatly democratizing research and usage of foundation models."", ""Claim32: Results show that MoE model does achieve similar model quality with less training cost (Tab. 3 case 1, 16)."", ""Claim33: On the other hand, our approach can further improve MoE model\u2019s model quality (case 17), confirming its broad applicability."", ""Claim34: Tab. 4 case 1 to 6 results show that 4 CL metrics provide better model quality than baseline, and the (5)CL_seqtru_voc provides the best quality."", ""Claim35: Results show that the GLUE score drops from 87.29 to 87.19 when baseline use less data (case 1, 9)."", ""Claim36: On the other hand, our CL solution (case 10) with 67% data is able to achieve on-par GLUE score as baseline with 100% data, achieving a 1.5x data and time saving."", ""Claim37: In terms of data saving random-LTD performs better than CL: it is able to achieve better GLUE score even with 2x less data than baseline (case 14), greatly surpassing the 1.33x data saving by the state-of-the-art TokenBypass method."", ""Claim38: However, even with this overhead random-LTD is still a more data/time-efficient solution than baseline/TokenBypass."", ""Claim39: At 50% data, the composed solution further improves the GLUE score from the CL/random-LTD-only cases (case 15), achieving a 2x data and 1.8x time saving while maintaining the GLUE score compared to baseline."", ""Claim40: To verify the effectiveness of the proposed work on small-scale tasks, we apply our techniques to PTB finetuning task for an already-pretrained GPT-2 350M model checkpoint from Huggingface."", ""Claim41: seqres provides the best model quality among the 5 CL metrics (case 3), unlike the two pretraining tasks where the seqtru_voc is the best metric."", ""Claim42: Applying random-LTD also improves the model quality (case 7)."", ""Claim43: Both CL best metric and random-LTD are able to surpass baseline on all 16 combinations of their hyperparameters, demonstrating that they are not sensitive to the hyperparameter choices."", ""Claim44: The composed CL+random-LTD case (case 8) further improves model quality from random-LTD-only case, but is only on-par with CL-only case."", ""Claim45: As presented in Tab. 6, results show that random-LTD is able to achieve 1.3-1.4x data savings while maintaining the model quality, demonstrating its broad applicability."", ""Claim46: In this work we propose the XYZ Data Efficiency framework, which demonstrate the power of composing 2 novel data efficiency techniques together."", ""Claim47: This enables us to achieve an up 12.5x data/time/cost saving (from $46.3K to $3.7K on Azure) while maintaining 95% of model quality for GPT-3 pretraining, an up to 2x saving for GPT-3 and BERT pretraining while maintaining 100% model quality, or to achieve even better model quality under similar data and cost."", ""Claim48: XYZ Data Efficiency is easy to use and tune, which enables us to apply it and verify the benefit on additional GPT-3 MoE pretraining and GPT-2/ViT finetuning tasks.""], ""review"": ""Review: Weakness: This paper touches on multiple aspects of improving training efficiency of large models, especially from the data perspective, but none of them seem to meet the NeurIPS standard.\n1. Firstly, I would argue that random-LTD has almost nothing to do with data efficiency. It looks to me that random-LTD is actually closer to some regularization, particularly dropout [1]. Just because one can achieve the same performance with 2x less data with some regularization techniques (e.g. weight decay), calling them as a \""data efficiency trick\"" cannot be justified in my opinion. Since it bypasses some computations of some tokens in some layers, I believe it's closer to a computation efficiency rather than data efficiency trick. From the systems perspective, however, random-LTD consistently hurts the overall throughput as shown in Table 3 & 4 somehow.\n2. I doubt the practical utility of map-reduce-based data difficulty calculation. The metrics used in this paper are all offline metrics in that they can be calculated only once before training and can be reused later. While I understand even such preprocessing can take a painfully long time with recent large datasets (e.g. Pile or C4), I don't think the value practitioners will get from this paper would be not so significant. If they can show their framework can be combined with some online or dynamic metrics (e.g. loss value for each token), I would be more convinced.\n3. XYZ Data Efficiency framework seems to lack the flexibility and/or modularity, a highly important aspect in the framework. For example, the use of certain CL techniques require specific LR schedules to enjoy the maximal improvement. This essentially means that users get a reduced flexibility in choosing their own LR schedulers. Such entanglement between LR schedulers and data sampling strategies can further harm the user experience when they want to implement their custom data sampling strategies. Overall, my impression is that XYZ Data Efficiency doesn't allow much flexibility for users to try out different things, but rather enforces users to follow their predefined pipeline, in this case, composed of random-LTD and several CL strategies.\n\nTo summarize, I find two major framing issues in this paper. First, while CL can be approached from the data efficiency perspective, I believe random-LTD (or data routing) has little to do with data efficiency. Second, I believe XYZ Data Efficiency is more of a combination of two algorithms (i.e. CL and random-LTD) rather than a some general framework due to its lack of flexibility and modularity.\n\n[1] Liu et al., Gating dropout: Communication-efficient regularization for sparsely activated transformers\nQuestions: 1. Can authors provide full train/validation loss curves (from beginning to end)? It's emphasized in the abstract that XYZ Data Efficiency achieves 95% of baseline performance with up to 12.5x less data/time. However, it's generally true that training significantly slows down as it gets closer to convergence. Therefore, it is also possible that the baseline training run also achieves 95% of its maximum performance in the very early stage and takes a very long time to improve final 5%. In this case, comparing time-to-95% would be a more fair metric. Such confusion can be easily resolved by having whole training loss curves.\n\n""}, ""review"": ""Title: XYZ Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing\nAbstract: Recent advances on deep learning models come at the price of formidable training cost. The increasing model size is one of the root causes, but another less-emphasized fact is that data scale is actually increasing at a similar speed as model scale, and the training cost is proportional to both of them. Compared to the rapidly evolving model architecture, how to efficiently use the training data (especially for the expensive foundation model pretraining) is both less explored and difficult to realize due to the lack of a convenient framework that focus on data efficiency capabilities. To this end, we present XYZ Data Efficiency, a framework that makes better use of data, increases training efficiency, and improves model quality. Specifically, we propose and combine two data efficiency techniques: efficient data sampling via a general curriculum learning library, and efficient data routing via a novel random layerwise token dropping technique. For GPT-3 1.3B language model pretraining, our work achieves 12.5x less data/time/cost (`$`3.7K if rent on Azure), while still maintaining 95% of model quality compared to baseline with full data and cost (`$`46.3K). For GPT-3 1.3B and BERT-large pretraining, our work can also achieve the same model quality with up to 2x less data/time/cost, or achieve better model quality under same data/time/cost. XYZ Data Efficiency is easy to use and tune, enabling us to easily apply it and verify its benefit on additional tasks including GPT-3 MoE model pretraining and small-scale GPT-2/ViT finetuning.\n\nClaim1: For GPT-3 1.3B language model pretraining, our work achieves 12.5x less data/time/cost (`$`3.7K if rent on Azure), while still maintaining 95% of model quality compared to baseline with full data and cost (`$`46.3K).\nClaim2: For GPT-3 1.3B and BERT-large pretraining, our work can also achieve the same model quality with up to 2x less data/time/cost, or achieve better model quality under same data/time/cost.\nClaim3: XYZ Data Efficiency is easy to use and tune, enabling us to easily apply it and verify its benefit on additional tasks including GPT-3 MoE model pretraining and small-scale GPT-2/ViT finetuning.\nClaim4: For GPT-3 1.3B and BERT-large pretraining, we can also achieve up to 2x data and 2x time saving together with better or similar model quality as compared to the baseline training with full data, greatly surpassing state-of-the-art data efficiency solutions as summarized in Tab. 1.\nClaim5: This enables us to easily apply proposed work and verify its benefits on additional workloads including GPT-3 Mixture-of-Experts ( ) model pretraining and small-scale GPT-2/ViT model finetuning.\nClaim6: Compared to TokenBypass, our random-LTD (1) does not require importance score metric, special token treatment, or the sandwich token dropping rule, which dramatically reduces the manual design effort; (2) has been broadly tested on GPT-3/BERT pretraining tasks and GPT-2/ViT finetuning tasks, providing better data/training efficiency than TokenBypass.\nClaim7: As a result, our experiments on BERT pretraining confirm that random-LTD does not require and won\u2019t benefit from special token treatment used by the TokenBypass work, further reducing the implementation complexity.\nClaim8: While TokenBypass needs to keep half of the layers in full sequence length training, random-LTD has no such limitation.\nClaim9: Our experiments show that keeping the first and last layers in full sequence length training usually leads to better performance since (1) the first layer directly connects to the embedding, and it can help refine the raw feature; (2) directly connected to the final prediction, the last layer provides a feature realignment for all tokens which could improve the model quality.\nClaim10: Our experiments show that MSLG provides better model quality than constant drop schedule under similar data/compute savings.\nClaim11: This token consumption calculation is also critical to the learning rate schedule: previous CL work [29] finds that if a CL technique reduces the number of tokens on certain steps, it is desirable to use a learning rate decay schedule based on consumed tokens instead of consumed steps.\nClaim12: To our knowledge this is the first work to apply such LR schedule to token dropping/data routing techniques, and our experiments show that it does help improving random-LTD\u2019s performance.\nClaim13: And the overall composibility of XYZ Data Efficiency enables us to leverage both data efficiency techniques and achieve even better data and training efficiency (Sec. 4).\nClaim14: For GPT-2 finetuning, given the low training cost we also perform full training of 16 different CL/random-LTD settings which confirm that (1) the low-cost tuning strategy is able to find very good hyperparameters; (2) both CL and random-LTD are not sensitive to hyperparameter choices.\nClaim15: We evaluate XYZ Data Efficiency by GPT-3/GPT-3 MoE/BERT pretraining and GPT-2/ViT finetuning.\nClaim16: Among the 5 CL difficulty metrics we have for GPT-3 model, to find out which metric provides the best model quality we pretrain the model (with 100% data) 5 times (each with 1 CL metric).\nClaim17: As presented in Tab. 3 case 1 to 6, results show that all 5 CL metrics provide better model quality than baseline (except (4)CL_voc\u2019s 0-shot accuracy), and the (5)CL_seqtru_voc provides the best quality.\nClaim18: Results show that the average 0-shot evaluation accuracy drops from 42.5 to 41.9 when baseline use less data (Tab. 3 case 1, 9).\nClaim19: On the other hand, our CL solution (case 10) with 67% data is able to achieve better 0-shot and 10-shot accuracy than baseline with 100% data, achieving a 1.5x data and time saving.\nClaim20: Results show similar benefit as CL: better model quality when using 100% data (Tab. 3 case 7), and 1.5x data/time saving while maintaining model quality (case 11).\nClaim21: Results (case 5, 7, 8) show that using both techniques together further improves the model quality, demonstrating the benefit of composability by our framework.\nClaim22: Results (case 12 to 15) show that the baseline has worse 0-shot and 10-shot evaluation accuracy under 2x less data.\nClaim23: Using CL or random-LTD can only recover part of the accuracy loss.\nClaim24: On the other hand, the composed data efficiency solution is able to achieve the same or better accuracy results as baseline with 100% data, demonstrating a 2x data and 2x time saving.\nClaim25: At the beginning of the training the proposed approach has slower convergence since we focus on easier/simpler data samples (CL) and drop more tokens (random-LTD) at the beginning.\nClaim26: On the other hand, at the later stage of training the proposed approach is able to provide faster convergence speed than baseline.\nClaim27: Our approach with 50% data is able to achieve similar final validation perplexity as baseline with 100% data (while baseline with 50% data cannot).\nClaim28: Our approach with 100% data is able to achieve even better final validation perplexity which leads to the highest model quality.\nClaim29: Results show that our approach provides better model quality at all cost budgets, advancing the whole cost-quality Pareto frontier.\nClaim30: In particular, we achieve up to 12.5x data/time/cost saving while still maintaining 95% of the model quality (zero-shot eval accuracy) compared to the baseline with full data.\nClaim31: Based on measured training time, this would be a cost reduction from $46.3K to $3.7K if renting similar hardware on Azure [2], greatly democratizing research and usage of foundation models.\nClaim32: Results show that MoE model does achieve similar model quality with less training cost (Tab. 3 case 1, 16).\nClaim33: On the other hand, our approach can further improve MoE model\u2019s model quality (case 17), confirming its broad applicability.\nClaim34: Tab. 4 case 1 to 6 results show that 4 CL metrics provide better model quality than baseline, and the (5)CL_seqtru_voc provides the best quality.\nClaim35: Results show that the GLUE score drops from 87.29 to 87.19 when baseline use less data (case 1, 9).\nClaim36: On the other hand, our CL solution (case 10) with 67% data is able to achieve on-par GLUE score as baseline with 100% data, achieving a 1.5x data and time saving.\nClaim37: In terms of data saving random-LTD performs better than CL: it is able to achieve better GLUE score even with 2x less data than baseline (case 14), greatly surpassing the 1.33x data saving by the state-of-the-art TokenBypass method.\nClaim38: However, even with this overhead random-LTD is still a more data/time-efficient solution than baseline/TokenBypass.\nClaim39: At 50% data, the composed solution further improves the GLUE score from the CL/random-LTD-only cases (case 15), achieving a 2x data and 1.8x time saving while maintaining the GLUE score compared to baseline.\nClaim40: To verify the effectiveness of the proposed work on small-scale tasks, we apply our techniques to PTB finetuning task for an already-pretrained GPT-2 350M model checkpoint from Huggingface.\nClaim41: seqres provides the best model quality among the 5 CL metrics (case 3), unlike the two pretraining tasks where the seqtru_voc is the best metric.\nClaim42: Applying random-LTD also improves the model quality (case 7).\nClaim43: Both CL best metric and random-LTD are able to surpass baseline on all 16 combinations of their hyperparameters, demonstrating that they are not sensitive to the hyperparameter choices.\nClaim44: The composed CL+random-LTD case (case 8) further improves model quality from random-LTD-only case, but is only on-par with CL-only case.\nClaim45: As presented in Tab. 6, results show that random-LTD is able to achieve 1.3-1.4x data savings while maintaining the model quality, demonstrating its broad applicability.\nClaim46: In this work we propose the XYZ Data Efficiency framework, which demonstrate the power of composing 2 novel data efficiency techniques together.\nClaim47: This enables us to achieve an up 12.5x data/time/cost saving (from $46.3K to $3.7K on Azure) while maintaining 95% of model quality for GPT-3 pretraining, an up to 2x saving for GPT-3 and BERT pretraining while maintaining 100% model quality, or to achieve even better model quality under similar data and cost.\nClaim48: XYZ Data Efficiency is easy to use and tune, which enables us to apply it and verify the benefit on additional GPT-3 MoE pretraining and GPT-2/ViT finetuning tasks.\n\nReview: Weakness: This paper touches on multiple aspects of improving training efficiency of large models, especially from the data perspective, but none of them seem to meet the NeurIPS standard.\n1. Firstly, I would argue that random-LTD has almost nothing to do with data efficiency. It looks to me that random-LTD is actually closer to some regularization, particularly dropout [1]. Just because one can achieve the same performance with 2x less data with some regularization techniques (e.g. weight decay), calling them as a \""data efficiency trick\"" cannot be justified in my opinion. Since it bypasses some computations of some tokens in some layers, I believe it's closer to a computation efficiency rather than data efficiency trick. From the systems perspective, however, random-LTD consistently hurts the overall throughput as shown in Table 3 & 4 somehow.\n2. I doubt the practical utility of map-reduce-based data difficulty calculation. The metrics used in this paper are all offline metrics in that they can be calculated only once before training and can be reused later. While I understand even such preprocessing can take a painfully long time with recent large datasets (e.g. Pile or C4), I don't think the value practitioners will get from this paper would be not so significant. If they can show their framework can be combined with some online or dynamic metrics (e.g. loss value for each token), I would be more convinced.\n3. XYZ Data Efficiency framework seems to lack the flexibility and/or modularity, a highly important aspect in the framework. For example, the use of certain CL techniques require specific LR schedules to enjoy the maximal improvement. This essentially means that users get a reduced flexibility in choosing their own LR schedulers. Such entanglement between LR schedulers and data sampling strategies can further harm the user experience when they want to implement their custom data sampling strategies. Overall, my impression is that XYZ Data Efficiency doesn't allow much flexibility for users to try out different things, but rather enforces users to follow their predefined pipeline, in this case, composed of random-LTD and several CL strategies.\n\nTo summarize, I find two major framing issues in this paper. First, while CL can be approached from the data efficiency perspective, I believe random-LTD (or data routing) has little to do with data efficiency. Second, I believe XYZ Data Efficiency is more of a combination of two algorithms (i.e. CL and random-LTD) rather than a some general framework due to its lack of flexibility and modularity.\n\n[1] Liu et al., Gating dropout: Communication-efficient regularization for sparsely activated transformers\nQuestions: 1. Can authors provide full train/validation loss curves (from beginning to end)? It's emphasized in the abstract that XYZ Data Efficiency achieves 95% of baseline performance with up to 12.5x less data/time. However, it's generally true that training significantly slows down as it gets closer to convergence. Therefore, it is also possible that the baseline training run also achieves 95% of its maximum performance in the very early stage and takes a very long time to improve final 5%. In this case, comparing time-to-95% would be a more fair metric. Such confusion can be easily resolved by having whole training loss curves.\n\n\n\n"", ""id"": ""nkfSodI4ow0"", ""pdf"": ""openreview.net/pdf?id=nkfSodI4ow""}","1"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""I think the previous work [2] also explores the efficiency at the data sample-level and data routing level. So, it is probably not appropriate to claim the proposed method is \u201cthe first to demonstrate that composing data sampling and routing techniques can lead to even better data/training efficiency \u2026\u201d"", ""Target claims"": [13], ""Reasoning"": ""The reviewer challenges the originality of Claim13, which states that the XYZ Data Efficiency framework is the first to demonstrate the advantages of composing data sampling and routing techniques. The reviewer argues that previous work ([2]) has already explored data sample-level and data routing efficiency. Therefore, Claim13 is called into question regarding its assertion of novelty and uniqueness.""}]}, ""meta"": {""id"": ""nkfSodI4ow"", ""review_idx"": 1, ""title"": ""Title: XYZ Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing\nAbstract: Recent advances on deep learning models come at the price of formidable training cost. The increasing model size is one of the root causes, but another less-emphasized fact is that data scale is actually increasing at a similar speed as model scale, and the training cost is proportional to both of them. Compared to the rapidly evolving model architecture, how to efficiently use the training data (especially for the expensive foundation model pretraining) is both less explored and difficult to realize due to the lack of a convenient framework that focus on data efficiency capabilities. To this end, we present XYZ Data Efficiency, a framework that makes better use of data, increases training efficiency, and improves model quality. Specifically, we propose and combine two data efficiency techniques: efficient data sampling via a general curriculum learning library, and efficient data routing via a novel random layerwise token dropping technique. For GPT-3 1.3B language model pretraining, our work achieves 12.5x less data/time/cost (`$`3.7K if rent on Azure), while still maintaining 95% of model quality compared to baseline with full data and cost (`$`46.3K). For GPT-3 1.3B and BERT-large pretraining, our work can also achieve the same model quality with up to 2x less data/time/cost, or achieve better model quality under same data/time/cost. XYZ Data Efficiency is easy to use and tune, enabling us to easily apply it and verify its benefit on additional tasks including GPT-3 MoE model pretraining and small-scale GPT-2/ViT finetuning.\n\nTLDR:We propose a framework that reduces GPT-3/BERT pretraining and GPT-2/ViT finetuning data and cost with boosted model quality via new innovations on data sampling and data routing.\n\nKeywords:data efficiency, training efficiency, foundation model, pretraining, finetuning, language model, GPT-3, BERT, ViT"", ""claims"": [""Claim1: For GPT-3 1.3B language model pretraining, our work achieves 12.5x less data/time/cost (`$`3.7K if rent on Azure), while still maintaining 95% of model quality compared to baseline with full data and cost (`$`46.3K)."", ""Claim2: For GPT-3 1.3B and BERT-large pretraining, our work can also achieve the same model quality with up to 2x less data/time/cost, or achieve better model quality under same data/time/cost."", ""Claim3: XYZ Data Efficiency is easy to use and tune, enabling us to easily apply it and verify its benefit on additional tasks including GPT-3 MoE model pretraining and small-scale GPT-2/ViT finetuning."", ""Claim4: For GPT-3 1.3B and BERT-large pretraining, we can also achieve up to 2x data and 2x time saving together with better or similar model quality as compared to the baseline training with full data, greatly surpassing state-of-the-art data efficiency solutions as summarized in Tab. 1."", ""Claim5: This enables us to easily apply proposed work and verify its benefits on additional workloads including GPT-3 Mixture-of-Experts ( ) model pretraining and small-scale GPT-2/ViT model finetuning."", ""Claim6: Compared to TokenBypass, our random-LTD (1) does not require importance score metric, special token treatment, or the sandwich token dropping rule, which dramatically reduces the manual design effort; (2) has been broadly tested on GPT-3/BERT pretraining tasks and GPT-2/ViT finetuning tasks, providing better data/training efficiency than TokenBypass."", ""Claim7: As a result, our experiments on BERT pretraining confirm that random-LTD does not require and won\u2019t benefit from special token treatment used by the TokenBypass work, further reducing the implementation complexity."", ""Claim8: While TokenBypass needs to keep half of the layers in full sequence length training, random-LTD has no such limitation."", ""Claim9: Our experiments show that keeping the first and last layers in full sequence length training usually leads to better performance since (1) the first layer directly connects to the embedding, and it can help refine the raw feature; (2) directly connected to the final prediction, the last layer provides a feature realignment for all tokens which could improve the model quality."", ""Claim10: Our experiments show that MSLG provides better model quality than constant drop schedule under similar data/compute savings."", ""Claim11: This token consumption calculation is also critical to the learning rate schedule: previous CL work [29] finds that if a CL technique reduces the number of tokens on certain steps, it is desirable to use a learning rate decay schedule based on consumed tokens instead of consumed steps."", ""Claim12: To our knowledge this is the first work to apply such LR schedule to token dropping/data routing techniques, and our experiments show that it does help improving random-LTD\u2019s performance."", ""Claim13: And the overall composibility of XYZ Data Efficiency enables us to leverage both data efficiency techniques and achieve even better data and training efficiency (Sec. 4)."", ""Claim14: For GPT-2 finetuning, given the low training cost we also perform full training of 16 different CL/random-LTD settings which confirm that (1) the low-cost tuning strategy is able to find very good hyperparameters; (2) both CL and random-LTD are not sensitive to hyperparameter choices."", ""Claim15: We evaluate XYZ Data Efficiency by GPT-3/GPT-3 MoE/BERT pretraining and GPT-2/ViT finetuning."", ""Claim16: Among the 5 CL difficulty metrics we have for GPT-3 model, to find out which metric provides the best model quality we pretrain the model (with 100% data) 5 times (each with 1 CL metric)."", ""Claim17: As presented in Tab. 3 case 1 to 6, results show that all 5 CL metrics provide better model quality than baseline (except (4)CL_voc\u2019s 0-shot accuracy), and the (5)CL_seqtru_voc provides the best quality."", ""Claim18: Results show that the average 0-shot evaluation accuracy drops from 42.5 to 41.9 when baseline use less data (Tab. 3 case 1, 9)."", ""Claim19: On the other hand, our CL solution (case 10) with 67% data is able to achieve better 0-shot and 10-shot accuracy than baseline with 100% data, achieving a 1.5x data and time saving."", ""Claim20: Results show similar benefit as CL: better model quality when using 100% data (Tab. 3 case 7), and 1.5x data/time saving while maintaining model quality (case 11)."", ""Claim21: Results (case 5, 7, 8) show that using both techniques together further improves the model quality, demonstrating the benefit of composability by our framework."", ""Claim22: Results (case 12 to 15) show that the baseline has worse 0-shot and 10-shot evaluation accuracy under 2x less data."", ""Claim23: Using CL or random-LTD can only recover part of the accuracy loss."", ""Claim24: On the other hand, the composed data efficiency solution is able to achieve the same or better accuracy results as baseline with 100% data, demonstrating a 2x data and 2x time saving."", ""Claim25: At the beginning of the training the proposed approach has slower convergence since we focus on easier/simpler data samples (CL) and drop more tokens (random-LTD) at the beginning."", ""Claim26: On the other hand, at the later stage of training the proposed approach is able to provide faster convergence speed than baseline."", ""Claim27: Our approach with 50% data is able to achieve similar final validation perplexity as baseline with 100% data (while baseline with 50% data cannot)."", ""Claim28: Our approach with 100% data is able to achieve even better final validation perplexity which leads to the highest model quality."", ""Claim29: Results show that our approach provides better model quality at all cost budgets, advancing the whole cost-quality Pareto frontier."", ""Claim30: In particular, we achieve up to 12.5x data/time/cost saving while still maintaining 95% of the model quality (zero-shot eval accuracy) compared to the baseline with full data."", ""Claim31: Based on measured training time, this would be a cost reduction from $46.3K to $3.7K if renting similar hardware on Azure [2], greatly democratizing research and usage of foundation models."", ""Claim32: Results show that MoE model does achieve similar model quality with less training cost (Tab. 3 case 1, 16)."", ""Claim33: On the other hand, our approach can further improve MoE model\u2019s model quality (case 17), confirming its broad applicability."", ""Claim34: Tab. 4 case 1 to 6 results show that 4 CL metrics provide better model quality than baseline, and the (5)CL_seqtru_voc provides the best quality."", ""Claim35: Results show that the GLUE score drops from 87.29 to 87.19 when baseline use less data (case 1, 9)."", ""Claim36: On the other hand, our CL solution (case 10) with 67% data is able to achieve on-par GLUE score as baseline with 100% data, achieving a 1.5x data and time saving."", ""Claim37: In terms of data saving random-LTD performs better than CL: it is able to achieve better GLUE score even with 2x less data than baseline (case 14), greatly surpassing the 1.33x data saving by the state-of-the-art TokenBypass method."", ""Claim38: However, even with this overhead random-LTD is still a more data/time-efficient solution than baseline/TokenBypass."", ""Claim39: At 50% data, the composed solution further improves the GLUE score from the CL/random-LTD-only cases (case 15), achieving a 2x data and 1.8x time saving while maintaining the GLUE score compared to baseline."", ""Claim40: To verify the effectiveness of the proposed work on small-scale tasks, we apply our techniques to PTB finetuning task for an already-pretrained GPT-2 350M model checkpoint from Huggingface."", ""Claim41: seqres provides the best model quality among the 5 CL metrics (case 3), unlike the two pretraining tasks where the seqtru_voc is the best metric."", ""Claim42: Applying random-LTD also improves the model quality (case 7)."", ""Claim43: Both CL best metric and random-LTD are able to surpass baseline on all 16 combinations of their hyperparameters, demonstrating that they are not sensitive to the hyperparameter choices."", ""Claim44: The composed CL+random-LTD case (case 8) further improves model quality from random-LTD-only case, but is only on-par with CL-only case."", ""Claim45: As presented in Tab. 6, results show that random-LTD is able to achieve 1.3-1.4x data savings while maintaining the model quality, demonstrating its broad applicability."", ""Claim46: In this work we propose the XYZ Data Efficiency framework, which demonstrate the power of composing 2 novel data efficiency techniques together."", ""Claim47: This enables us to achieve an up 12.5x data/time/cost saving (from $46.3K to $3.7K on Azure) while maintaining 95% of model quality for GPT-3 pretraining, an up to 2x saving for GPT-3 and BERT pretraining while maintaining 100% model quality, or to achieve even better model quality under similar data and cost."", ""Claim48: XYZ Data Efficiency is easy to use and tune, which enables us to apply it and verify the benefit on additional GPT-3 MoE pretraining and GPT-2/ViT finetuning tasks.""], ""review"": ""Review: Weakness: * Missing the full term of \u201cCL\u201d in the introduction section (line 44). The first explanation shows in line 64.\n\n* The author argues that the previous methods require changing the data loader, data sampler etc. However, the proposed method still needs to change then as well.\n\n* Besides the TokenBypass, there are also several data routing techniques for foundation model training. E.g., [1] [2]. The author should also include those works for discussion and comparison.\n\n[1] EViT: Expediting Vision Transformers via Token Reorganizations. ICLR 2022\n\n[2] Peeling the Onion: Hierarchical Reduction of Data Redundancy for Efficient Vision. AAAI 2023\n\n* I think the previous work [2] also explores the efficiency at the data sample-level and data routing level. So, it is probably not appropriate to claim the proposed method is \u201cthe first to demonstrate that composing data sampling and routing techniques can lead to even better data/training efficiency \u2026\u201d\n\n* As the author claims the proposed framework is easy-to-use and admits being open-sourced as one of the contributions of this work, it would be better if the author could submit the anonymous code with the supplementary materials.\n\n* Though the proposed method can achieve considerable overall acceleration, it would be great if the author can provide a discussion about the overhead of data sampling and routing part.\n\nQuestions: Please refer to the weakness part.\n\n""}, ""review"": ""Title: XYZ Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing\nAbstract: Recent advances on deep learning models come at the price of formidable training cost. The increasing model size is one of the root causes, but another less-emphasized fact is that data scale is actually increasing at a similar speed as model scale, and the training cost is proportional to both of them. Compared to the rapidly evolving model architecture, how to efficiently use the training data (especially for the expensive foundation model pretraining) is both less explored and difficult to realize due to the lack of a convenient framework that focus on data efficiency capabilities. To this end, we present XYZ Data Efficiency, a framework that makes better use of data, increases training efficiency, and improves model quality. Specifically, we propose and combine two data efficiency techniques: efficient data sampling via a general curriculum learning library, and efficient data routing via a novel random layerwise token dropping technique. For GPT-3 1.3B language model pretraining, our work achieves 12.5x less data/time/cost (`$`3.7K if rent on Azure), while still maintaining 95% of model quality compared to baseline with full data and cost (`$`46.3K). For GPT-3 1.3B and BERT-large pretraining, our work can also achieve the same model quality with up to 2x less data/time/cost, or achieve better model quality under same data/time/cost. XYZ Data Efficiency is easy to use and tune, enabling us to easily apply it and verify its benefit on additional tasks including GPT-3 MoE model pretraining and small-scale GPT-2/ViT finetuning.\n\nClaim1: For GPT-3 1.3B language model pretraining, our work achieves 12.5x less data/time/cost (`$`3.7K if rent on Azure), while still maintaining 95% of model quality compared to baseline with full data and cost (`$`46.3K).\nClaim2: For GPT-3 1.3B and BERT-large pretraining, our work can also achieve the same model quality with up to 2x less data/time/cost, or achieve better model quality under same data/time/cost.\nClaim3: XYZ Data Efficiency is easy to use and tune, enabling us to easily apply it and verify its benefit on additional tasks including GPT-3 MoE model pretraining and small-scale GPT-2/ViT finetuning.\nClaim4: For GPT-3 1.3B and BERT-large pretraining, we can also achieve up to 2x data and 2x time saving together with better or similar model quality as compared to the baseline training with full data, greatly surpassing state-of-the-art data efficiency solutions as summarized in Tab. 1.\nClaim5: This enables us to easily apply proposed work and verify its benefits on additional workloads including GPT-3 Mixture-of-Experts ( ) model pretraining and small-scale GPT-2/ViT model finetuning.\nClaim6: Compared to TokenBypass, our random-LTD (1) does not require importance score metric, special token treatment, or the sandwich token dropping rule, which dramatically reduces the manual design effort; (2) has been broadly tested on GPT-3/BERT pretraining tasks and GPT-2/ViT finetuning tasks, providing better data/training efficiency than TokenBypass.\nClaim7: As a result, our experiments on BERT pretraining confirm that random-LTD does not require and won\u2019t benefit from special token treatment used by the TokenBypass work, further reducing the implementation complexity.\nClaim8: While TokenBypass needs to keep half of the layers in full sequence length training, random-LTD has no such limitation.\nClaim9: Our experiments show that keeping the first and last layers in full sequence length training usually leads to better performance since (1) the first layer directly connects to the embedding, and it can help refine the raw feature; (2) directly connected to the final prediction, the last layer provides a feature realignment for all tokens which could improve the model quality.\nClaim10: Our experiments show that MSLG provides better model quality than constant drop schedule under similar data/compute savings.\nClaim11: This token consumption calculation is also critical to the learning rate schedule: previous CL work [29] finds that if a CL technique reduces the number of tokens on certain steps, it is desirable to use a learning rate decay schedule based on consumed tokens instead of consumed steps.\nClaim12: To our knowledge this is the first work to apply such LR schedule to token dropping/data routing techniques, and our experiments show that it does help improving random-LTD\u2019s performance.\nClaim13: And the overall composibility of XYZ Data Efficiency enables us to leverage both data efficiency techniques and achieve even better data and training efficiency (Sec. 4).\nClaim14: For GPT-2 finetuning, given the low training cost we also perform full training of 16 different CL/random-LTD settings which confirm that (1) the low-cost tuning strategy is able to find very good hyperparameters; (2) both CL and random-LTD are not sensitive to hyperparameter choices.\nClaim15: We evaluate XYZ Data Efficiency by GPT-3/GPT-3 MoE/BERT pretraining and GPT-2/ViT finetuning.\nClaim16: Among the 5 CL difficulty metrics we have for GPT-3 model, to find out which metric provides the best model quality we pretrain the model (with 100% data) 5 times (each with 1 CL metric).\nClaim17: As presented in Tab. 3 case 1 to 6, results show that all 5 CL metrics provide better model quality than baseline (except (4)CL_voc\u2019s 0-shot accuracy), and the (5)CL_seqtru_voc provides the best quality.\nClaim18: Results show that the average 0-shot evaluation accuracy drops from 42.5 to 41.9 when baseline use less data (Tab. 3 case 1, 9).\nClaim19: On the other hand, our CL solution (case 10) with 67% data is able to achieve better 0-shot and 10-shot accuracy than baseline with 100% data, achieving a 1.5x data and time saving.\nClaim20: Results show similar benefit as CL: better model quality when using 100% data (Tab. 3 case 7), and 1.5x data/time saving while maintaining model quality (case 11).\nClaim21: Results (case 5, 7, 8) show that using both techniques together further improves the model quality, demonstrating the benefit of composability by our framework.\nClaim22: Results (case 12 to 15) show that the baseline has worse 0-shot and 10-shot evaluation accuracy under 2x less data.\nClaim23: Using CL or random-LTD can only recover part of the accuracy loss.\nClaim24: On the other hand, the composed data efficiency solution is able to achieve the same or better accuracy results as baseline with 100% data, demonstrating a 2x data and 2x time saving.\nClaim25: At the beginning of the training the proposed approach has slower convergence since we focus on easier/simpler data samples (CL) and drop more tokens (random-LTD) at the beginning.\nClaim26: On the other hand, at the later stage of training the proposed approach is able to provide faster convergence speed than baseline.\nClaim27: Our approach with 50% data is able to achieve similar final validation perplexity as baseline with 100% data (while baseline with 50% data cannot).\nClaim28: Our approach with 100% data is able to achieve even better final validation perplexity which leads to the highest model quality.\nClaim29: Results show that our approach provides better model quality at all cost budgets, advancing the whole cost-quality Pareto frontier.\nClaim30: In particular, we achieve up to 12.5x data/time/cost saving while still maintaining 95% of the model quality (zero-shot eval accuracy) compared to the baseline with full data.\nClaim31: Based on measured training time, this would be a cost reduction from $46.3K to $3.7K if renting similar hardware on Azure [2], greatly democratizing research and usage of foundation models.\nClaim32: Results show that MoE model does achieve similar model quality with less training cost (Tab. 3 case 1, 16).\nClaim33: On the other hand, our approach can further improve MoE model\u2019s model quality (case 17), confirming its broad applicability.\nClaim34: Tab. 4 case 1 to 6 results show that 4 CL metrics provide better model quality than baseline, and the (5)CL_seqtru_voc provides the best quality.\nClaim35: Results show that the GLUE score drops from 87.29 to 87.19 when baseline use less data (case 1, 9).\nClaim36: On the other hand, our CL solution (case 10) with 67% data is able to achieve on-par GLUE score as baseline with 100% data, achieving a 1.5x data and time saving.\nClaim37: In terms of data saving random-LTD performs better than CL: it is able to achieve better GLUE score even with 2x less data than baseline (case 14), greatly surpassing the 1.33x data saving by the state-of-the-art TokenBypass method.\nClaim38: However, even with this overhead random-LTD is still a more data/time-efficient solution than baseline/TokenBypass.\nClaim39: At 50% data, the composed solution further improves the GLUE score from the CL/random-LTD-only cases (case 15), achieving a 2x data and 1.8x time saving while maintaining the GLUE score compared to baseline.\nClaim40: To verify the effectiveness of the proposed work on small-scale tasks, we apply our techniques to PTB finetuning task for an already-pretrained GPT-2 350M model checkpoint from Huggingface.\nClaim41: seqres provides the best model quality among the 5 CL metrics (case 3), unlike the two pretraining tasks where the seqtru_voc is the best metric.\nClaim42: Applying random-LTD also improves the model quality (case 7).\nClaim43: Both CL best metric and random-LTD are able to surpass baseline on all 16 combinations of their hyperparameters, demonstrating that they are not sensitive to the hyperparameter choices.\nClaim44: The composed CL+random-LTD case (case 8) further improves model quality from random-LTD-only case, but is only on-par with CL-only case.\nClaim45: As presented in Tab. 6, results show that random-LTD is able to achieve 1.3-1.4x data savings while maintaining the model quality, demonstrating its broad applicability.\nClaim46: In this work we propose the XYZ Data Efficiency framework, which demonstrate the power of composing 2 novel data efficiency techniques together.\nClaim47: This enables us to achieve an up 12.5x data/time/cost saving (from $46.3K to $3.7K on Azure) while maintaining 95% of model quality for GPT-3 pretraining, an up to 2x saving for GPT-3 and BERT pretraining while maintaining 100% model quality, or to achieve even better model quality under similar data and cost.\nClaim48: XYZ Data Efficiency is easy to use and tune, which enables us to apply it and verify the benefit on additional GPT-3 MoE pretraining and GPT-2/ViT finetuning tasks.\n\nReview: Weakness: * Missing the full term of \u201cCL\u201d in the introduction section (line 44). The first explanation shows in line 64.\n\n* The author argues that the previous methods require changing the data loader, data sampler etc. However, the proposed method still needs to change then as well.\n\n* Besides the TokenBypass, there are also several data routing techniques for foundation model training. E.g., [1] [2]. The author should also include those works for discussion and comparison.\n\n[1] EViT: Expediting Vision Transformers via Token Reorganizations. ICLR 2022\n\n[2] Peeling the Onion: Hierarchical Reduction of Data Redundancy for Efficient Vision. AAAI 2023\n\n* I think the previous work [2] also explores the efficiency at the data sample-level and data routing level. So, it is probably not appropriate to claim the proposed method is \u201cthe first to demonstrate that composing data sampling and routing techniques can lead to even better data/training efficiency \u2026\u201d\n\n* As the author claims the proposed framework is easy-to-use and admits being open-sourced as one of the contributions of this work, it would be better if the author could submit the anonymous code with the supplementary materials.\n\n* Though the proposed method can achieve considerable overall acceleration, it would be great if the author can provide a discussion about the overhead of data sampling and routing part.\n\nQuestions: Please refer to the weakness part.\n\n\n\n"", ""id"": ""nkfSodI4ow1"", ""pdf"": ""openreview.net/pdf?id=nkfSodI4ow""}","2"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""My main concern about this paper is the evaluation observations. Specifically, it seems that under lower-budget training settings (e.g., training with less data and training time), the improvement over the baseline actually shrinks. Will this make this method can only be suitable for teams with a large amount of data and computation resources?"", ""Target claims"": [2, 4], ""Reasoning"": ""This span raises concerns about claim-related results under specific conditions, arguing that the claimed efficiency improvements (Claim2, Claim4) may not hold under lower-budget training settings, which questions the broad applicability of the claims about achieving data/time/cost efficiency.""}, {""Weakness span"": ""In the paper, the authors mentioned that previous methods for improving data/training efficiency fail to achieve satisfactory performance under large-scale settings. Is there some numerical evidence for this claim other than the one shown in Table 1?"", ""Target claims"": [], ""Reasoning"": ""This span questions a general statement made in the paper regarding prior methods' inefficiencies at large scale, but it does not directly target the truthfulness or accuracy of any specific claim listed in the paper. Therefore, it is not directly related to any of the numbered claims.""}]}, ""meta"": {""id"": ""nkfSodI4ow"", ""review_idx"": 2, ""title"": ""Title: XYZ Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing\nAbstract: Recent advances on deep learning models come at the price of formidable training cost. The increasing model size is one of the root causes, but another less-emphasized fact is that data scale is actually increasing at a similar speed as model scale, and the training cost is proportional to both of them. Compared to the rapidly evolving model architecture, how to efficiently use the training data (especially for the expensive foundation model pretraining) is both less explored and difficult to realize due to the lack of a convenient framework that focus on data efficiency capabilities. To this end, we present XYZ Data Efficiency, a framework that makes better use of data, increases training efficiency, and improves model quality. Specifically, we propose and combine two data efficiency techniques: efficient data sampling via a general curriculum learning library, and efficient data routing via a novel random layerwise token dropping technique. For GPT-3 1.3B language model pretraining, our work achieves 12.5x less data/time/cost (`$`3.7K if rent on Azure), while still maintaining 95% of model quality compared to baseline with full data and cost (`$`46.3K). For GPT-3 1.3B and BERT-large pretraining, our work can also achieve the same model quality with up to 2x less data/time/cost, or achieve better model quality under same data/time/cost. XYZ Data Efficiency is easy to use and tune, enabling us to easily apply it and verify its benefit on additional tasks including GPT-3 MoE model pretraining and small-scale GPT-2/ViT finetuning.\n\nTLDR:We propose a framework that reduces GPT-3/BERT pretraining and GPT-2/ViT finetuning data and cost with boosted model quality via new innovations on data sampling and data routing.\n\nKeywords:data efficiency, training efficiency, foundation model, pretraining, finetuning, language model, GPT-3, BERT, ViT"", ""claims"": [""Claim1: For GPT-3 1.3B language model pretraining, our work achieves 12.5x less data/time/cost (`$`3.7K if rent on Azure), while still maintaining 95% of model quality compared to baseline with full data and cost (`$`46.3K)."", ""Claim2: For GPT-3 1.3B and BERT-large pretraining, our work can also achieve the same model quality with up to 2x less data/time/cost, or achieve better model quality under same data/time/cost."", ""Claim3: XYZ Data Efficiency is easy to use and tune, enabling us to easily apply it and verify its benefit on additional tasks including GPT-3 MoE model pretraining and small-scale GPT-2/ViT finetuning."", ""Claim4: For GPT-3 1.3B and BERT-large pretraining, we can also achieve up to 2x data and 2x time saving together with better or similar model quality as compared to the baseline training with full data, greatly surpassing state-of-the-art data efficiency solutions as summarized in Tab. 1."", ""Claim5: This enables us to easily apply proposed work and verify its benefits on additional workloads including GPT-3 Mixture-of-Experts ( ) model pretraining and small-scale GPT-2/ViT model finetuning."", ""Claim6: Compared to TokenBypass, our random-LTD (1) does not require importance score metric, special token treatment, or the sandwich token dropping rule, which dramatically reduces the manual design effort; (2) has been broadly tested on GPT-3/BERT pretraining tasks and GPT-2/ViT finetuning tasks, providing better data/training efficiency than TokenBypass."", ""Claim7: As a result, our experiments on BERT pretraining confirm that random-LTD does not require and won\u2019t benefit from special token treatment used by the TokenBypass work, further reducing the implementation complexity."", ""Claim8: While TokenBypass needs to keep half of the layers in full sequence length training, random-LTD has no such limitation."", ""Claim9: Our experiments show that keeping the first and last layers in full sequence length training usually leads to better performance since (1) the first layer directly connects to the embedding, and it can help refine the raw feature; (2) directly connected to the final prediction, the last layer provides a feature realignment for all tokens which could improve the model quality."", ""Claim10: Our experiments show that MSLG provides better model quality than constant drop schedule under similar data/compute savings."", ""Claim11: This token consumption calculation is also critical to the learning rate schedule: previous CL work [29] finds that if a CL technique reduces the number of tokens on certain steps, it is desirable to use a learning rate decay schedule based on consumed tokens instead of consumed steps."", ""Claim12: To our knowledge this is the first work to apply such LR schedule to token dropping/data routing techniques, and our experiments show that it does help improving random-LTD\u2019s performance."", ""Claim13: And the overall composibility of XYZ Data Efficiency enables us to leverage both data efficiency techniques and achieve even better data and training efficiency (Sec. 4)."", ""Claim14: For GPT-2 finetuning, given the low training cost we also perform full training of 16 different CL/random-LTD settings which confirm that (1) the low-cost tuning strategy is able to find very good hyperparameters; (2) both CL and random-LTD are not sensitive to hyperparameter choices."", ""Claim15: We evaluate XYZ Data Efficiency by GPT-3/GPT-3 MoE/BERT pretraining and GPT-2/ViT finetuning."", ""Claim16: Among the 5 CL difficulty metrics we have for GPT-3 model, to find out which metric provides the best model quality we pretrain the model (with 100% data) 5 times (each with 1 CL metric)."", ""Claim17: As presented in Tab. 3 case 1 to 6, results show that all 5 CL metrics provide better model quality than baseline (except (4)CL_voc\u2019s 0-shot accuracy), and the (5)CL_seqtru_voc provides the best quality."", ""Claim18: Results show that the average 0-shot evaluation accuracy drops from 42.5 to 41.9 when baseline use less data (Tab. 3 case 1, 9)."", ""Claim19: On the other hand, our CL solution (case 10) with 67% data is able to achieve better 0-shot and 10-shot accuracy than baseline with 100% data, achieving a 1.5x data and time saving."", ""Claim20: Results show similar benefit as CL: better model quality when using 100% data (Tab. 3 case 7), and 1.5x data/time saving while maintaining model quality (case 11)."", ""Claim21: Results (case 5, 7, 8) show that using both techniques together further improves the model quality, demonstrating the benefit of composability by our framework."", ""Claim22: Results (case 12 to 15) show that the baseline has worse 0-shot and 10-shot evaluation accuracy under 2x less data."", ""Claim23: Using CL or random-LTD can only recover part of the accuracy loss."", ""Claim24: On the other hand, the composed data efficiency solution is able to achieve the same or better accuracy results as baseline with 100% data, demonstrating a 2x data and 2x time saving."", ""Claim25: At the beginning of the training the proposed approach has slower convergence since we focus on easier/simpler data samples (CL) and drop more tokens (random-LTD) at the beginning."", ""Claim26: On the other hand, at the later stage of training the proposed approach is able to provide faster convergence speed than baseline."", ""Claim27: Our approach with 50% data is able to achieve similar final validation perplexity as baseline with 100% data (while baseline with 50% data cannot)."", ""Claim28: Our approach with 100% data is able to achieve even better final validation perplexity which leads to the highest model quality."", ""Claim29: Results show that our approach provides better model quality at all cost budgets, advancing the whole cost-quality Pareto frontier."", ""Claim30: In particular, we achieve up to 12.5x data/time/cost saving while still maintaining 95% of the model quality (zero-shot eval accuracy) compared to the baseline with full data."", ""Claim31: Based on measured training time, this would be a cost reduction from $46.3K to $3.7K if renting similar hardware on Azure [2], greatly democratizing research and usage of foundation models."", ""Claim32: Results show that MoE model does achieve similar model quality with less training cost (Tab. 3 case 1, 16)."", ""Claim33: On the other hand, our approach can further improve MoE model\u2019s model quality (case 17), confirming its broad applicability."", ""Claim34: Tab. 4 case 1 to 6 results show that 4 CL metrics provide better model quality than baseline, and the (5)CL_seqtru_voc provides the best quality."", ""Claim35: Results show that the GLUE score drops from 87.29 to 87.19 when baseline use less data (case 1, 9)."", ""Claim36: On the other hand, our CL solution (case 10) with 67% data is able to achieve on-par GLUE score as baseline with 100% data, achieving a 1.5x data and time saving."", ""Claim37: In terms of data saving random-LTD performs better than CL: it is able to achieve better GLUE score even with 2x less data than baseline (case 14), greatly surpassing the 1.33x data saving by the state-of-the-art TokenBypass method."", ""Claim38: However, even with this overhead random-LTD is still a more data/time-efficient solution than baseline/TokenBypass."", ""Claim39: At 50% data, the composed solution further improves the GLUE score from the CL/random-LTD-only cases (case 15), achieving a 2x data and 1.8x time saving while maintaining the GLUE score compared to baseline."", ""Claim40: To verify the effectiveness of the proposed work on small-scale tasks, we apply our techniques to PTB finetuning task for an already-pretrained GPT-2 350M model checkpoint from Huggingface."", ""Claim41: seqres provides the best model quality among the 5 CL metrics (case 3), unlike the two pretraining tasks where the seqtru_voc is the best metric."", ""Claim42: Applying random-LTD also improves the model quality (case 7)."", ""Claim43: Both CL best metric and random-LTD are able to surpass baseline on all 16 combinations of their hyperparameters, demonstrating that they are not sensitive to the hyperparameter choices."", ""Claim44: The composed CL+random-LTD case (case 8) further improves model quality from random-LTD-only case, but is only on-par with CL-only case."", ""Claim45: As presented in Tab. 6, results show that random-LTD is able to achieve 1.3-1.4x data savings while maintaining the model quality, demonstrating its broad applicability."", ""Claim46: In this work we propose the XYZ Data Efficiency framework, which demonstrate the power of composing 2 novel data efficiency techniques together."", ""Claim47: This enables us to achieve an up 12.5x data/time/cost saving (from $46.3K to $3.7K on Azure) while maintaining 95% of model quality for GPT-3 pretraining, an up to 2x saving for GPT-3 and BERT pretraining while maintaining 100% model quality, or to achieve even better model quality under similar data and cost."", ""Claim48: XYZ Data Efficiency is easy to use and tune, which enables us to apply it and verify the benefit on additional GPT-3 MoE pretraining and GPT-2/ViT finetuning tasks.""], ""review"": ""Review: Weakness: - My main concern about this paper is the evaluation observations. Specifically, it seems that under lower-budget training settings (e.g., training with less data and training time), the improvement over the baseline actually shrinks. Will this make this method can only be suitable for teams with a large amount of data and computation resources?\n\n- In the paper, the authors mentioned that previous methods for improving data/training efficiency fail to achieve satisfactory performance under large-scale settings. Is there some numerical evidence for this claim other than the one shown in Table 1? \nQuestions: Please refer to weakness. \n\n""}, ""review"": ""Title: XYZ Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing\nAbstract: Recent advances on deep learning models come at the price of formidable training cost. The increasing model size is one of the root causes, but another less-emphasized fact is that data scale is actually increasing at a similar speed as model scale, and the training cost is proportional to both of them. Compared to the rapidly evolving model architecture, how to efficiently use the training data (especially for the expensive foundation model pretraining) is both less explored and difficult to realize due to the lack of a convenient framework that focus on data efficiency capabilities. To this end, we present XYZ Data Efficiency, a framework that makes better use of data, increases training efficiency, and improves model quality. Specifically, we propose and combine two data efficiency techniques: efficient data sampling via a general curriculum learning library, and efficient data routing via a novel random layerwise token dropping technique. For GPT-3 1.3B language model pretraining, our work achieves 12.5x less data/time/cost (`$`3.7K if rent on Azure), while still maintaining 95% of model quality compared to baseline with full data and cost (`$`46.3K). For GPT-3 1.3B and BERT-large pretraining, our work can also achieve the same model quality with up to 2x less data/time/cost, or achieve better model quality under same data/time/cost. XYZ Data Efficiency is easy to use and tune, enabling us to easily apply it and verify its benefit on additional tasks including GPT-3 MoE model pretraining and small-scale GPT-2/ViT finetuning.\n\nClaim1: For GPT-3 1.3B language model pretraining, our work achieves 12.5x less data/time/cost (`$`3.7K if rent on Azure), while still maintaining 95% of model quality compared to baseline with full data and cost (`$`46.3K).\nClaim2: For GPT-3 1.3B and BERT-large pretraining, our work can also achieve the same model quality with up to 2x less data/time/cost, or achieve better model quality under same data/time/cost.\nClaim3: XYZ Data Efficiency is easy to use and tune, enabling us to easily apply it and verify its benefit on additional tasks including GPT-3 MoE model pretraining and small-scale GPT-2/ViT finetuning.\nClaim4: For GPT-3 1.3B and BERT-large pretraining, we can also achieve up to 2x data and 2x time saving together with better or similar model quality as compared to the baseline training with full data, greatly surpassing state-of-the-art data efficiency solutions as summarized in Tab. 1.\nClaim5: This enables us to easily apply proposed work and verify its benefits on additional workloads including GPT-3 Mixture-of-Experts ( ) model pretraining and small-scale GPT-2/ViT model finetuning.\nClaim6: Compared to TokenBypass, our random-LTD (1) does not require importance score metric, special token treatment, or the sandwich token dropping rule, which dramatically reduces the manual design effort; (2) has been broadly tested on GPT-3/BERT pretraining tasks and GPT-2/ViT finetuning tasks, providing better data/training efficiency than TokenBypass.\nClaim7: As a result, our experiments on BERT pretraining confirm that random-LTD does not require and won\u2019t benefit from special token treatment used by the TokenBypass work, further reducing the implementation complexity.\nClaim8: While TokenBypass needs to keep half of the layers in full sequence length training, random-LTD has no such limitation.\nClaim9: Our experiments show that keeping the first and last layers in full sequence length training usually leads to better performance since (1) the first layer directly connects to the embedding, and it can help refine the raw feature; (2) directly connected to the final prediction, the last layer provides a feature realignment for all tokens which could improve the model quality.\nClaim10: Our experiments show that MSLG provides better model quality than constant drop schedule under similar data/compute savings.\nClaim11: This token consumption calculation is also critical to the learning rate schedule: previous CL work [29] finds that if a CL technique reduces the number of tokens on certain steps, it is desirable to use a learning rate decay schedule based on consumed tokens instead of consumed steps.\nClaim12: To our knowledge this is the first work to apply such LR schedule to token dropping/data routing techniques, and our experiments show that it does help improving random-LTD\u2019s performance.\nClaim13: And the overall composibility of XYZ Data Efficiency enables us to leverage both data efficiency techniques and achieve even better data and training efficiency (Sec. 4).\nClaim14: For GPT-2 finetuning, given the low training cost we also perform full training of 16 different CL/random-LTD settings which confirm that (1) the low-cost tuning strategy is able to find very good hyperparameters; (2) both CL and random-LTD are not sensitive to hyperparameter choices.\nClaim15: We evaluate XYZ Data Efficiency by GPT-3/GPT-3 MoE/BERT pretraining and GPT-2/ViT finetuning.\nClaim16: Among the 5 CL difficulty metrics we have for GPT-3 model, to find out which metric provides the best model quality we pretrain the model (with 100% data) 5 times (each with 1 CL metric).\nClaim17: As presented in Tab. 3 case 1 to 6, results show that all 5 CL metrics provide better model quality than baseline (except (4)CL_voc\u2019s 0-shot accuracy), and the (5)CL_seqtru_voc provides the best quality.\nClaim18: Results show that the average 0-shot evaluation accuracy drops from 42.5 to 41.9 when baseline use less data (Tab. 3 case 1, 9).\nClaim19: On the other hand, our CL solution (case 10) with 67% data is able to achieve better 0-shot and 10-shot accuracy than baseline with 100% data, achieving a 1.5x data and time saving.\nClaim20: Results show similar benefit as CL: better model quality when using 100% data (Tab. 3 case 7), and 1.5x data/time saving while maintaining model quality (case 11).\nClaim21: Results (case 5, 7, 8) show that using both techniques together further improves the model quality, demonstrating the benefit of composability by our framework.\nClaim22: Results (case 12 to 15) show that the baseline has worse 0-shot and 10-shot evaluation accuracy under 2x less data.\nClaim23: Using CL or random-LTD can only recover part of the accuracy loss.\nClaim24: On the other hand, the composed data efficiency solution is able to achieve the same or better accuracy results as baseline with 100% data, demonstrating a 2x data and 2x time saving.\nClaim25: At the beginning of the training the proposed approach has slower convergence since we focus on easier/simpler data samples (CL) and drop more tokens (random-LTD) at the beginning.\nClaim26: On the other hand, at the later stage of training the proposed approach is able to provide faster convergence speed than baseline.\nClaim27: Our approach with 50% data is able to achieve similar final validation perplexity as baseline with 100% data (while baseline with 50% data cannot).\nClaim28: Our approach with 100% data is able to achieve even better final validation perplexity which leads to the highest model quality.\nClaim29: Results show that our approach provides better model quality at all cost budgets, advancing the whole cost-quality Pareto frontier.\nClaim30: In particular, we achieve up to 12.5x data/time/cost saving while still maintaining 95% of the model quality (zero-shot eval accuracy) compared to the baseline with full data.\nClaim31: Based on measured training time, this would be a cost reduction from $46.3K to $3.7K if renting similar hardware on Azure [2], greatly democratizing research and usage of foundation models.\nClaim32: Results show that MoE model does achieve similar model quality with less training cost (Tab. 3 case 1, 16).\nClaim33: On the other hand, our approach can further improve MoE model\u2019s model quality (case 17), confirming its broad applicability.\nClaim34: Tab. 4 case 1 to 6 results show that 4 CL metrics provide better model quality than baseline, and the (5)CL_seqtru_voc provides the best quality.\nClaim35: Results show that the GLUE score drops from 87.29 to 87.19 when baseline use less data (case 1, 9).\nClaim36: On the other hand, our CL solution (case 10) with 67% data is able to achieve on-par GLUE score as baseline with 100% data, achieving a 1.5x data and time saving.\nClaim37: In terms of data saving random-LTD performs better than CL: it is able to achieve better GLUE score even with 2x less data than baseline (case 14), greatly surpassing the 1.33x data saving by the state-of-the-art TokenBypass method.\nClaim38: However, even with this overhead random-LTD is still a more data/time-efficient solution than baseline/TokenBypass.\nClaim39: At 50% data, the composed solution further improves the GLUE score from the CL/random-LTD-only cases (case 15), achieving a 2x data and 1.8x time saving while maintaining the GLUE score compared to baseline.\nClaim40: To verify the effectiveness of the proposed work on small-scale tasks, we apply our techniques to PTB finetuning task for an already-pretrained GPT-2 350M model checkpoint from Huggingface.\nClaim41: seqres provides the best model quality among the 5 CL metrics (case 3), unlike the two pretraining tasks where the seqtru_voc is the best metric.\nClaim42: Applying random-LTD also improves the model quality (case 7).\nClaim43: Both CL best metric and random-LTD are able to surpass baseline on all 16 combinations of their hyperparameters, demonstrating that they are not sensitive to the hyperparameter choices.\nClaim44: The composed CL+random-LTD case (case 8) further improves model quality from random-LTD-only case, but is only on-par with CL-only case.\nClaim45: As presented in Tab. 6, results show that random-LTD is able to achieve 1.3-1.4x data savings while maintaining the model quality, demonstrating its broad applicability.\nClaim46: In this work we propose the XYZ Data Efficiency framework, which demonstrate the power of composing 2 novel data efficiency techniques together.\nClaim47: This enables us to achieve an up 12.5x data/time/cost saving (from $46.3K to $3.7K on Azure) while maintaining 95% of model quality for GPT-3 pretraining, an up to 2x saving for GPT-3 and BERT pretraining while maintaining 100% model quality, or to achieve even better model quality under similar data and cost.\nClaim48: XYZ Data Efficiency is easy to use and tune, which enables us to apply it and verify the benefit on additional GPT-3 MoE pretraining and GPT-2/ViT finetuning tasks.\n\nReview: Weakness: - My main concern about this paper is the evaluation observations. Specifically, it seems that under lower-budget training settings (e.g., training with less data and training time), the improvement over the baseline actually shrinks. Will this make this method can only be suitable for teams with a large amount of data and computation resources?\n\n- In the paper, the authors mentioned that previous methods for improving data/training efficiency fail to achieve satisfactory performance under large-scale settings. Is there some numerical evidence for this claim other than the one shown in Table 1? \nQuestions: Please refer to weakness. \n\n\n\n"", ""id"": ""nkfSodI4ow2"", ""pdf"": ""openreview.net/pdf?id=nkfSodI4ow""}","3"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""One notable weakness of the proposed framework is its relatively limited performance compared to the baseline when operating at a smaller data scale, as indicated in Figure 6. Further investigation and clarity on the factors contributing to this limitation would be valuable for understanding the framework's practical applicability across various data scales."", ""Target claims"": [], ""Reasoning"": ""This span indicates a weakness related to the performance of the proposed framework at a smaller data scale, which might suggest that the claims about efficiency and performance improvements (e.g., Claim1, Claim2) may not hold under all conditions, specifically with smaller data scales. However, the review does not explicitly question the validity of any claim regarding specific metrics or improvements achieved, hence no specific claims can be directly identified as targets.""}, {""Weakness span"": ""Despite claims of open-sourcing the XYZ framework, anonymized code or a link to access the implementation is not provided."", ""Target claims"": [], ""Reasoning"": ""This comment does not directly relate to the validity or accuracy of any of the specific claims extracted from the paper. It highlights a procedural weakness rather than questioning the claims' scientific merit, accuracy, or evidence support, and hence no claims are identified as targets.""}, {""Weakness span"": ""The paper does not explicitly address the limitations of their proposed framework or discuss any potential negative societal impact."", ""Target claims"": [], ""Reasoning"": ""This comment aims at discussing the completeness of the paper in terms of addressing limitations and societal impact but does not question any specific technical claims or evidence provided. Therefore, no claims are directly targeted by this weakness.""}]}, ""meta"": {""id"": ""nkfSodI4ow"", ""review_idx"": 3, ""title"": ""Title: XYZ Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing\nAbstract: Recent advances on deep learning models come at the price of formidable training cost. The increasing model size is one of the root causes, but another less-emphasized fact is that data scale is actually increasing at a similar speed as model scale, and the training cost is proportional to both of them. Compared to the rapidly evolving model architecture, how to efficiently use the training data (especially for the expensive foundation model pretraining) is both less explored and difficult to realize due to the lack of a convenient framework that focus on data efficiency capabilities. To this end, we present XYZ Data Efficiency, a framework that makes better use of data, increases training efficiency, and improves model quality. Specifically, we propose and combine two data efficiency techniques: efficient data sampling via a general curriculum learning library, and efficient data routing via a novel random layerwise token dropping technique. For GPT-3 1.3B language model pretraining, our work achieves 12.5x less data/time/cost (`$`3.7K if rent on Azure), while still maintaining 95% of model quality compared to baseline with full data and cost (`$`46.3K). For GPT-3 1.3B and BERT-large pretraining, our work can also achieve the same model quality with up to 2x less data/time/cost, or achieve better model quality under same data/time/cost. XYZ Data Efficiency is easy to use and tune, enabling us to easily apply it and verify its benefit on additional tasks including GPT-3 MoE model pretraining and small-scale GPT-2/ViT finetuning.\n\nTLDR:We propose a framework that reduces GPT-3/BERT pretraining and GPT-2/ViT finetuning data and cost with boosted model quality via new innovations on data sampling and data routing.\n\nKeywords:data efficiency, training efficiency, foundation model, pretraining, finetuning, language model, GPT-3, BERT, ViT"", ""claims"": [""Claim1: For GPT-3 1.3B language model pretraining, our work achieves 12.5x less data/time/cost (`$`3.7K if rent on Azure), while still maintaining 95% of model quality compared to baseline with full data and cost (`$`46.3K)."", ""Claim2: For GPT-3 1.3B and BERT-large pretraining, our work can also achieve the same model quality with up to 2x less data/time/cost, or achieve better model quality under same data/time/cost."", ""Claim3: XYZ Data Efficiency is easy to use and tune, enabling us to easily apply it and verify its benefit on additional tasks including GPT-3 MoE model pretraining and small-scale GPT-2/ViT finetuning."", ""Claim4: For GPT-3 1.3B and BERT-large pretraining, we can also achieve up to 2x data and 2x time saving together with better or similar model quality as compared to the baseline training with full data, greatly surpassing state-of-the-art data efficiency solutions as summarized in Tab. 1."", ""Claim5: This enables us to easily apply proposed work and verify its benefits on additional workloads including GPT-3 Mixture-of-Experts ( ) model pretraining and small-scale GPT-2/ViT model finetuning."", ""Claim6: Compared to TokenBypass, our random-LTD (1) does not require importance score metric, special token treatment, or the sandwich token dropping rule, which dramatically reduces the manual design effort; (2) has been broadly tested on GPT-3/BERT pretraining tasks and GPT-2/ViT finetuning tasks, providing better data/training efficiency than TokenBypass."", ""Claim7: As a result, our experiments on BERT pretraining confirm that random-LTD does not require and won\u2019t benefit from special token treatment used by the TokenBypass work, further reducing the implementation complexity."", ""Claim8: While TokenBypass needs to keep half of the layers in full sequence length training, random-LTD has no such limitation."", ""Claim9: Our experiments show that keeping the first and last layers in full sequence length training usually leads to better performance since (1) the first layer directly connects to the embedding, and it can help refine the raw feature; (2) directly connected to the final prediction, the last layer provides a feature realignment for all tokens which could improve the model quality."", ""Claim10: Our experiments show that MSLG provides better model quality than constant drop schedule under similar data/compute savings."", ""Claim11: This token consumption calculation is also critical to the learning rate schedule: previous CL work [29] finds that if a CL technique reduces the number of tokens on certain steps, it is desirable to use a learning rate decay schedule based on consumed tokens instead of consumed steps."", ""Claim12: To our knowledge this is the first work to apply such LR schedule to token dropping/data routing techniques, and our experiments show that it does help improving random-LTD\u2019s performance."", ""Claim13: And the overall composibility of XYZ Data Efficiency enables us to leverage both data efficiency techniques and achieve even better data and training efficiency (Sec. 4)."", ""Claim14: For GPT-2 finetuning, given the low training cost we also perform full training of 16 different CL/random-LTD settings which confirm that (1) the low-cost tuning strategy is able to find very good hyperparameters; (2) both CL and random-LTD are not sensitive to hyperparameter choices."", ""Claim15: We evaluate XYZ Data Efficiency by GPT-3/GPT-3 MoE/BERT pretraining and GPT-2/ViT finetuning."", ""Claim16: Among the 5 CL difficulty metrics we have for GPT-3 model, to find out which metric provides the best model quality we pretrain the model (with 100% data) 5 times (each with 1 CL metric)."", ""Claim17: As presented in Tab. 3 case 1 to 6, results show that all 5 CL metrics provide better model quality than baseline (except (4)CL_voc\u2019s 0-shot accuracy), and the (5)CL_seqtru_voc provides the best quality."", ""Claim18: Results show that the average 0-shot evaluation accuracy drops from 42.5 to 41.9 when baseline use less data (Tab. 3 case 1, 9)."", ""Claim19: On the other hand, our CL solution (case 10) with 67% data is able to achieve better 0-shot and 10-shot accuracy than baseline with 100% data, achieving a 1.5x data and time saving."", ""Claim20: Results show similar benefit as CL: better model quality when using 100% data (Tab. 3 case 7), and 1.5x data/time saving while maintaining model quality (case 11)."", ""Claim21: Results (case 5, 7, 8) show that using both techniques together further improves the model quality, demonstrating the benefit of composability by our framework."", ""Claim22: Results (case 12 to 15) show that the baseline has worse 0-shot and 10-shot evaluation accuracy under 2x less data."", ""Claim23: Using CL or random-LTD can only recover part of the accuracy loss."", ""Claim24: On the other hand, the composed data efficiency solution is able to achieve the same or better accuracy results as baseline with 100% data, demonstrating a 2x data and 2x time saving."", ""Claim25: At the beginning of the training the proposed approach has slower convergence since we focus on easier/simpler data samples (CL) and drop more tokens (random-LTD) at the beginning."", ""Claim26: On the other hand, at the later stage of training the proposed approach is able to provide faster convergence speed than baseline."", ""Claim27: Our approach with 50% data is able to achieve similar final validation perplexity as baseline with 100% data (while baseline with 50% data cannot)."", ""Claim28: Our approach with 100% data is able to achieve even better final validation perplexity which leads to the highest model quality."", ""Claim29: Results show that our approach provides better model quality at all cost budgets, advancing the whole cost-quality Pareto frontier."", ""Claim30: In particular, we achieve up to 12.5x data/time/cost saving while still maintaining 95% of the model quality (zero-shot eval accuracy) compared to the baseline with full data."", ""Claim31: Based on measured training time, this would be a cost reduction from $46.3K to $3.7K if renting similar hardware on Azure [2], greatly democratizing research and usage of foundation models."", ""Claim32: Results show that MoE model does achieve similar model quality with less training cost (Tab. 3 case 1, 16)."", ""Claim33: On the other hand, our approach can further improve MoE model\u2019s model quality (case 17), confirming its broad applicability."", ""Claim34: Tab. 4 case 1 to 6 results show that 4 CL metrics provide better model quality than baseline, and the (5)CL_seqtru_voc provides the best quality."", ""Claim35: Results show that the GLUE score drops from 87.29 to 87.19 when baseline use less data (case 1, 9)."", ""Claim36: On the other hand, our CL solution (case 10) with 67% data is able to achieve on-par GLUE score as baseline with 100% data, achieving a 1.5x data and time saving."", ""Claim37: In terms of data saving random-LTD performs better than CL: it is able to achieve better GLUE score even with 2x less data than baseline (case 14), greatly surpassing the 1.33x data saving by the state-of-the-art TokenBypass method."", ""Claim38: However, even with this overhead random-LTD is still a more data/time-efficient solution than baseline/TokenBypass."", ""Claim39: At 50% data, the composed solution further improves the GLUE score from the CL/random-LTD-only cases (case 15), achieving a 2x data and 1.8x time saving while maintaining the GLUE score compared to baseline."", ""Claim40: To verify the effectiveness of the proposed work on small-scale tasks, we apply our techniques to PTB finetuning task for an already-pretrained GPT-2 350M model checkpoint from Huggingface."", ""Claim41: seqres provides the best model quality among the 5 CL metrics (case 3), unlike the two pretraining tasks where the seqtru_voc is the best metric."", ""Claim42: Applying random-LTD also improves the model quality (case 7)."", ""Claim43: Both CL best metric and random-LTD are able to surpass baseline on all 16 combinations of their hyperparameters, demonstrating that they are not sensitive to the hyperparameter choices."", ""Claim44: The composed CL+random-LTD case (case 8) further improves model quality from random-LTD-only case, but is only on-par with CL-only case."", ""Claim45: As presented in Tab. 6, results show that random-LTD is able to achieve 1.3-1.4x data savings while maintaining the model quality, demonstrating its broad applicability."", ""Claim46: In this work we propose the XYZ Data Efficiency framework, which demonstrate the power of composing 2 novel data efficiency techniques together."", ""Claim47: This enables us to achieve an up 12.5x data/time/cost saving (from $46.3K to $3.7K on Azure) while maintaining 95% of model quality for GPT-3 pretraining, an up to 2x saving for GPT-3 and BERT pretraining while maintaining 100% model quality, or to achieve even better model quality under similar data and cost."", ""Claim48: XYZ Data Efficiency is easy to use and tune, which enables us to apply it and verify the benefit on additional GPT-3 MoE pretraining and GPT-2/ViT finetuning tasks.""], ""review"": ""Review: Weakness: 1. One notable weakness of the proposed framework is its relatively limited performance compared to the baseline when operating at a smaller data scale, as indicated in Figure 6. Further investigation and clarity on the factors contributing to this limitation would be valuable for understanding the framework's practical applicability across various data scales.\n2. Despite claims of open-sourcing the XYZ framework, anonymized code or a link to access the implementation is not provided. \n3. The paper does not explicitly address the limitations of their proposed framework or discuss any potential negative societal impact.\nQuestions: 1. How does the proposed XYZ framework compare to the baseline when pre-training GPT-3 1.3B on a data set of 75B tokens?\n2. Given the observed slower convergence of XYZ compared to the baseline, what are the underlying factors contributing to this behavior? Specifically, does the convergence difference primarily result from the data sampling methodology (curriculum learning - CL) or the token dropping technique (random layer-wise token dropping - rLTD)?\n\n""}, ""review"": ""Title: XYZ Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing\nAbstract: Recent advances on deep learning models come at the price of formidable training cost. The increasing model size is one of the root causes, but another less-emphasized fact is that data scale is actually increasing at a similar speed as model scale, and the training cost is proportional to both of them. Compared to the rapidly evolving model architecture, how to efficiently use the training data (especially for the expensive foundation model pretraining) is both less explored and difficult to realize due to the lack of a convenient framework that focus on data efficiency capabilities. To this end, we present XYZ Data Efficiency, a framework that makes better use of data, increases training efficiency, and improves model quality. Specifically, we propose and combine two data efficiency techniques: efficient data sampling via a general curriculum learning library, and efficient data routing via a novel random layerwise token dropping technique. For GPT-3 1.3B language model pretraining, our work achieves 12.5x less data/time/cost (`$`3.7K if rent on Azure), while still maintaining 95% of model quality compared to baseline with full data and cost (`$`46.3K). For GPT-3 1.3B and BERT-large pretraining, our work can also achieve the same model quality with up to 2x less data/time/cost, or achieve better model quality under same data/time/cost. XYZ Data Efficiency is easy to use and tune, enabling us to easily apply it and verify its benefit on additional tasks including GPT-3 MoE model pretraining and small-scale GPT-2/ViT finetuning.\n\nClaim1: For GPT-3 1.3B language model pretraining, our work achieves 12.5x less data/time/cost (`$`3.7K if rent on Azure), while still maintaining 95% of model quality compared to baseline with full data and cost (`$`46.3K).\nClaim2: For GPT-3 1.3B and BERT-large pretraining, our work can also achieve the same model quality with up to 2x less data/time/cost, or achieve better model quality under same data/time/cost.\nClaim3: XYZ Data Efficiency is easy to use and tune, enabling us to easily apply it and verify its benefit on additional tasks including GPT-3 MoE model pretraining and small-scale GPT-2/ViT finetuning.\nClaim4: For GPT-3 1.3B and BERT-large pretraining, we can also achieve up to 2x data and 2x time saving together with better or similar model quality as compared to the baseline training with full data, greatly surpassing state-of-the-art data efficiency solutions as summarized in Tab. 1.\nClaim5: This enables us to easily apply proposed work and verify its benefits on additional workloads including GPT-3 Mixture-of-Experts ( ) model pretraining and small-scale GPT-2/ViT model finetuning.\nClaim6: Compared to TokenBypass, our random-LTD (1) does not require importance score metric, special token treatment, or the sandwich token dropping rule, which dramatically reduces the manual design effort; (2) has been broadly tested on GPT-3/BERT pretraining tasks and GPT-2/ViT finetuning tasks, providing better data/training efficiency than TokenBypass.\nClaim7: As a result, our experiments on BERT pretraining confirm that random-LTD does not require and won\u2019t benefit from special token treatment used by the TokenBypass work, further reducing the implementation complexity.\nClaim8: While TokenBypass needs to keep half of the layers in full sequence length training, random-LTD has no such limitation.\nClaim9: Our experiments show that keeping the first and last layers in full sequence length training usually leads to better performance since (1) the first layer directly connects to the embedding, and it can help refine the raw feature; (2) directly connected to the final prediction, the last layer provides a feature realignment for all tokens which could improve the model quality.\nClaim10: Our experiments show that MSLG provides better model quality than constant drop schedule under similar data/compute savings.\nClaim11: This token consumption calculation is also critical to the learning rate schedule: previous CL work [29] finds that if a CL technique reduces the number of tokens on certain steps, it is desirable to use a learning rate decay schedule based on consumed tokens instead of consumed steps.\nClaim12: To our knowledge this is the first work to apply such LR schedule to token dropping/data routing techniques, and our experiments show that it does help improving random-LTD\u2019s performance.\nClaim13: And the overall composibility of XYZ Data Efficiency enables us to leverage both data efficiency techniques and achieve even better data and training efficiency (Sec. 4).\nClaim14: For GPT-2 finetuning, given the low training cost we also perform full training of 16 different CL/random-LTD settings which confirm that (1) the low-cost tuning strategy is able to find very good hyperparameters; (2) both CL and random-LTD are not sensitive to hyperparameter choices.\nClaim15: We evaluate XYZ Data Efficiency by GPT-3/GPT-3 MoE/BERT pretraining and GPT-2/ViT finetuning.\nClaim16: Among the 5 CL difficulty metrics we have for GPT-3 model, to find out which metric provides the best model quality we pretrain the model (with 100% data) 5 times (each with 1 CL metric).\nClaim17: As presented in Tab. 3 case 1 to 6, results show that all 5 CL metrics provide better model quality than baseline (except (4)CL_voc\u2019s 0-shot accuracy), and the (5)CL_seqtru_voc provides the best quality.\nClaim18: Results show that the average 0-shot evaluation accuracy drops from 42.5 to 41.9 when baseline use less data (Tab. 3 case 1, 9).\nClaim19: On the other hand, our CL solution (case 10) with 67% data is able to achieve better 0-shot and 10-shot accuracy than baseline with 100% data, achieving a 1.5x data and time saving.\nClaim20: Results show similar benefit as CL: better model quality when using 100% data (Tab. 3 case 7), and 1.5x data/time saving while maintaining model quality (case 11).\nClaim21: Results (case 5, 7, 8) show that using both techniques together further improves the model quality, demonstrating the benefit of composability by our framework.\nClaim22: Results (case 12 to 15) show that the baseline has worse 0-shot and 10-shot evaluation accuracy under 2x less data.\nClaim23: Using CL or random-LTD can only recover part of the accuracy loss.\nClaim24: On the other hand, the composed data efficiency solution is able to achieve the same or better accuracy results as baseline with 100% data, demonstrating a 2x data and 2x time saving.\nClaim25: At the beginning of the training the proposed approach has slower convergence since we focus on easier/simpler data samples (CL) and drop more tokens (random-LTD) at the beginning.\nClaim26: On the other hand, at the later stage of training the proposed approach is able to provide faster convergence speed than baseline.\nClaim27: Our approach with 50% data is able to achieve similar final validation perplexity as baseline with 100% data (while baseline with 50% data cannot).\nClaim28: Our approach with 100% data is able to achieve even better final validation perplexity which leads to the highest model quality.\nClaim29: Results show that our approach provides better model quality at all cost budgets, advancing the whole cost-quality Pareto frontier.\nClaim30: In particular, we achieve up to 12.5x data/time/cost saving while still maintaining 95% of the model quality (zero-shot eval accuracy) compared to the baseline with full data.\nClaim31: Based on measured training time, this would be a cost reduction from $46.3K to $3.7K if renting similar hardware on Azure [2], greatly democratizing research and usage of foundation models.\nClaim32: Results show that MoE model does achieve similar model quality with less training cost (Tab. 3 case 1, 16).\nClaim33: On the other hand, our approach can further improve MoE model\u2019s model quality (case 17), confirming its broad applicability.\nClaim34: Tab. 4 case 1 to 6 results show that 4 CL metrics provide better model quality than baseline, and the (5)CL_seqtru_voc provides the best quality.\nClaim35: Results show that the GLUE score drops from 87.29 to 87.19 when baseline use less data (case 1, 9).\nClaim36: On the other hand, our CL solution (case 10) with 67% data is able to achieve on-par GLUE score as baseline with 100% data, achieving a 1.5x data and time saving.\nClaim37: In terms of data saving random-LTD performs better than CL: it is able to achieve better GLUE score even with 2x less data than baseline (case 14), greatly surpassing the 1.33x data saving by the state-of-the-art TokenBypass method.\nClaim38: However, even with this overhead random-LTD is still a more data/time-efficient solution than baseline/TokenBypass.\nClaim39: At 50% data, the composed solution further improves the GLUE score from the CL/random-LTD-only cases (case 15), achieving a 2x data and 1.8x time saving while maintaining the GLUE score compared to baseline.\nClaim40: To verify the effectiveness of the proposed work on small-scale tasks, we apply our techniques to PTB finetuning task for an already-pretrained GPT-2 350M model checkpoint from Huggingface.\nClaim41: seqres provides the best model quality among the 5 CL metrics (case 3), unlike the two pretraining tasks where the seqtru_voc is the best metric.\nClaim42: Applying random-LTD also improves the model quality (case 7).\nClaim43: Both CL best metric and random-LTD are able to surpass baseline on all 16 combinations of their hyperparameters, demonstrating that they are not sensitive to the hyperparameter choices.\nClaim44: The composed CL+random-LTD case (case 8) further improves model quality from random-LTD-only case, but is only on-par with CL-only case.\nClaim45: As presented in Tab. 6, results show that random-LTD is able to achieve 1.3-1.4x data savings while maintaining the model quality, demonstrating its broad applicability.\nClaim46: In this work we propose the XYZ Data Efficiency framework, which demonstrate the power of composing 2 novel data efficiency techniques together.\nClaim47: This enables us to achieve an up 12.5x data/time/cost saving (from $46.3K to $3.7K on Azure) while maintaining 95% of model quality for GPT-3 pretraining, an up to 2x saving for GPT-3 and BERT pretraining while maintaining 100% model quality, or to achieve even better model quality under similar data and cost.\nClaim48: XYZ Data Efficiency is easy to use and tune, which enables us to apply it and verify the benefit on additional GPT-3 MoE pretraining and GPT-2/ViT finetuning tasks.\n\nReview: Weakness: 1. One notable weakness of the proposed framework is its relatively limited performance compared to the baseline when operating at a smaller data scale, as indicated in Figure 6. Further investigation and clarity on the factors contributing to this limitation would be valuable for understanding the framework's practical applicability across various data scales.\n2. Despite claims of open-sourcing the XYZ framework, anonymized code or a link to access the implementation is not provided. \n3. The paper does not explicitly address the limitations of their proposed framework or discuss any potential negative societal impact.\nQuestions: 1. How does the proposed XYZ framework compare to the baseline when pre-training GPT-3 1.3B on a data set of 75B tokens?\n2. Given the observed slower convergence of XYZ compared to the baseline, what are the underlying factors contributing to this behavior? Specifically, does the convergence difference primarily result from the data sampling methodology (curriculum learning - CL) or the token dropping technique (random layer-wise token dropping - rLTD)?\n\n\n\n"", ""id"": ""nkfSodI4ow3"", ""pdf"": ""openreview.net/pdf?id=nkfSodI4ow""}","4"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""The analysis of faithfulness and hallucination in the paper is not in-depth enough. There is no metric (either manual or automatic) to quantify faithfulness and hallucinations, and the improvement in BLEU is not sufficient to indicate that the faithfulness issue has been effectively addressed. The few cases presented in the paper are not enough to support the conclusions."", ""Target claims"": [2, 26, 31, 29], ""Reasoning"": ""This weakness questions the claims about the effectiveness of the proposed method in improving faithfulness and addressing hallucinations. Claim2 states that the MonoSLT increases the utilization of visual signals, which suggests enhanced faithfulness, but the review argues that the empirical evidence (BLEU improvement) is insufficient. Claim26 discusses improvements in faithfulness through joint learning, but without adequate metrics or demonstrated cases, this improvement is questioned. Claim31 about improved faithfulness and correct translation is undermined by inadequate evidence and lack of quantification of faithfulness. Claim29 mentions faithfulness despite precise glosses, but the review suggests the few examples provided are not convincing enough.""}, {""Weakness span"": ""The analysis in section 3.2 is not thorough enough, and the conclusions are somewhat forced. For example, the statement that overfitting is caused by faithfulness is not well-supported, and the conclusion that there is no obvious negative correlation between SLT and SLR in Figure 3(c) is due to hallucination lacks data support and quantitative analysis."", ""Target claims"": [10, 12, 11, 13], ""Reasoning"": ""The review critiques the interpretation of convergence behaviors and overfitting related to faithfulness mentioned in Claim10, as well as the lack of negative correlation between SLT and SLR performance as stated in Claim12. It implies that the relationships inferred in Claim11 (overfitting linked to faithfulness) and hallucination in Claim13 (fluency but unrelated output) are speculative without robust data or analysis to back them. Through pointing out the lack of data support for these inferences, the review contests the validity and empirical support of these claims.""}]}, ""meta"": {""id"": ""mWMJN0vbDF"", ""review_idx"": 0, ""title"": ""Title: Towards Faithful Sign Language Translation\nAbstract: Sign language translation (SLT) aims to translate perceived visual signals into spoken language. Recent works have achieved impressive performance by improving visual representations and adopting advanced machine translation techniques, but the faithfulness (\\ie, whether the SLT model captures correct visual signals) in SLT has not received enough attention. In this paper, we explore the association among SLT-relevant tasks and find that the imprecise glosses and limited corpora may hinder faithfulness in SLT. To improve faithfulness in SLT, we first integrate SLT subtasks into a single framework named MonoSLT, which can share the acquired knowledge among SLT subtasks based on their monotonically aligned nature. We further propose two kinds of constraints: the alignment constraint aligns the visual and linguistic embeddings through a sharing translation module and synthetic code-switching corpora; the consistency constraint integrates the advantages of subtasks by regularizing the prediction consistency. Experimental results show that the proposed MonoSLT is competitive against previous SLT methods by increasing the utilization of visual signals, especially when glosses are imprecise.\n\nTLDR:This paper integrates SLT subtasks into a single framework named MonoSLT based on the monotonically aligned nature of SLT subtasks, and adopts two kinds of constraints to further improve the faithfulness of SLT models.\n\nKeywords:Sign Language Recognition; Sign Language Translation;"", ""claims"": [""Claim1: In this paper, we explore the association among SLT-relevant tasks and find that the imprecise glosses and limited corpora may hinder faithfulness in SLT."", ""Claim2: Experimental results show that the proposed MonoSLT is competitive against previous SLT methods by increasing the utilization of visual signals, especially when glosses are imprecise."", ""Claim3: Previous works attempt to improve SLT performance by adopting large-scale pretrained LMs and leveraging extra corpus. These works are developed under the paradigm that \u2018improves G2T first and then transfers to S2T\u2019, which greatly improve S2T performance but inevitably face the hallucination problem (i.e., S2T models tend to generate fluent but inadequate translations), and we attribute this problem to the lack of faithfulness (i.e., the S2T models cannot capture correct visual signals). Besides, the availability of G2T corpora is also the bottleneck for the generalization of the pretrained model."", ""Claim4: In this paper, we attempt to increase the utilization of visual signals in S2T to improve faithfulness, especially when glosses are imprecise."", ""Claim5: Moreover, improving the alignment between visual clips and target words can improve the faithfulness of translation and relieve the hallucination problem."", ""Claim6: Experimental results show that the proposed approach can surpass previous SLT methods on Phoenix14T by increasing the utilization of visual signals."", ""Claim7: Recent empirical results indicate that adopting more accurate SLR models in SLR-pretrained and SLR-supervised solutions often leads to better translation quality, but little work has been done to investigate the association between them."", ""Claim8: Recent works reveals that data scarcity hinders the further development of SLT."", ""Claim9: Our motivation is similar with but we focus more on faithfulness, and leverage the monotonically aligned nature of SLT subtasks to align visual and linguistic embeddings."", ""Claim10: We can observe different convergence behaviors on SLT subtasks: the G2T model converges faster at the beginning and achieves higher performance on the dev set, the S2T model achieves comparable performance on the dev set but tends to overfit the training set, while the V2T model encounters difficulties in converging."", ""Claim11: This observation indicates that the C2T model meets the issue of overfitting before finding the correct visual signals, especially when adopting a powerful translation module, and we identify this issue as the lack of faithfulness."", ""Claim12: It is surprising to observe that there is no significant negative correlation (i.e., achieving higher BLEU scores on the subset with lower WER) between the performance of SLR and SLT, even though lower WER indicates the less accumulated error."", ""Claim13: We analyze results and find that C2T models tend to generate hallucinations, which are fluent but unrelated to source gloss sequences. This is another phenomenon that reflects a lack of faithfulness."", ""Claim14: Based on the above observations, we conclude that enhancing the capability of SLT models to accurately identify visual signals is crucial, which can improve the faithfulness of SLT models."", ""Claim15: We assume imprecise gloss representations may hinder the further development of SLT models, and it is essential to increase the utilization of visual information."", ""Claim16: Different from recent works that explore the use of linguistic information to guide the learning of visual features, we prefer to take advantage of both modalities based on their different characteristics."", ""Claim17: Previous works have shown remarkable success in modeling multi-lingual languages and cross-modal information with a single transformer-based model, which verifies the capability of transformer-based models for aligning multi-source domains."", ""Claim18: The proposed method includes a joint training scheme and two constraints."", ""Claim19: All of the SLT subtasks (V2T, C2T, and G2T) are monotonically aligned and this characteristic of SLT indicates the acquired knowledge about translation can be shared across subtasks, which can not only control model complexity but also reduce overfitting."", ""Claim20: Therefore, we first integrate SLT subtasks into a single framework and learn them jointly."", ""Claim21: Experimental results show that the proposed MonoSLT is competitive against previous SLT methods by increasing the utilization of visual signals, especially when glosses are imprecise."", ""Claim22: As shown in Table 1, we adopt a strong baseline, and the proposed method can bring further improvement (+3.79 BLEU-4)."", ""Claim23: Besides, the proposed MonoSLT is not the best SLR approach, but outperforms the previous SLT method [15] with the best SLR performance by 2.2% (WER: 21.4% vs. 19.3 %, BLEU-4: 31.15 % vs. 28.95%)."", ""Claim24: MonoSLT also surpasses other previous methods with similar SLR performance, which indicates MonoSLT can increase the utilization of visual signals."", ""Claim25: This observation also reveals the lack of faithfulness in recent SLT methods, e.g., TwoStream-SLT [15] with multi-modality inputs (both skeleton sequence and video) achieve much better SLR performance than with skeleton sequence only (WER: 27.14% vs. 17.72%), but it achieves comparable SLT performance (BLEU-4: 28.23% vs. 28.95%) under these two settings."", ""Claim26: Besides, the proposed method improves faithfulness through joint learning and two constraints, which can be applied to any SLR model and has the potential to achieve better translation performance with a more powerful SLR model."", ""Claim27: To show the generalization of the proposed method, we also report relevant performance on CSL-Daily in Table 2."", ""Claim28: The proposed MonoSLT achieves inferior SLR and SLT performance than previous works [12, 15] but is still better than other works."", ""Claim29: Besides, the proposed method achieves better SLT performance than the baseline, which indicates that although glosses are precise intermediate tokenization, the lack of faithfulness still exists."", ""Claim30: It can be observed that part of the translation cannot find corresponding glosses, which indicates glosses are imprecise representations."", ""Claim31: The proposed MonoSLT can improve faithfulness and translate \u2018on the coasts\u2019 correctly."", ""Claim32: We notice that learning V2T and C2T subtasks jointly obtain the most significant improvements (5.72% for V2T and 0.87% for C2T), which shows V2T can achieve comparable results to G2T with proper regularization and the performance of V2T and C2T can be mutually improved."", ""Claim33: Moreover, simply sharing more subtasks can not bring further improvements, which indicates the importance of designing proper solutions to exploit SLT subtasks."", ""Claim34: Both token-wise and sentence-wise code-switching achieve better performance than the best performance of joint learning, and we notice they can also accelerate training process and increase training stability."", ""Claim35: Adopting the cyclical annealing schedule can improve the G2T performance at the cost of a little performance loss of V2T and S2T, and combining it with the consistency constraint can bring further improvement."", ""Claim36: Besides, we can also observe that the proposed method can also improve the performance of G2T, which indicates S2T is also beneficial for G2T, and the previous \u2018G2T first\u2019 paradigm not fully exploits the potential of visual information."", ""Claim37: Although the proposed MonoSLT achieves competitive results on two benchmarks, we notice several limitations of our model."", ""Claim38: Firstly, the proposed method is motivated to solve the hallucination problem of SLT, however, we have not found proper metrics to quantitatively evaluate the faithfulness of SLT models and still use BLEU and ROUGE for evaluation."", ""Claim39: We believe faithfulness is important when SLT is applied in real life because an unfaithful SLT model may produce unexpected consequences."", ""Claim40: Secondly, although the proposed method can improve faithfulness in SLT, as shown in Table. 6, it does not extract new visual features, which indicates that expensive gloss annotations are still essential."", ""Claim41: Thirdly, although we design several approaches to make the training stable, it still encounters difficulties in converging occasionally, and we will continue to enhance its stability."", ""Claim42: Faithfulness is one of the desired criteria to evaluate the applicability of SLT models."", ""Claim43: In this paper, we explore the association among different SLT-relevant tasks and reveal that the lack of faithfulness exists in recent SLT methods."", ""Claim44: To improve faithfulness in SLT, we attempt to increase the utilization of visual signals in SLT and propose a framework named MonoSLT, which leverages the monotonically aligned nature of SLT subtasks to train them jointly."", ""Claim45: We further propose two kinds of constraints to align visual and linguistic embeddings and leverage the advantage of subtasks."", ""Claim46: Experimental results show that the proposed MonoSLT is competitive against previous SLT methods by increasing the utilization of visual signals, especially when glosses are imprecise."", ""Claim47: We hope the proposed method and empirical conclusions can inspire future studies on SLT and relevant tasks.""], ""review"": ""Review: Weakness: 1. There is no comparison with [12,15] on the bug-free dataset, which is a concern. Although I understand that reproducing [15] would require additional effort, since your code is based on MMTLB, it would be reasonable to verify the effectiveness of MMTLAB on the bug-free data.\n\n2. The analysis of faithfulness and hallucination in the paper is not in-depth enough. There is no metric (either manual or automatic) to quantify faithfulness and hallucinations, and the improvement in BLEU is not sufficient to indicate that the faithfulness issue has been effectively addressed. The few cases presented in the paper are not enough to support the conclusions.\n\n3. The analysis in section 3.2 is not thorough enough, and the conclusions are somewhat forced. For example, the statement that overfitting is caused by faithfulness is not well-supported, and the conclusion that there is no obvious negative correlation between SLT and SLR in Figure 3(c) is due to hallucination lacks data support and quantitative analysis. The few examples presented in section 4 are not sufficient to demonstrate the issue of hallucination.\nQuestions: n/a\n\n""}, ""review"": ""Title: Towards Faithful Sign Language Translation\nAbstract: Sign language translation (SLT) aims to translate perceived visual signals into spoken language. Recent works have achieved impressive performance by improving visual representations and adopting advanced machine translation techniques, but the faithfulness (\\ie, whether the SLT model captures correct visual signals) in SLT has not received enough attention. In this paper, we explore the association among SLT-relevant tasks and find that the imprecise glosses and limited corpora may hinder faithfulness in SLT. To improve faithfulness in SLT, we first integrate SLT subtasks into a single framework named MonoSLT, which can share the acquired knowledge among SLT subtasks based on their monotonically aligned nature. We further propose two kinds of constraints: the alignment constraint aligns the visual and linguistic embeddings through a sharing translation module and synthetic code-switching corpora; the consistency constraint integrates the advantages of subtasks by regularizing the prediction consistency. Experimental results show that the proposed MonoSLT is competitive against previous SLT methods by increasing the utilization of visual signals, especially when glosses are imprecise.\n\nClaim1: In this paper, we explore the association among SLT-relevant tasks and find that the imprecise glosses and limited corpora may hinder faithfulness in SLT.\nClaim2: Experimental results show that the proposed MonoSLT is competitive against previous SLT methods by increasing the utilization of visual signals, especially when glosses are imprecise.\nClaim3: Previous works attempt to improve SLT performance by adopting large-scale pretrained LMs and leveraging extra corpus. These works are developed under the paradigm that \u2018improves G2T first and then transfers to S2T\u2019, which greatly improve S2T performance but inevitably face the hallucination problem (i.e., S2T models tend to generate fluent but inadequate translations), and we attribute this problem to the lack of faithfulness (i.e., the S2T models cannot capture correct visual signals). Besides, the availability of G2T corpora is also the bottleneck for the generalization of the pretrained model.\nClaim4: In this paper, we attempt to increase the utilization of visual signals in S2T to improve faithfulness, especially when glosses are imprecise.\nClaim5: Moreover, improving the alignment between visual clips and target words can improve the faithfulness of translation and relieve the hallucination problem.\nClaim6: Experimental results show that the proposed approach can surpass previous SLT methods on Phoenix14T by increasing the utilization of visual signals.\nClaim7: Recent empirical results indicate that adopting more accurate SLR models in SLR-pretrained and SLR-supervised solutions often leads to better translation quality, but little work has been done to investigate the association between them.\nClaim8: Recent works reveals that data scarcity hinders the further development of SLT.\nClaim9: Our motivation is similar with but we focus more on faithfulness, and leverage the monotonically aligned nature of SLT subtasks to align visual and linguistic embeddings.\nClaim10: We can observe different convergence behaviors on SLT subtasks: the G2T model converges faster at the beginning and achieves higher performance on the dev set, the S2T model achieves comparable performance on the dev set but tends to overfit the training set, while the V2T model encounters difficulties in converging.\nClaim11: This observation indicates that the C2T model meets the issue of overfitting before finding the correct visual signals, especially when adopting a powerful translation module, and we identify this issue as the lack of faithfulness.\nClaim12: It is surprising to observe that there is no significant negative correlation (i.e., achieving higher BLEU scores on the subset with lower WER) between the performance of SLR and SLT, even though lower WER indicates the less accumulated error.\nClaim13: We analyze results and find that C2T models tend to generate hallucinations, which are fluent but unrelated to source gloss sequences. This is another phenomenon that reflects a lack of faithfulness.\nClaim14: Based on the above observations, we conclude that enhancing the capability of SLT models to accurately identify visual signals is crucial, which can improve the faithfulness of SLT models.\nClaim15: We assume imprecise gloss representations may hinder the further development of SLT models, and it is essential to increase the utilization of visual information.\nClaim16: Different from recent works that explore the use of linguistic information to guide the learning of visual features, we prefer to take advantage of both modalities based on their different characteristics.\nClaim17: Previous works have shown remarkable success in modeling multi-lingual languages and cross-modal information with a single transformer-based model, which verifies the capability of transformer-based models for aligning multi-source domains.\nClaim18: The proposed method includes a joint training scheme and two constraints.\nClaim19: All of the SLT subtasks (V2T, C2T, and G2T) are monotonically aligned and this characteristic of SLT indicates the acquired knowledge about translation can be shared across subtasks, which can not only control model complexity but also reduce overfitting.\nClaim20: Therefore, we first integrate SLT subtasks into a single framework and learn them jointly.\nClaim21: Experimental results show that the proposed MonoSLT is competitive against previous SLT methods by increasing the utilization of visual signals, especially when glosses are imprecise.\nClaim22: As shown in Table 1, we adopt a strong baseline, and the proposed method can bring further improvement (+3.79 BLEU-4).\nClaim23: Besides, the proposed MonoSLT is not the best SLR approach, but outperforms the previous SLT method [15] with the best SLR performance by 2.2% (WER: 21.4% vs. 19.3 %, BLEU-4: 31.15 % vs. 28.95%).\nClaim24: MonoSLT also surpasses other previous methods with similar SLR performance, which indicates MonoSLT can increase the utilization of visual signals.\nClaim25: This observation also reveals the lack of faithfulness in recent SLT methods, e.g., TwoStream-SLT [15] with multi-modality inputs (both skeleton sequence and video) achieve much better SLR performance than with skeleton sequence only (WER: 27.14% vs. 17.72%), but it achieves comparable SLT performance (BLEU-4: 28.23% vs. 28.95%) under these two settings.\nClaim26: Besides, the proposed method improves faithfulness through joint learning and two constraints, which can be applied to any SLR model and has the potential to achieve better translation performance with a more powerful SLR model.\nClaim27: To show the generalization of the proposed method, we also report relevant performance on CSL-Daily in Table 2.\nClaim28: The proposed MonoSLT achieves inferior SLR and SLT performance than previous works [12, 15] but is still better than other works.\nClaim29: Besides, the proposed method achieves better SLT performance than the baseline, which indicates that although glosses are precise intermediate tokenization, the lack of faithfulness still exists.\nClaim30: It can be observed that part of the translation cannot find corresponding glosses, which indicates glosses are imprecise representations.\nClaim31: The proposed MonoSLT can improve faithfulness and translate \u2018on the coasts\u2019 correctly.\nClaim32: We notice that learning V2T and C2T subtasks jointly obtain the most significant improvements (5.72% for V2T and 0.87% for C2T), which shows V2T can achieve comparable results to G2T with proper regularization and the performance of V2T and C2T can be mutually improved.\nClaim33: Moreover, simply sharing more subtasks can not bring further improvements, which indicates the importance of designing proper solutions to exploit SLT subtasks.\nClaim34: Both token-wise and sentence-wise code-switching achieve better performance than the best performance of joint learning, and we notice they can also accelerate training process and increase training stability.\nClaim35: Adopting the cyclical annealing schedule can improve the G2T performance at the cost of a little performance loss of V2T and S2T, and combining it with the consistency constraint can bring further improvement.\nClaim36: Besides, we can also observe that the proposed method can also improve the performance of G2T, which indicates S2T is also beneficial for G2T, and the previous \u2018G2T first\u2019 paradigm not fully exploits the potential of visual information.\nClaim37: Although the proposed MonoSLT achieves competitive results on two benchmarks, we notice several limitations of our model.\nClaim38: Firstly, the proposed method is motivated to solve the hallucination problem of SLT, however, we have not found proper metrics to quantitatively evaluate the faithfulness of SLT models and still use BLEU and ROUGE for evaluation.\nClaim39: We believe faithfulness is important when SLT is applied in real life because an unfaithful SLT model may produce unexpected consequences.\nClaim40: Secondly, although the proposed method can improve faithfulness in SLT, as shown in Table. 6, it does not extract new visual features, which indicates that expensive gloss annotations are still essential.\nClaim41: Thirdly, although we design several approaches to make the training stable, it still encounters difficulties in converging occasionally, and we will continue to enhance its stability.\nClaim42: Faithfulness is one of the desired criteria to evaluate the applicability of SLT models.\nClaim43: In this paper, we explore the association among different SLT-relevant tasks and reveal that the lack of faithfulness exists in recent SLT methods.\nClaim44: To improve faithfulness in SLT, we attempt to increase the utilization of visual signals in SLT and propose a framework named MonoSLT, which leverages the monotonically aligned nature of SLT subtasks to train them jointly.\nClaim45: We further propose two kinds of constraints to align visual and linguistic embeddings and leverage the advantage of subtasks.\nClaim46: Experimental results show that the proposed MonoSLT is competitive against previous SLT methods by increasing the utilization of visual signals, especially when glosses are imprecise.\nClaim47: We hope the proposed method and empirical conclusions can inspire future studies on SLT and relevant tasks.\n\nReview: Weakness: 1. There is no comparison with [12,15] on the bug-free dataset, which is a concern. Although I understand that reproducing [15] would require additional effort, since your code is based on MMTLB, it would be reasonable to verify the effectiveness of MMTLAB on the bug-free data.\n\n2. The analysis of faithfulness and hallucination in the paper is not in-depth enough. There is no metric (either manual or automatic) to quantify faithfulness and hallucinations, and the improvement in BLEU is not sufficient to indicate that the faithfulness issue has been effectively addressed. The few cases presented in the paper are not enough to support the conclusions.\n\n3. The analysis in section 3.2 is not thorough enough, and the conclusions are somewhat forced. For example, the statement that overfitting is caused by faithfulness is not well-supported, and the conclusion that there is no obvious negative correlation between SLT and SLR in Figure 3(c) is due to hallucination lacks data support and quantitative analysis. The few examples presented in section 4 are not sufficient to demonstrate the issue of hallucination.\nQuestions: n/a\n\n\n\n"", ""id"": ""mWMJN0vbDF0"", ""pdf"": ""openreview.net/pdf?id=mWMJN0vbDF""}","5"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""One of the main technical novelties is the AI feedback technique used in their method. However, this technique seems like a result of random attempts plus some intuition. More detailed though on how did the authors develop this technique or the comparison between other possible candidate techniques is needed. Moreover, this technique is similar to CoT. It would be better for the authors to discuss on the relationship between these two techniques. I am curious about the performance of guiding LLMs to think step by step without relying on additional critics in negotiation problems. More thorough explanations or experimental results of this aspect would provide deeper insights into the effectiveness of the proposed approach."", ""Target claims"": [], ""Reasoning"": ""This weakness questions the novelty and effectiveness of the AI feedback technique, which underlies the approach of models improving through self-play in negotiation tasks. However, it doesn't directly target any specific claim from the paper, as it is a broader critique of the method rather than the specific findings such as improvement (Claim1, Claim2, etc.). Therefore, it represents skepticism about the foundation of several claims but doesn't directly target them.""}, {""Weakness span"": ""This paper aims to improve the ability of large language model by self-play with each other. However, to strengthen the paper's claims, it would be better to provide additional evidence regarding the transferability of the proposed framework to different types of games."", ""Target claims"": [], ""Reasoning"": ""While this feedback suggests a limitation in testing for generalization across various games, it doesn't directly contest the truthfulness or support of specific claims about negotiation (e.g., Claim1, Claim3). Thus, it's not a direct challenge to the claims themselves but highlights an area for the additional robustness test of the claims.""}, {""Weakness span"": ""The stability of the environment is also a concern since the results appear to heavily rely on the reliability of the moderator. This dependence raises doubts regarding the individual contributions of various components in the system."", ""Target claims"": [3, 6], ""Reasoning"": ""This critique questions the validity of claims related to the iterative improvement through experience and AI feedback (Claim3 and Claim6), as it suggests environmental stability issues might obscure the contributions of the mechanism to achieve claimed improvements.""}, {""Weakness span"": ""It is not clear whether there exists an upper limit of improvement using ICL. It would be better for the authors to also discuss ICL with, e.g., fine-tuning or other trainable parts."", ""Target claims"": [10, 11], ""Reasoning"": ""This directly questions the assertion of continuous improvement in Claims 10 and 11 by indicating that limits to improvement with ICL aren\u2019t addressed, thus directly challenging the notion of sustainable improvement across multiple rounds (as claimed).""}]}, ""meta"": {""id"": ""lSZXSDwvGv"", ""review_idx"": 0, ""title"": ""Title: Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback\nAbstract: We study whether multiple large language models (LLMs) can autonomously\nimprove each other in a negotiation game by playing, reflecting, and criticizing.\nWe are interested in this question because if LLMs were able to improve each other, it\nwould imply the possibility of creating strong AI agents with minimal human\nintervention. We ask two LLMs to bargain with each other, playing the roles of a\nbuyer and a seller, respectively. They aim to reach a deal with the buyer targeting\na lower price and the seller a higher one. A third language model, playing the\ncritic, provides feedback to a player to improve the player\u2019s negotiation strategies.\nWe let the two agents play multiple rounds, using previous negotiation history\nand AI feedback as in-context demonstrations to improve the\nmodel\u2019s negotiation strategy iteratively. We use different LLMs (GPT and Claude)\nfor different roles and use the deal price as the evaluation metric. Our experiments\nreveal multiple intriguing findings: (1) Only a subset of the language models we\nconsider can self-play and improve the deal price from AI feedback, weaker models\neither do not understand the game\u2019s rules or cannot incorporate AI feedback for\nfurther improvement. (2) Models\u2019 abilities to learn from the feedback differ when\nplaying different roles. For example, it is harder for Claude-instant to improve\nas the buyer than as the seller. (3) When unrolling the game to multiple rounds,\nstronger agents can consistently improve their performance by meaningfully using\nprevious experiences and iterative AI feedback, yet have a higher risk of breaking\nthe deal. We hope our work provides insightful initial explorations of having\nmodels autonomously improve each other with game playing and AI feedback.\n\nTLDR:We ask two language models to negotiate with each other and continuously improve their negotiation strategies by multi-round game playing and iterative in-context learning from AI feedback\n\nKeywords:large language models, negotiation game, in-context learning, self-play, AI feedback"", ""claims"": [""Claim1: Only a subset of the language models we consider can self-play and improve the deal price from AI feedback, weaker models either do not understand the game\u2019s rules or cannot incorporate AI feedback for further improvement."", ""Claim2: Models\u2019 abilities to learn from the feedback differ when playing different roles. For example, it is harder for Claude-instant to improve as the buyer than as the seller."", ""Claim3: When unrolling the game to multiple rounds, stronger agents can consistently improve their performance by meaningfully using previous experiences and iterative AI feedback, yet have a higher risk of breaking the deal."", ""Claim4: Our experiments lead to several intriguing findings: (1) The requirements of our bargaining game effectively serve as a testbed for assessing the abilities of LLMs: although most models can do chitchat in a casual scenario, as of our experiment date (May 2023), cohere-command model does not understand the rule of bargaining, ai21-jurassic model does not respond to AI feedback, claude-instant-v1.0 can at most improve one round, and only gpt-3.5-turbo, gpt-4, and claude-v1.3 can continuously improve over multiple rounds."", ""Claim5: Models behave differently upon receiving feedback when playing different roles. Models playing the buyer role may be harder to improve than when in the seller role."", ""Claim6: It is indeed possible for strong agents like gpt-4 to continuously improve meaningfully using previous experiences and online iterative AI feedback, yet the attempt to sell at a higher price (or buy at a lower price) comes with the risk of failing to reach a deal at all."", ""Claim7: We further show evidence of the model being able to negotiation in a less verbose but more strategic (thus more effective) way."", ""Claim8: We let the models play multiple runs and measure the average deal price before and after AI feedback."", ""Claim9: We measure the performance of the players based on the final deal price."", ""Claim10: Finally, we would like to explore whether the players can continuously improve from AI feedback in a game over multiple rounds."", ""Claim11: In the experiments, we will show that only gpt-4 can improve over 5 rounds while other models\u2019 improvements may saturate at about 3 rounds."", ""Claim12: In our experiments, we consider three stages that gradually deepen our exploration of learning from AI feedback: (1) We first set up the basics of the game (Sec. 4.2), showing that only a few models can improve from AI critics, in which case AI feedback can be comparable (but more scalable) as human feedback. Other models either do not understand/ follow the rule of bargaining, or cannot incorporate AI feedback for improvements."", ""Claim13: Then we study the models\u2019 behaviors when playing different roles (Sec. 4.3). We discover the intriguing result that buyers are in general harder to improve than sellers."", ""Claim14: Finally, we study whether models can continuously improve over multiple rounds (Sec. 4.4), and show a tradeoff of deal price versus success rate: although some models can continuously improve the deal price, it comes with a higher risk of breaking a deal."", ""Claim15: We further show evidence of negotiation in a more strategic way: both gpt-4 and claude-v1.3 \u2019s responses become longer after multiple rounds of AI feedback (note that verbosity is a straightforward negotiation strategy), yet gpt-4 is less verbose than claude-v1.3 but achieves higher deal price and deal rate, meaning that its responses, although using fewer words, are more strategic and effective."", ""Claim16: We will show that, aligning with other concurrent model rankings [14, 21], gpt-3.5-turbo is a middle-level powerful engine (worse than gpt-4 , better than claude-instant-v1.0 )."", ""Claim17: If they do improve one round, we let them play multiple rounds and see if they could continuously improve their strategy."", ""Claim18: Conversational ability does not guarantee ability to negotiate or learning from feedback."", ""Claim19: We study whether conversational models can understand the rule of bargaining by manually checking traces of the dialog, and found that cohere-command fails to understand the rules, as is shown in Fig 3A."", ""Claim20: We also observe that AI21\u2019s j2-jumbo-instruct model, although understanding the rule of bargaining, cannot incorporate AI feedback, as is shown in Fig. 3B."", ""Claim21: After ruling out the cohere-command and j2-jumbo-instruct models, we consider the three remaining models: gpt-3.5-turbo , claude-instant-v1.0 and claude-v1.3 . For these three engines, we do not observe the problems in Fig. 3. This means that these models can be used for our multi-round games."", ""Claim22: We note that this may underestimate the performance of human feedback, yet it would be unpractical to ask human to write done 3 suggestions for all 1500 runs (while AI feedback does not have this problem)."", ""Claim23: The results are shown in Table 1 where we see that all three models (gpt-3.5-turbo, claude-instant-v1.0 and claude-v1.3) exhibit comparable improvements over human and AI feedback."", ""Claim24: When claude-instant-v1.0 and gpt-3.5-turbo play the seller, they are able to improve their average deal price after AI feedback (Fig. 4A). But when they play the buyer role, their average deal price does not improve, which indicates that buyers tend to be a harder role than sellers (Fig. 4B). Yet this observation does not hold for engines like gpt-4 and claude-v1.3, as they can still improve from AI feedback even playing buyers."", ""Claim25: Overall, this set of experiments reveal the nuanced capability differences between the four engines we consider."", ""Claim26: We see that when playing sellers, all four model engines can improve over at least one round, but this comes at the cost of decreasing deal success ratio."", ""Claim27: When playing buyers, there are models that cannot improve (claude-instant-v1.0), or saturate over 3 rounds (claude-v1.3), while gpt-4 and gpt-3.5-turbo can continuously improve, and gpt-4 achieves better (lower) deal price and higher deal rate than gpt-3.5-turbo."", ""Claim28: Finally, we study how iterative AI feedback influences the language complexity used by the agents by plotting the average response length (measured in number of characters) after each round, as is shown in Fig. 7."", ""Claim29: We see that both claude-v1.3 and gpt-4 become more verbose after iterative AI feedback with a continuously increasing response length."", ""Claim30: This observation serve as strong evidence that AI feedback improves players\u2019 response towards a word-tuned, strategic direction."", ""Claim31: Our experiments show that certain models can indeed improve by continuously playing competition games with iterative AI feedback, under well-defined rules in an AlphaGo Zero fashion."", ""Claim32: We also show the tradeoff between next-round price improvement and success rate, as a better deal price also comes with a higher risk of deal breaking."", ""Claim33: We further show evidence of improved language from iterative AI feedback: in a multi-round game, one model may be less verbose than another, but be better word-tuned, thus more effective in getting a better deal."", ""Claim34: We believe our results have profound implications for AI research: on the positive side, it indicates the possibility of continuously improving language models with minimal human intervention."", ""Claim35: On the risky side, it might be more challenging to oversight the model behavior in our framework because models are acting autonomously, which calls for future alignment and safety research in the multi-agent game setting."", ""Claim36: Overall, we believe our work provides a meaningful initial exploration for large language models\u2019 learning from game-playing and iterative AI feedback.""], ""review"": ""Review: Weakness: One of the main technical novelties is the AI feedback technique used in their method. However, this technique seems like a result of random attempts plus some intuition. More detailed though on how did the authors develop this technique or the comparison between other possible candidate techniques is needed. Moreover, this technique is similar to CoT. It would be better for the authors to discuss on the relationship between these two techniques. I am curious about the performance of guiding LLMs to think step by step without relying on additional critics in negotiation problems. More thorough explanations or experimental results of this aspect would provide deeper insights into the effectiveness of the proposed approach.\nThis paper aims to improve the ability of large language model by self-play with each other. However, to strengthen the paper's claims, it would be better to provide additional evidence regarding the transferability of the proposed framework to different types of games. Suggestions on the specific implementation process would also be valuable in applying this framework to different domains.\nThe stability of the environment is also a concern since the results appear to heavily rely on the reliability of the moderator. This dependence raises doubts regarding the individual contributions of various components in the system. Furthermore, the claim of proposing a technique for prompt optimization for generic classification tasks in Line 120 lacks sufficient details and evidence. Elaborating on this technique and demonstrating its effectiveness would enhance the paper's credibility and address this specific weakness.\nIt is not clear whether there exists an upper limit of improvement using ICL. It would be better for the authors to also discuss ICL with, e.g., fine-tuning or other trainable parts.\nQuestions: What is the exact definition of \""self-play\""? It seems that only one of the two players improves in the setting of this paper, which slightly different previous definition in, e.g., https://arxiv.org/pdf/2002.04017.pdf.\nLine 59: It would be beneficial to provide additional details, possibly with references, comparing the effectiveness of using AI feedback with RHLF. This comparison would contribute to a more comprehensive understanding of the proposed approach's advantages and distinguish it from existing techniques.\nLine 85: Why the LM engine behind the critic is consistently the same as the player it provides feedback.  \nFigure 4: It lacks the experimental results of Claude-v1.3 and GPT-4 as buyers. Why these scenarios are omitted? Including these results in the figure would ensure a complete representation of the experiments conducted.\n\n \n\nPresentation:\n\nLine 118, 119: inspect -> inspecting, add -> adding\nLine 120: recommend -> and recommending\nLine 211: comparison -> comparisons\nFigure 5: whild -> while\nLing 257: use less -> uses fewer\nLine 258: serve -> serves\n\n""}, ""review"": ""Title: Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback\nAbstract: We study whether multiple large language models (LLMs) can autonomously\nimprove each other in a negotiation game by playing, reflecting, and criticizing.\nWe are interested in this question because if LLMs were able to improve each other, it\nwould imply the possibility of creating strong AI agents with minimal human\nintervention. We ask two LLMs to bargain with each other, playing the roles of a\nbuyer and a seller, respectively. They aim to reach a deal with the buyer targeting\na lower price and the seller a higher one. A third language model, playing the\ncritic, provides feedback to a player to improve the player\u2019s negotiation strategies.\nWe let the two agents play multiple rounds, using previous negotiation history\nand AI feedback as in-context demonstrations to improve the\nmodel\u2019s negotiation strategy iteratively. We use different LLMs (GPT and Claude)\nfor different roles and use the deal price as the evaluation metric. Our experiments\nreveal multiple intriguing findings: (1) Only a subset of the language models we\nconsider can self-play and improve the deal price from AI feedback, weaker models\neither do not understand the game\u2019s rules or cannot incorporate AI feedback for\nfurther improvement. (2) Models\u2019 abilities to learn from the feedback differ when\nplaying different roles. For example, it is harder for Claude-instant to improve\nas the buyer than as the seller. (3) When unrolling the game to multiple rounds,\nstronger agents can consistently improve their performance by meaningfully using\nprevious experiences and iterative AI feedback, yet have a higher risk of breaking\nthe deal. We hope our work provides insightful initial explorations of having\nmodels autonomously improve each other with game playing and AI feedback.\n\nClaim1: Only a subset of the language models we consider can self-play and improve the deal price from AI feedback, weaker models either do not understand the game\u2019s rules or cannot incorporate AI feedback for further improvement.\nClaim2: Models\u2019 abilities to learn from the feedback differ when playing different roles. For example, it is harder for Claude-instant to improve as the buyer than as the seller.\nClaim3: When unrolling the game to multiple rounds, stronger agents can consistently improve their performance by meaningfully using previous experiences and iterative AI feedback, yet have a higher risk of breaking the deal.\nClaim4: Our experiments lead to several intriguing findings: (1) The requirements of our bargaining game effectively serve as a testbed for assessing the abilities of LLMs: although most models can do chitchat in a casual scenario, as of our experiment date (May 2023), cohere-command model does not understand the rule of bargaining, ai21-jurassic model does not respond to AI feedback, claude-instant-v1.0 can at most improve one round, and only gpt-3.5-turbo, gpt-4, and claude-v1.3 can continuously improve over multiple rounds.\nClaim5: Models behave differently upon receiving feedback when playing different roles. Models playing the buyer role may be harder to improve than when in the seller role.\nClaim6: It is indeed possible for strong agents like gpt-4 to continuously improve meaningfully using previous experiences and online iterative AI feedback, yet the attempt to sell at a higher price (or buy at a lower price) comes with the risk of failing to reach a deal at all.\nClaim7: We further show evidence of the model being able to negotiation in a less verbose but more strategic (thus more effective) way.\nClaim8: We let the models play multiple runs and measure the average deal price before and after AI feedback.\nClaim9: We measure the performance of the players based on the final deal price.\nClaim10: Finally, we would like to explore whether the players can continuously improve from AI feedback in a game over multiple rounds.\nClaim11: In the experiments, we will show that only gpt-4 can improve over 5 rounds while other models\u2019 improvements may saturate at about 3 rounds.\nClaim12: In our experiments, we consider three stages that gradually deepen our exploration of learning from AI feedback: (1) We first set up the basics of the game (Sec. 4.2), showing that only a few models can improve from AI critics, in which case AI feedback can be comparable (but more scalable) as human feedback. Other models either do not understand/ follow the rule of bargaining, or cannot incorporate AI feedback for improvements.\nClaim13: Then we study the models\u2019 behaviors when playing different roles (Sec. 4.3). We discover the intriguing result that buyers are in general harder to improve than sellers.\nClaim14: Finally, we study whether models can continuously improve over multiple rounds (Sec. 4.4), and show a tradeoff of deal price versus success rate: although some models can continuously improve the deal price, it comes with a higher risk of breaking a deal.\nClaim15: We further show evidence of negotiation in a more strategic way: both gpt-4 and claude-v1.3 \u2019s responses become longer after multiple rounds of AI feedback (note that verbosity is a straightforward negotiation strategy), yet gpt-4 is less verbose than claude-v1.3 but achieves higher deal price and deal rate, meaning that its responses, although using fewer words, are more strategic and effective.\nClaim16: We will show that, aligning with other concurrent model rankings [14, 21], gpt-3.5-turbo is a middle-level powerful engine (worse than gpt-4 , better than claude-instant-v1.0 ).\nClaim17: If they do improve one round, we let them play multiple rounds and see if they could continuously improve their strategy.\nClaim18: Conversational ability does not guarantee ability to negotiate or learning from feedback.\nClaim19: We study whether conversational models can understand the rule of bargaining by manually checking traces of the dialog, and found that cohere-command fails to understand the rules, as is shown in Fig 3A.\nClaim20: We also observe that AI21\u2019s j2-jumbo-instruct model, although understanding the rule of bargaining, cannot incorporate AI feedback, as is shown in Fig. 3B.\nClaim21: After ruling out the cohere-command and j2-jumbo-instruct models, we consider the three remaining models: gpt-3.5-turbo , claude-instant-v1.0 and claude-v1.3 . For these three engines, we do not observe the problems in Fig. 3. This means that these models can be used for our multi-round games.\nClaim22: We note that this may underestimate the performance of human feedback, yet it would be unpractical to ask human to write done 3 suggestions for all 1500 runs (while AI feedback does not have this problem).\nClaim23: The results are shown in Table 1 where we see that all three models (gpt-3.5-turbo, claude-instant-v1.0 and claude-v1.3) exhibit comparable improvements over human and AI feedback.\nClaim24: When claude-instant-v1.0 and gpt-3.5-turbo play the seller, they are able to improve their average deal price after AI feedback (Fig. 4A). But when they play the buyer role, their average deal price does not improve, which indicates that buyers tend to be a harder role than sellers (Fig. 4B). Yet this observation does not hold for engines like gpt-4 and claude-v1.3, as they can still improve from AI feedback even playing buyers.\nClaim25: Overall, this set of experiments reveal the nuanced capability differences between the four engines we consider.\nClaim26: We see that when playing sellers, all four model engines can improve over at least one round, but this comes at the cost of decreasing deal success ratio.\nClaim27: When playing buyers, there are models that cannot improve (claude-instant-v1.0), or saturate over 3 rounds (claude-v1.3), while gpt-4 and gpt-3.5-turbo can continuously improve, and gpt-4 achieves better (lower) deal price and higher deal rate than gpt-3.5-turbo.\nClaim28: Finally, we study how iterative AI feedback influences the language complexity used by the agents by plotting the average response length (measured in number of characters) after each round, as is shown in Fig. 7.\nClaim29: We see that both claude-v1.3 and gpt-4 become more verbose after iterative AI feedback with a continuously increasing response length.\nClaim30: This observation serve as strong evidence that AI feedback improves players\u2019 response towards a word-tuned, strategic direction.\nClaim31: Our experiments show that certain models can indeed improve by continuously playing competition games with iterative AI feedback, under well-defined rules in an AlphaGo Zero fashion.\nClaim32: We also show the tradeoff between next-round price improvement and success rate, as a better deal price also comes with a higher risk of deal breaking.\nClaim33: We further show evidence of improved language from iterative AI feedback: in a multi-round game, one model may be less verbose than another, but be better word-tuned, thus more effective in getting a better deal.\nClaim34: We believe our results have profound implications for AI research: on the positive side, it indicates the possibility of continuously improving language models with minimal human intervention.\nClaim35: On the risky side, it might be more challenging to oversight the model behavior in our framework because models are acting autonomously, which calls for future alignment and safety research in the multi-agent game setting.\nClaim36: Overall, we believe our work provides a meaningful initial exploration for large language models\u2019 learning from game-playing and iterative AI feedback.\n\nReview: Weakness: One of the main technical novelties is the AI feedback technique used in their method. However, this technique seems like a result of random attempts plus some intuition. More detailed though on how did the authors develop this technique or the comparison between other possible candidate techniques is needed. Moreover, this technique is similar to CoT. It would be better for the authors to discuss on the relationship between these two techniques. I am curious about the performance of guiding LLMs to think step by step without relying on additional critics in negotiation problems. More thorough explanations or experimental results of this aspect would provide deeper insights into the effectiveness of the proposed approach.\nThis paper aims to improve the ability of large language model by self-play with each other. However, to strengthen the paper's claims, it would be better to provide additional evidence regarding the transferability of the proposed framework to different types of games. Suggestions on the specific implementation process would also be valuable in applying this framework to different domains.\nThe stability of the environment is also a concern since the results appear to heavily rely on the reliability of the moderator. This dependence raises doubts regarding the individual contributions of various components in the system. Furthermore, the claim of proposing a technique for prompt optimization for generic classification tasks in Line 120 lacks sufficient details and evidence. Elaborating on this technique and demonstrating its effectiveness would enhance the paper's credibility and address this specific weakness.\nIt is not clear whether there exists an upper limit of improvement using ICL. It would be better for the authors to also discuss ICL with, e.g., fine-tuning or other trainable parts.\nQuestions: What is the exact definition of \""self-play\""? It seems that only one of the two players improves in the setting of this paper, which slightly different previous definition in, e.g., https://arxiv.org/pdf/2002.04017.pdf.\nLine 59: It would be beneficial to provide additional details, possibly with references, comparing the effectiveness of using AI feedback with RHLF. This comparison would contribute to a more comprehensive understanding of the proposed approach's advantages and distinguish it from existing techniques.\nLine 85: Why the LM engine behind the critic is consistently the same as the player it provides feedback.  \nFigure 4: It lacks the experimental results of Claude-v1.3 and GPT-4 as buyers. Why these scenarios are omitted? Including these results in the figure would ensure a complete representation of the experiments conducted.\n\n \n\nPresentation:\n\nLine 118, 119: inspect -> inspecting, add -> adding\nLine 120: recommend -> and recommending\nLine 211: comparison -> comparisons\nFigure 5: whild -> while\nLing 257: use less -> uses fewer\nLine 258: serve -> serves\n\n\n\n"", ""id"": ""lSZXSDwvGv0"", ""pdf"": ""openreview.net/pdf?id=lSZXSDwvGv""}","6"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""The baselines here are insufficiently strong, and do not seem to reflect the statements in the cited papers. The core technique _could_ be a component of a strong paper, but this paper is not it."", ""Target claims"": [8, 40], ""Reasoning"": ""This span suggests that the empirical results in the paper are not supported by strong baselines, challenging the validity of claims related to state-of-the-art performance. Claim8 states that the work achieves state-of-the-art performance on tasks, and Claim40 asserts superior results on language modeling tasks. However, the reviewer believes the baselines are inadequate, putting these claims into question.""}, {""Weakness span"": ""Lack of clear comparison to the 'upper bound' of _forgetting_ about the privacy cost of hparam search, which _should_ be an upper bound in _all_ scenarios considered here (IE, performing a sufficiently large grid search directly targeted at the problem at hand)."", ""Target claims"": [7, 8, 46], ""Reasoning"": ""The review points out the lack of a specific comparison that should serve as an upper bound, questioning claims regarding the novelty and effectiveness of the privacy-preserving hyperparameter tuning. Claim7 describes accounting for the privacy cost, Claim8 states achievement of state-of-the-art performance while considering the privacy cost, and Claim46 highlights that only a small fraction of the privacy budget is needed. These claims are weakened by the absence of comparison to a theoretically superior approach that ignores privacy costs.""}, {""Weakness span"": ""On [51]/[52], I see the reporeted CIFAR10 numbers from [51] as 98.8% at \u03b5=1 and 98.9 at \u03b5=\u221e (table 1 of [the arxiv version](https://arxiv.org/pdf/2211.13403.pdf)). Is there a typo in figure 2 of the paper under submission? Similarly, [51] seems to claim 88.1% and 90.6% at the \u03b5=1, \u221e for CIFAR-100. I uncovered these discrepancies since the paper under submission seemed to present implausibly strong results to me--e.g. it should be _impossible_ to achieve at epsilon=1 what none of the cited papers achieved at epsilon=\u221e just by tuning hyperparameters (see figure 2)."", ""Target claims"": [19, 30], ""Reasoning"": ""This span questions the reported performance numbers as possibly incorrect or misleading, targeting claims about achieving specific results under private conditions at epsilon=1 (Claim19) and reducing the private-non private utility gap (Claim30). If these numbers are inaccurate, it undermines the credibility of these claims.""}]}, ""meta"": {""id"": ""lENeWLXn4W"", ""review_idx"": 0, ""title"": ""Title: A New Linear Scaling Rule for Differentially Private Hyperparameter Optimization\nAbstract: A major direction in differentially private (DP) machine learning is DP fine-tuning: pretraining a model on a source of public data and transferring the extracted features to downstream tasks.\nThis is an important setting because many industry deployments fine-tune publicly available feature extractors on proprietary data for downstream tasks.\nIn this paper we propose a new linear scaling rule, a hyperparameter optimization algorithm that privately selects hyperparameters to optimize the privacy-utility tradeoff.\nA key insight into the design of our method is that our new linear scaling rule jointly increases the step size and number of steps as $\\varepsilon$ increases.\nOur work is the first to obtain state-of-the-art performance on a suite of 16 benchmark tasks across computer vision and natural language processing for a wide range of $\\varepsilon \\in [0.01,8.0]$ while accounting for the privacy cost of hyperparameter tuning.\n\nTLDR:We provide new SOTA methods for DP image classification when fine-tuning that account for the cost of hyperparameter search\n\nKeywords:Differential privacy, deep learning"", ""claims"": [""Claim1: Our work is the first to obtain state-of-the-art performance on a suite of 16 benchmark tasks across computer vision and natural language processing for a wide range of \u03b5 \u2208 [0.01,8.0] while accounting for the privacy cost of hyperparameter tuning."", ""Claim2: The privacy analysis of current approaches for private training does not account for the cost of hyperparameter tuning, and DP-SGD additionally increases the hyperparameter tuning burden compared to vanilla SGD."", ""Claim3: Because private training introduces additional hyperparameters, biases optimization by clipping the gradient, and imposes privacy-utility tradeoffs for existing hyperparameters, it is challenging to apply hyperparameter selection strategies from non-private training, even on the same dataset."", ""Claim4: Furthermore prior SOTA work in private training does not use similar hyperparameters as non-private training so hyperparameter search algorithms cannot be leveraged from the broader literature."", ""Claim5: We propose a new linear scaling rule (Alg. 1, Fig. 1) that automatically selects hyperparameters to optimize the privacy-utility tradeoff of private fine-tuning."", ""Claim6: In particular, as our privacy budget increases from \u03b5 = 0 \u2192 \u221e , we increase the step size and number of steps."", ""Claim7: Our method accounts for the privacy cost of hyperparameter selection by allotting a small portion of the budget to find the best hyperparameters at \u03b5 \u226a 1 and scaling these up to \u03b5 = 1."", ""Claim8: Our work is the first to obtain state-of-the-art performance on a suite of 16 benchmark tasks across computer vision and natural language processing for a wide range of \u03b5 \u2208 [0.01,8.0] while accounting for the privacy cost of hyperparameter tuning."", ""Claim9: In line with this insight, we propose a linear scaling rule: jointly increase the step size and number of steps linearly with \u03b5."", ""Claim10: Linear Scaling is intuitive. Applying the linear scaling rule improves the cosine similarity between noisy weight updates and the optimal solution without degrading accuracy."", ""Claim11: Theory We introduce two theoretical results."", ""Claim12: We apply this theorem to logistic regression (fine-tuning a linear model on extracted features). In this setting our theorem provides an upper bound on the radius of the range of solutions that DP-SGD produces. For linear models, this radius converts directly into an upper bound on the generalization error."", ""Claim13: While our theorem only holds for linear models, we will show that it holds empirically for the deep GPT2 and RoBERTa models, in line with Li et al. [47] who find that even the updates of a large model lie in a low-dimensional space during fine-tuning."", ""Claim14: We provide results on a range of image classification, distribution shift, and natural language processing tasks."", ""Claim15: We are the first to show that DP-SGD is capable of learning to handle distribution shifts without using any techniques from the distributionally robust optimization (DRO) literature [64]."", ""Claim16: Linear Scaling outperforms prior hyperparameter search techniques."", ""Claim17: We validate the effectiveness of linear scaling against the grid search baseline."", ""Claim18: It is trivial that linear scaling outperforms a naive grid search, but we also compare the effectiveness of linear scaling against the hyperparameter selection strategies used in prior work [51]."", ""Claim19: We recover performance of 82.7% for \u03b5 = 1, a 2% improvement over the best result for DP-Adam in [51] while accounting for the privacy cost of hyperparameter tuning."", ""Claim20: This is farther from the optimal non-private training, as evidenced by the performance gap."", ""Claim21: We observe that our method is competitive with [51] even when accounting for the privacy cost of hyperparameter search, and that the linear scaling rule holds up at the scale of ImageNet for very large values of r = \u03b7 \u00d7 T."", ""Claim22: We note that all numbers we report for models pretrained on ImageNet-21k using first-order methods surpass those in [51], but for sufficiently small values of \u03b5 on harder datasets the second-order methods they propose provide better performance."", ""Claim23: We note that the method in Mehta et al. [51] only works for vision tasks, whereas our approach works for both vision and language tasks."", ""Claim24: Linear Scaling produces robust results."", ""Claim25: In Fig. 3 we report that following Algorithm 1 produces new state-of-the-art results for all values of \u03b5 , shown in Table 5."", ""Claim26: Our results validate that this rule is robust: we can move from one set of hyperparameters to another similarly performing set of hyperparameters by increasing the number of iterations T by a constant factor and decreasing the learning rate \u03b7 by the same factor (or vice versa)."", ""Claim27: We find that any inaccuracy incurred by estimating the best value of r with the linear scaling rule will not reduce accuracy by much compared to doing grid search for the optimal value of r , but does reduce the privacy cost of hyperparameter tuning immensely."", ""Claim28: We find that all architectures can serve as good backbones for high-accuracy DP classification."", ""Claim29: We note that one architecture, beitv2, performs the best on all benchmarks and also has the highest non-private ImageNet accuracy [78]."", ""Claim30: We are encouraged to report that the private-non private utility gap diminishes with model accuracy, enabling us to report for the first time lossless privacy of 99.0% on CIFAR10 at \u03b5 = 1."", ""Claim31: We expect that as pretrained models become even better, future works may even be able to attain lossless privacy on CIFAR100, that we note remains somewhat challenging for private fine-tuning."", ""Claim32: In Table 6 we compare the performance of our method across 8 benchmarks and find that the answer to all three of these questions is yes."", ""Claim33: We are surprised to find that in the absence of any other regularization methods, DP fine-tuning actually improves performance on the OOD split."", ""Claim34: By comparing our results to Mehta et al. [53] we determine that this robustness is unique to DP rather than an artifact of the pretrained model."", ""Claim35: Although DP does significantly degrade the ID performance, in situations where minimizing OOD error is more important, we believe that DP by itself can mitigate the domain shift from private fine-tuning to test."", ""Claim36: If we compare our results on Camelyon to the best results in Ghalebikesabi et al. [25] we find that we can improve their best performance from 91.1% at \u03b5 = 10 to 93.91% at \u03b5 = 1."", ""Claim37: We believe that DP fine-tuning from pretrained models remains a viable strategy even when the publicly available pretraining data has a very large distribution shift from the private target data."", ""Claim38: Our OOD performance on these larger distribution shifts is much worse, particularly for CIFAR100 where there is a > 20% degradation."", ""Claim39: Although this is lower than the top result on the RobustBench leaderboard [13] obtains 85% accuracy, we note that once again we used no additional methods beyond DP to ensure robustness but managed to achieve reasonable performance to distribution shifts in zero-shot classification."", ""Claim40: The linear scaling rule outperforms prior results on differentially private language modeling tasks."", ""Claim41: We can see in Table 9 that we can push the perplexity under 18 for \u03b5 = 3 and \u03b5 = 8 ; this performance is competitive with the non-private baseline."", ""Claim42: Furthermore, even when pushing for a stricter privacy guarantee \u03b5 = 0 . 5 , we can still get perplexity of 21 . 25 , that is better than the result of \u03b5 = 8 in [46]."", ""Claim43: We quantitatively validate the linear scaling rule on WikiText-2 and Enron email dataset and report the result in Table 10 respectively."", ""Claim44: For WikiText-2, a key observation is that when we compare our results to the best prior reported results in [70], for the same number of passes over the training data (20), we obtain lower perplexity for \u03b5 = 0 . 2 than they report for \u03b5 = 3."", ""Claim45: We adapt their techniques to the hyperparameter settings that we show are optimal for DP fine-tuning, and produce similar performance to non-private fine-tuning on the PersonaChat dataset."", ""Claim46: By contrast our linear scaling rule needs only a small fraction of the overall privacy budget for hyperparameter search."", ""Claim47: Our rule accounts for multiple hyperparameters (batch size, clipping norm, momentum, learning rate, number of iterations) and produces SOTA results."", ""Claim48: We complement the existing body of work by introducing a new linear scaling rule to privately optimize hyperparameters."", ""Claim49: We provide find that our method attains new state-of-the-art accuracy across 20 tasks, on benchmark image classification tasks, distribution shift datasets, and natural language modeling tasks."", ""Claim50: We evaluate 20 datasets across multiple data modalities with multiple model architectures for two types of fine-tuning methods, linear probing and end-to-end training of deep ( > 100 M param) transformers."", ""Claim51: We find that with privacy budgets as small as \u03b5 = 0 . 01 we can still effectively forecast the linear trend to determine the best hyperparameters for the main privacy budget we consider \u03b5 = 1.""], ""review"": ""Review: Weakness: * A primarily empirical paper will live and die with the strength of its baselines (as well as its upper bounds in a case like this one where upper bounds on the efficacy of the technique can be computed). The baselines here are insufficiently strong, and do not seem to reflect the statements in the cited papers. The core technique _could_ be a component of a strong paper, but this paper is not it.\n\n* Some baseline issues: the citation problems with [51], [52] (detailed below). Lack of comparison to the 'naive baseline' of directly applying gaussian mechanism to results of grid search, say given known training statistics / optimal hparam values for nonprivate datasets (to avoid infinite regress, and here not so much of a problem since the experiments are all focused on public feature extractor settings). Lack of clear comparison to the 'upper bound' of _forgetting_ about the privacy cost of hparam search, which _should_ be an upper bound in _all_ scenarios considered here (IE, performing a sufficiently large grid search directly targeted at the problem at hand).\n\n* On [51]/[52], I see the reporeted CIFAR10 numbers from [51] as 98.8\\% at $\\epsilon=1$ and 98.9 at $\\epsilon=\\infty$ (table 1 of [the arxiv version](https://arxiv.org/pdf/2211.13403.pdf)). Is there a typo in figure 2 of the paper under submission? Similarly, [51] seems to claim 88.1\\% and 90.6\\% at the $\\epsilon=1, \\infty$ for CIFAR-100. I uncovered these discrepancies since the paper under submission seemed to present implausibly strong results to me--e.g. it should be _impossible_ to achieve at epsilon=1 what none of the cited papers achieved at epsilon=\\infty just by tuning hyperparameters (see figure 2). \n\n* The statements of timing on Imagenet seem wrong? The cited paper [51] seems to be pointing to a version from Nov 2022, clicking through to [52] seems to show a version uploaded in May 2022--so where are Jan 2023 and 'within the last month' coming from?\n\n* Some more consideration required in decomposition of $r$--do we know that random decomposition 'is enough'? Presumably it's not, since we _can_ generate an $\\eta$ for which the problem will presumably diverge?\nQuestions: The major questions I have for the authors here are the sources for both the claims on timing in relation to ImageNet (see weaknesses above) and the issues in citations (particularly with [51] and [52]). \n\n""}, ""review"": ""Title: A New Linear Scaling Rule for Differentially Private Hyperparameter Optimization\nAbstract: A major direction in differentially private (DP) machine learning is DP fine-tuning: pretraining a model on a source of public data and transferring the extracted features to downstream tasks.\nThis is an important setting because many industry deployments fine-tune publicly available feature extractors on proprietary data for downstream tasks.\nIn this paper we propose a new linear scaling rule, a hyperparameter optimization algorithm that privately selects hyperparameters to optimize the privacy-utility tradeoff.\nA key insight into the design of our method is that our new linear scaling rule jointly increases the step size and number of steps as $\\varepsilon$ increases.\nOur work is the first to obtain state-of-the-art performance on a suite of 16 benchmark tasks across computer vision and natural language processing for a wide range of $\\varepsilon \\in [0.01,8.0]$ while accounting for the privacy cost of hyperparameter tuning.\n\nClaim1: Our work is the first to obtain state-of-the-art performance on a suite of 16 benchmark tasks across computer vision and natural language processing for a wide range of \u03b5 \u2208 [0.01,8.0] while accounting for the privacy cost of hyperparameter tuning.\nClaim2: The privacy analysis of current approaches for private training does not account for the cost of hyperparameter tuning, and DP-SGD additionally increases the hyperparameter tuning burden compared to vanilla SGD.\nClaim3: Because private training introduces additional hyperparameters, biases optimization by clipping the gradient, and imposes privacy-utility tradeoffs for existing hyperparameters, it is challenging to apply hyperparameter selection strategies from non-private training, even on the same dataset.\nClaim4: Furthermore prior SOTA work in private training does not use similar hyperparameters as non-private training so hyperparameter search algorithms cannot be leveraged from the broader literature.\nClaim5: We propose a new linear scaling rule (Alg. 1, Fig. 1) that automatically selects hyperparameters to optimize the privacy-utility tradeoff of private fine-tuning.\nClaim6: In particular, as our privacy budget increases from \u03b5 = 0 \u2192 \u221e , we increase the step size and number of steps.\nClaim7: Our method accounts for the privacy cost of hyperparameter selection by allotting a small portion of the budget to find the best hyperparameters at \u03b5 \u226a 1 and scaling these up to \u03b5 = 1.\nClaim8: Our work is the first to obtain state-of-the-art performance on a suite of 16 benchmark tasks across computer vision and natural language processing for a wide range of \u03b5 \u2208 [0.01,8.0] while accounting for the privacy cost of hyperparameter tuning.\nClaim9: In line with this insight, we propose a linear scaling rule: jointly increase the step size and number of steps linearly with \u03b5.\nClaim10: Linear Scaling is intuitive. Applying the linear scaling rule improves the cosine similarity between noisy weight updates and the optimal solution without degrading accuracy.\nClaim11: Theory We introduce two theoretical results.\nClaim12: We apply this theorem to logistic regression (fine-tuning a linear model on extracted features). In this setting our theorem provides an upper bound on the radius of the range of solutions that DP-SGD produces. For linear models, this radius converts directly into an upper bound on the generalization error.\nClaim13: While our theorem only holds for linear models, we will show that it holds empirically for the deep GPT2 and RoBERTa models, in line with Li et al. [47] who find that even the updates of a large model lie in a low-dimensional space during fine-tuning.\nClaim14: We provide results on a range of image classification, distribution shift, and natural language processing tasks.\nClaim15: We are the first to show that DP-SGD is capable of learning to handle distribution shifts without using any techniques from the distributionally robust optimization (DRO) literature [64].\nClaim16: Linear Scaling outperforms prior hyperparameter search techniques.\nClaim17: We validate the effectiveness of linear scaling against the grid search baseline.\nClaim18: It is trivial that linear scaling outperforms a naive grid search, but we also compare the effectiveness of linear scaling against the hyperparameter selection strategies used in prior work [51].\nClaim19: We recover performance of 82.7% for \u03b5 = 1, a 2% improvement over the best result for DP-Adam in [51] while accounting for the privacy cost of hyperparameter tuning.\nClaim20: This is farther from the optimal non-private training, as evidenced by the performance gap.\nClaim21: We observe that our method is competitive with [51] even when accounting for the privacy cost of hyperparameter search, and that the linear scaling rule holds up at the scale of ImageNet for very large values of r = \u03b7 \u00d7 T.\nClaim22: We note that all numbers we report for models pretrained on ImageNet-21k using first-order methods surpass those in [51], but for sufficiently small values of \u03b5 on harder datasets the second-order methods they propose provide better performance.\nClaim23: We note that the method in Mehta et al. [51] only works for vision tasks, whereas our approach works for both vision and language tasks.\nClaim24: Linear Scaling produces robust results.\nClaim25: In Fig. 3 we report that following Algorithm 1 produces new state-of-the-art results for all values of \u03b5 , shown in Table 5.\nClaim26: Our results validate that this rule is robust: we can move from one set of hyperparameters to another similarly performing set of hyperparameters by increasing the number of iterations T by a constant factor and decreasing the learning rate \u03b7 by the same factor (or vice versa).\nClaim27: We find that any inaccuracy incurred by estimating the best value of r with the linear scaling rule will not reduce accuracy by much compared to doing grid search for the optimal value of r , but does reduce the privacy cost of hyperparameter tuning immensely.\nClaim28: We find that all architectures can serve as good backbones for high-accuracy DP classification.\nClaim29: We note that one architecture, beitv2, performs the best on all benchmarks and also has the highest non-private ImageNet accuracy [78].\nClaim30: We are encouraged to report that the private-non private utility gap diminishes with model accuracy, enabling us to report for the first time lossless privacy of 99.0% on CIFAR10 at \u03b5 = 1.\nClaim31: We expect that as pretrained models become even better, future works may even be able to attain lossless privacy on CIFAR100, that we note remains somewhat challenging for private fine-tuning.\nClaim32: In Table 6 we compare the performance of our method across 8 benchmarks and find that the answer to all three of these questions is yes.\nClaim33: We are surprised to find that in the absence of any other regularization methods, DP fine-tuning actually improves performance on the OOD split.\nClaim34: By comparing our results to Mehta et al. [53] we determine that this robustness is unique to DP rather than an artifact of the pretrained model.\nClaim35: Although DP does significantly degrade the ID performance, in situations where minimizing OOD error is more important, we believe that DP by itself can mitigate the domain shift from private fine-tuning to test.\nClaim36: If we compare our results on Camelyon to the best results in Ghalebikesabi et al. [25] we find that we can improve their best performance from 91.1% at \u03b5 = 10 to 93.91% at \u03b5 = 1.\nClaim37: We believe that DP fine-tuning from pretrained models remains a viable strategy even when the publicly available pretraining data has a very large distribution shift from the private target data.\nClaim38: Our OOD performance on these larger distribution shifts is much worse, particularly for CIFAR100 where there is a > 20% degradation.\nClaim39: Although this is lower than the top result on the RobustBench leaderboard [13] obtains 85% accuracy, we note that once again we used no additional methods beyond DP to ensure robustness but managed to achieve reasonable performance to distribution shifts in zero-shot classification.\nClaim40: The linear scaling rule outperforms prior results on differentially private language modeling tasks.\nClaim41: We can see in Table 9 that we can push the perplexity under 18 for \u03b5 = 3 and \u03b5 = 8 ; this performance is competitive with the non-private baseline.\nClaim42: Furthermore, even when pushing for a stricter privacy guarantee \u03b5 = 0 . 5 , we can still get perplexity of 21 . 25 , that is better than the result of \u03b5 = 8 in [46].\nClaim43: We quantitatively validate the linear scaling rule on WikiText-2 and Enron email dataset and report the result in Table 10 respectively.\nClaim44: For WikiText-2, a key observation is that when we compare our results to the best prior reported results in [70], for the same number of passes over the training data (20), we obtain lower perplexity for \u03b5 = 0 . 2 than they report for \u03b5 = 3.\nClaim45: We adapt their techniques to the hyperparameter settings that we show are optimal for DP fine-tuning, and produce similar performance to non-private fine-tuning on the PersonaChat dataset.\nClaim46: By contrast our linear scaling rule needs only a small fraction of the overall privacy budget for hyperparameter search.\nClaim47: Our rule accounts for multiple hyperparameters (batch size, clipping norm, momentum, learning rate, number of iterations) and produces SOTA results.\nClaim48: We complement the existing body of work by introducing a new linear scaling rule to privately optimize hyperparameters.\nClaim49: We provide find that our method attains new state-of-the-art accuracy across 20 tasks, on benchmark image classification tasks, distribution shift datasets, and natural language modeling tasks.\nClaim50: We evaluate 20 datasets across multiple data modalities with multiple model architectures for two types of fine-tuning methods, linear probing and end-to-end training of deep ( > 100 M param) transformers.\nClaim51: We find that with privacy budgets as small as \u03b5 = 0 . 01 we can still effectively forecast the linear trend to determine the best hyperparameters for the main privacy budget we consider \u03b5 = 1.\n\nReview: Weakness: * A primarily empirical paper will live and die with the strength of its baselines (as well as its upper bounds in a case like this one where upper bounds on the efficacy of the technique can be computed). The baselines here are insufficiently strong, and do not seem to reflect the statements in the cited papers. The core technique _could_ be a component of a strong paper, but this paper is not it.\n\n* Some baseline issues: the citation problems with [51], [52] (detailed below). Lack of comparison to the 'naive baseline' of directly applying gaussian mechanism to results of grid search, say given known training statistics / optimal hparam values for nonprivate datasets (to avoid infinite regress, and here not so much of a problem since the experiments are all focused on public feature extractor settings). Lack of clear comparison to the 'upper bound' of _forgetting_ about the privacy cost of hparam search, which _should_ be an upper bound in _all_ scenarios considered here (IE, performing a sufficiently large grid search directly targeted at the problem at hand).\n\n* On [51]/[52], I see the reporeted CIFAR10 numbers from [51] as 98.8\\% at $\\epsilon=1$ and 98.9 at $\\epsilon=\\infty$ (table 1 of [the arxiv version](https://arxiv.org/pdf/2211.13403.pdf)). Is there a typo in figure 2 of the paper under submission? Similarly, [51] seems to claim 88.1\\% and 90.6\\% at the $\\epsilon=1, \\infty$ for CIFAR-100. I uncovered these discrepancies since the paper under submission seemed to present implausibly strong results to me--e.g. it should be _impossible_ to achieve at epsilon=1 what none of the cited papers achieved at epsilon=\\infty just by tuning hyperparameters (see figure 2). \n\n* The statements of timing on Imagenet seem wrong? The cited paper [51] seems to be pointing to a version from Nov 2022, clicking through to [52] seems to show a version uploaded in May 2022--so where are Jan 2023 and 'within the last month' coming from?\n\n* Some more consideration required in decomposition of $r$--do we know that random decomposition 'is enough'? Presumably it's not, since we _can_ generate an $\\eta$ for which the problem will presumably diverge?\nQuestions: The major questions I have for the authors here are the sources for both the claims on timing in relation to ImageNet (see weaknesses above) and the issues in citations (particularly with [51] and [52]). \n\n\n\n"", ""id"": ""lENeWLXn4W0"", ""pdf"": ""openreview.net/pdf?id=lENeWLXn4W""}","7"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""- The contribution remains too thin in my opinion. There is really no theoretical or even heuristic explanation for the proposed scaling rule. There two theoretical results given, a GDP composition result (which is well known and should be cited as such) and another result of which importance I find difficult to judge."", ""Target claims"": [9, 11, 12], ""Reasoning"": ""The review points out that the contribution of the paper is perceived as thin due to the lack of a theoretical or heuristic explanation for the proposed scaling rule and questions the importance of the theoretical results provided. Claim9 introduces the linear scaling rule, while Claim11 and Claim12 describe the theoretical results. The weakness directly challenges the sufficiency and novelty of the claims related to the scaling rule and theoretical contributions.""}, {""Weakness span"": ""- I somehow find it hard to believe that you would get 99 percent test accuracy for CIFAR-10 with \u03b5=1.0. The SOTA results by De at al. (\""Unlocking High-Accuracy Differentially Private Image Classification through Scale\"") are somewhere at 95 percent for \u03b5=1.0. What made you to get this good results?"", ""Target claims"": [30], ""Reasoning"": ""The reviewer explicitly doubts the validity of Claim30, which states a 99% test accuracy for CIFAR-10 with \u03b5 = 1.0, noting that prior state-of-the-art results are much lower at 95%. This question the accuracy and credibility of Claim30.""}]}, ""meta"": {""id"": ""lENeWLXn4W"", ""review_idx"": 1, ""title"": ""Title: A New Linear Scaling Rule for Differentially Private Hyperparameter Optimization\nAbstract: A major direction in differentially private (DP) machine learning is DP fine-tuning: pretraining a model on a source of public data and transferring the extracted features to downstream tasks.\nThis is an important setting because many industry deployments fine-tune publicly available feature extractors on proprietary data for downstream tasks.\nIn this paper we propose a new linear scaling rule, a hyperparameter optimization algorithm that privately selects hyperparameters to optimize the privacy-utility tradeoff.\nA key insight into the design of our method is that our new linear scaling rule jointly increases the step size and number of steps as $\\varepsilon$ increases.\nOur work is the first to obtain state-of-the-art performance on a suite of 16 benchmark tasks across computer vision and natural language processing for a wide range of $\\varepsilon \\in [0.01,8.0]$ while accounting for the privacy cost of hyperparameter tuning.\n\nTLDR:We provide new SOTA methods for DP image classification when fine-tuning that account for the cost of hyperparameter search\n\nKeywords:Differential privacy, deep learning"", ""claims"": [""Claim1: Our work is the first to obtain state-of-the-art performance on a suite of 16 benchmark tasks across computer vision and natural language processing for a wide range of \u03b5 \u2208 [0.01,8.0] while accounting for the privacy cost of hyperparameter tuning."", ""Claim2: The privacy analysis of current approaches for private training does not account for the cost of hyperparameter tuning, and DP-SGD additionally increases the hyperparameter tuning burden compared to vanilla SGD."", ""Claim3: Because private training introduces additional hyperparameters, biases optimization by clipping the gradient, and imposes privacy-utility tradeoffs for existing hyperparameters, it is challenging to apply hyperparameter selection strategies from non-private training, even on the same dataset."", ""Claim4: Furthermore prior SOTA work in private training does not use similar hyperparameters as non-private training so hyperparameter search algorithms cannot be leveraged from the broader literature."", ""Claim5: We propose a new linear scaling rule (Alg. 1, Fig. 1) that automatically selects hyperparameters to optimize the privacy-utility tradeoff of private fine-tuning."", ""Claim6: In particular, as our privacy budget increases from \u03b5 = 0 \u2192 \u221e , we increase the step size and number of steps."", ""Claim7: Our method accounts for the privacy cost of hyperparameter selection by allotting a small portion of the budget to find the best hyperparameters at \u03b5 \u226a 1 and scaling these up to \u03b5 = 1."", ""Claim8: Our work is the first to obtain state-of-the-art performance on a suite of 16 benchmark tasks across computer vision and natural language processing for a wide range of \u03b5 \u2208 [0.01,8.0] while accounting for the privacy cost of hyperparameter tuning."", ""Claim9: In line with this insight, we propose a linear scaling rule: jointly increase the step size and number of steps linearly with \u03b5."", ""Claim10: Linear Scaling is intuitive. Applying the linear scaling rule improves the cosine similarity between noisy weight updates and the optimal solution without degrading accuracy."", ""Claim11: Theory We introduce two theoretical results."", ""Claim12: We apply this theorem to logistic regression (fine-tuning a linear model on extracted features). In this setting our theorem provides an upper bound on the radius of the range of solutions that DP-SGD produces. For linear models, this radius converts directly into an upper bound on the generalization error."", ""Claim13: While our theorem only holds for linear models, we will show that it holds empirically for the deep GPT2 and RoBERTa models, in line with Li et al. [47] who find that even the updates of a large model lie in a low-dimensional space during fine-tuning."", ""Claim14: We provide results on a range of image classification, distribution shift, and natural language processing tasks."", ""Claim15: We are the first to show that DP-SGD is capable of learning to handle distribution shifts without using any techniques from the distributionally robust optimization (DRO) literature [64]."", ""Claim16: Linear Scaling outperforms prior hyperparameter search techniques."", ""Claim17: We validate the effectiveness of linear scaling against the grid search baseline."", ""Claim18: It is trivial that linear scaling outperforms a naive grid search, but we also compare the effectiveness of linear scaling against the hyperparameter selection strategies used in prior work [51]."", ""Claim19: We recover performance of 82.7% for \u03b5 = 1, a 2% improvement over the best result for DP-Adam in [51] while accounting for the privacy cost of hyperparameter tuning."", ""Claim20: This is farther from the optimal non-private training, as evidenced by the performance gap."", ""Claim21: We observe that our method is competitive with [51] even when accounting for the privacy cost of hyperparameter search, and that the linear scaling rule holds up at the scale of ImageNet for very large values of r = \u03b7 \u00d7 T."", ""Claim22: We note that all numbers we report for models pretrained on ImageNet-21k using first-order methods surpass those in [51], but for sufficiently small values of \u03b5 on harder datasets the second-order methods they propose provide better performance."", ""Claim23: We note that the method in Mehta et al. [51] only works for vision tasks, whereas our approach works for both vision and language tasks."", ""Claim24: Linear Scaling produces robust results."", ""Claim25: In Fig. 3 we report that following Algorithm 1 produces new state-of-the-art results for all values of \u03b5 , shown in Table 5."", ""Claim26: Our results validate that this rule is robust: we can move from one set of hyperparameters to another similarly performing set of hyperparameters by increasing the number of iterations T by a constant factor and decreasing the learning rate \u03b7 by the same factor (or vice versa)."", ""Claim27: We find that any inaccuracy incurred by estimating the best value of r with the linear scaling rule will not reduce accuracy by much compared to doing grid search for the optimal value of r , but does reduce the privacy cost of hyperparameter tuning immensely."", ""Claim28: We find that all architectures can serve as good backbones for high-accuracy DP classification."", ""Claim29: We note that one architecture, beitv2, performs the best on all benchmarks and also has the highest non-private ImageNet accuracy [78]."", ""Claim30: We are encouraged to report that the private-non private utility gap diminishes with model accuracy, enabling us to report for the first time lossless privacy of 99.0% on CIFAR10 at \u03b5 = 1."", ""Claim31: We expect that as pretrained models become even better, future works may even be able to attain lossless privacy on CIFAR100, that we note remains somewhat challenging for private fine-tuning."", ""Claim32: In Table 6 we compare the performance of our method across 8 benchmarks and find that the answer to all three of these questions is yes."", ""Claim33: We are surprised to find that in the absence of any other regularization methods, DP fine-tuning actually improves performance on the OOD split."", ""Claim34: By comparing our results to Mehta et al. [53] we determine that this robustness is unique to DP rather than an artifact of the pretrained model."", ""Claim35: Although DP does significantly degrade the ID performance, in situations where minimizing OOD error is more important, we believe that DP by itself can mitigate the domain shift from private fine-tuning to test."", ""Claim36: If we compare our results on Camelyon to the best results in Ghalebikesabi et al. [25] we find that we can improve their best performance from 91.1% at \u03b5 = 10 to 93.91% at \u03b5 = 1."", ""Claim37: We believe that DP fine-tuning from pretrained models remains a viable strategy even when the publicly available pretraining data has a very large distribution shift from the private target data."", ""Claim38: Our OOD performance on these larger distribution shifts is much worse, particularly for CIFAR100 where there is a > 20% degradation."", ""Claim39: Although this is lower than the top result on the RobustBench leaderboard [13] obtains 85% accuracy, we note that once again we used no additional methods beyond DP to ensure robustness but managed to achieve reasonable performance to distribution shifts in zero-shot classification."", ""Claim40: The linear scaling rule outperforms prior results on differentially private language modeling tasks."", ""Claim41: We can see in Table 9 that we can push the perplexity under 18 for \u03b5 = 3 and \u03b5 = 8 ; this performance is competitive with the non-private baseline."", ""Claim42: Furthermore, even when pushing for a stricter privacy guarantee \u03b5 = 0 . 5 , we can still get perplexity of 21 . 25 , that is better than the result of \u03b5 = 8 in [46]."", ""Claim43: We quantitatively validate the linear scaling rule on WikiText-2 and Enron email dataset and report the result in Table 10 respectively."", ""Claim44: For WikiText-2, a key observation is that when we compare our results to the best prior reported results in [70], for the same number of passes over the training data (20), we obtain lower perplexity for \u03b5 = 0 . 2 than they report for \u03b5 = 3."", ""Claim45: We adapt their techniques to the hyperparameter settings that we show are optimal for DP fine-tuning, and produce similar performance to non-private fine-tuning on the PersonaChat dataset."", ""Claim46: By contrast our linear scaling rule needs only a small fraction of the overall privacy budget for hyperparameter search."", ""Claim47: Our rule accounts for multiple hyperparameters (batch size, clipping norm, momentum, learning rate, number of iterations) and produces SOTA results."", ""Claim48: We complement the existing body of work by introducing a new linear scaling rule to privately optimize hyperparameters."", ""Claim49: We provide find that our method attains new state-of-the-art accuracy across 20 tasks, on benchmark image classification tasks, distribution shift datasets, and natural language modeling tasks."", ""Claim50: We evaluate 20 datasets across multiple data modalities with multiple model architectures for two types of fine-tuning methods, linear probing and end-to-end training of deep ( > 100 M param) transformers."", ""Claim51: We find that with privacy budgets as small as \u03b5 = 0 . 01 we can still effectively forecast the linear trend to determine the best hyperparameters for the main privacy budget we consider \u03b5 = 1.""], ""review"": ""Review: Weakness: - The technique is restricted to optimising the learning rate and the length of the training. I wonder if similar extrapolation (perhaps more generally polynomial extrapolation) could be used to find optimal the optimal hyperparameter values for other hyperparameters.\n\n- The technical part could be written more carefully. It remains unclear whether you use RDP or GDP. The hyperparameter tuning cost of the method by Papernot and Steinke is in terms of RDP, but you list theoretical results in terms of GDP. In the end of Alg. 1 you write that the total cost is \""$\\varepsilon_f + \\varepsilon_0 + \\varepsilon_1$\"". Is that approximate DP? In case you use the classical composition result where you just add up the privacy parameters, what happens to the $\\delta$-parameters?\n\n- Some conclusions are a vaguely formulated/confusing. On p. 7 you have the subtitle \""Linear Scaling is robust to distribution shifts\"", but then you seem to show and also claim in the subsequent text that DP itself is robust to distribution shifts. Somehow the message is vague here.\n\n- The contribution remains too thin in my opinion. There is really no theoretical or even heuristic explanation for the proposed scaling rule. There two theoretical results given, a GDP composition result (which is well known and should be cited as such) and another result of which importance I find difficult to judge.\n\nQuestions: \n\nWhat does the following line in Alg. 1 mean: \""Use privacy loss variable accounting to calibrate noise parameter $\\sigma$ given $\\varepsilon$\"" ?\n\n- I somehow find it hard to believe that you would get 99 percent test accuracy for CIFAR-10 with $\\varepsilon=1.0$. The SOTA results by De at al. (\""Unlocking High-Accuracy Differentially Private Image Classification through Scale\"") are somewhere at 95 percent for  $\\varepsilon=1.0$. What made you to get this good results?\n\n- You mention in the description of the method that you scale up to $\\varepsilon=1.0$. Why is that? You seem to use varying $\\varepsilon$-values in the experiments.\n\nThere seem to be typos here and there, please go through the writing carefully. Here few examples:\nline 387-388: \"" treat < 10% of the private training dataset and public\"" -> \""as public\""\nline 394: \""We provide find that our method attains...\""\n\nI don't quite understand the following sentence:\n\""The key assumption in DP fine-tuning is that there is no privacy leakage between public data and private data.\""\nYou mean that there are not too big distribution shifts?\n\n""}, ""review"": ""Title: A New Linear Scaling Rule for Differentially Private Hyperparameter Optimization\nAbstract: A major direction in differentially private (DP) machine learning is DP fine-tuning: pretraining a model on a source of public data and transferring the extracted features to downstream tasks.\nThis is an important setting because many industry deployments fine-tune publicly available feature extractors on proprietary data for downstream tasks.\nIn this paper we propose a new linear scaling rule, a hyperparameter optimization algorithm that privately selects hyperparameters to optimize the privacy-utility tradeoff.\nA key insight into the design of our method is that our new linear scaling rule jointly increases the step size and number of steps as $\\varepsilon$ increases.\nOur work is the first to obtain state-of-the-art performance on a suite of 16 benchmark tasks across computer vision and natural language processing for a wide range of $\\varepsilon \\in [0.01,8.0]$ while accounting for the privacy cost of hyperparameter tuning.\n\nClaim1: Our work is the first to obtain state-of-the-art performance on a suite of 16 benchmark tasks across computer vision and natural language processing for a wide range of \u03b5 \u2208 [0.01,8.0] while accounting for the privacy cost of hyperparameter tuning.\nClaim2: The privacy analysis of current approaches for private training does not account for the cost of hyperparameter tuning, and DP-SGD additionally increases the hyperparameter tuning burden compared to vanilla SGD.\nClaim3: Because private training introduces additional hyperparameters, biases optimization by clipping the gradient, and imposes privacy-utility tradeoffs for existing hyperparameters, it is challenging to apply hyperparameter selection strategies from non-private training, even on the same dataset.\nClaim4: Furthermore prior SOTA work in private training does not use similar hyperparameters as non-private training so hyperparameter search algorithms cannot be leveraged from the broader literature.\nClaim5: We propose a new linear scaling rule (Alg. 1, Fig. 1) that automatically selects hyperparameters to optimize the privacy-utility tradeoff of private fine-tuning.\nClaim6: In particular, as our privacy budget increases from \u03b5 = 0 \u2192 \u221e , we increase the step size and number of steps.\nClaim7: Our method accounts for the privacy cost of hyperparameter selection by allotting a small portion of the budget to find the best hyperparameters at \u03b5 \u226a 1 and scaling these up to \u03b5 = 1.\nClaim8: Our work is the first to obtain state-of-the-art performance on a suite of 16 benchmark tasks across computer vision and natural language processing for a wide range of \u03b5 \u2208 [0.01,8.0] while accounting for the privacy cost of hyperparameter tuning.\nClaim9: In line with this insight, we propose a linear scaling rule: jointly increase the step size and number of steps linearly with \u03b5.\nClaim10: Linear Scaling is intuitive. Applying the linear scaling rule improves the cosine similarity between noisy weight updates and the optimal solution without degrading accuracy.\nClaim11: Theory We introduce two theoretical results.\nClaim12: We apply this theorem to logistic regression (fine-tuning a linear model on extracted features). In this setting our theorem provides an upper bound on the radius of the range of solutions that DP-SGD produces. For linear models, this radius converts directly into an upper bound on the generalization error.\nClaim13: While our theorem only holds for linear models, we will show that it holds empirically for the deep GPT2 and RoBERTa models, in line with Li et al. [47] who find that even the updates of a large model lie in a low-dimensional space during fine-tuning.\nClaim14: We provide results on a range of image classification, distribution shift, and natural language processing tasks.\nClaim15: We are the first to show that DP-SGD is capable of learning to handle distribution shifts without using any techniques from the distributionally robust optimization (DRO) literature [64].\nClaim16: Linear Scaling outperforms prior hyperparameter search techniques.\nClaim17: We validate the effectiveness of linear scaling against the grid search baseline.\nClaim18: It is trivial that linear scaling outperforms a naive grid search, but we also compare the effectiveness of linear scaling against the hyperparameter selection strategies used in prior work [51].\nClaim19: We recover performance of 82.7% for \u03b5 = 1, a 2% improvement over the best result for DP-Adam in [51] while accounting for the privacy cost of hyperparameter tuning.\nClaim20: This is farther from the optimal non-private training, as evidenced by the performance gap.\nClaim21: We observe that our method is competitive with [51] even when accounting for the privacy cost of hyperparameter search, and that the linear scaling rule holds up at the scale of ImageNet for very large values of r = \u03b7 \u00d7 T.\nClaim22: We note that all numbers we report for models pretrained on ImageNet-21k using first-order methods surpass those in [51], but for sufficiently small values of \u03b5 on harder datasets the second-order methods they propose provide better performance.\nClaim23: We note that the method in Mehta et al. [51] only works for vision tasks, whereas our approach works for both vision and language tasks.\nClaim24: Linear Scaling produces robust results.\nClaim25: In Fig. 3 we report that following Algorithm 1 produces new state-of-the-art results for all values of \u03b5 , shown in Table 5.\nClaim26: Our results validate that this rule is robust: we can move from one set of hyperparameters to another similarly performing set of hyperparameters by increasing the number of iterations T by a constant factor and decreasing the learning rate \u03b7 by the same factor (or vice versa).\nClaim27: We find that any inaccuracy incurred by estimating the best value of r with the linear scaling rule will not reduce accuracy by much compared to doing grid search for the optimal value of r , but does reduce the privacy cost of hyperparameter tuning immensely.\nClaim28: We find that all architectures can serve as good backbones for high-accuracy DP classification.\nClaim29: We note that one architecture, beitv2, performs the best on all benchmarks and also has the highest non-private ImageNet accuracy [78].\nClaim30: We are encouraged to report that the private-non private utility gap diminishes with model accuracy, enabling us to report for the first time lossless privacy of 99.0% on CIFAR10 at \u03b5 = 1.\nClaim31: We expect that as pretrained models become even better, future works may even be able to attain lossless privacy on CIFAR100, that we note remains somewhat challenging for private fine-tuning.\nClaim32: In Table 6 we compare the performance of our method across 8 benchmarks and find that the answer to all three of these questions is yes.\nClaim33: We are surprised to find that in the absence of any other regularization methods, DP fine-tuning actually improves performance on the OOD split.\nClaim34: By comparing our results to Mehta et al. [53] we determine that this robustness is unique to DP rather than an artifact of the pretrained model.\nClaim35: Although DP does significantly degrade the ID performance, in situations where minimizing OOD error is more important, we believe that DP by itself can mitigate the domain shift from private fine-tuning to test.\nClaim36: If we compare our results on Camelyon to the best results in Ghalebikesabi et al. [25] we find that we can improve their best performance from 91.1% at \u03b5 = 10 to 93.91% at \u03b5 = 1.\nClaim37: We believe that DP fine-tuning from pretrained models remains a viable strategy even when the publicly available pretraining data has a very large distribution shift from the private target data.\nClaim38: Our OOD performance on these larger distribution shifts is much worse, particularly for CIFAR100 where there is a > 20% degradation.\nClaim39: Although this is lower than the top result on the RobustBench leaderboard [13] obtains 85% accuracy, we note that once again we used no additional methods beyond DP to ensure robustness but managed to achieve reasonable performance to distribution shifts in zero-shot classification.\nClaim40: The linear scaling rule outperforms prior results on differentially private language modeling tasks.\nClaim41: We can see in Table 9 that we can push the perplexity under 18 for \u03b5 = 3 and \u03b5 = 8 ; this performance is competitive with the non-private baseline.\nClaim42: Furthermore, even when pushing for a stricter privacy guarantee \u03b5 = 0 . 5 , we can still get perplexity of 21 . 25 , that is better than the result of \u03b5 = 8 in [46].\nClaim43: We quantitatively validate the linear scaling rule on WikiText-2 and Enron email dataset and report the result in Table 10 respectively.\nClaim44: For WikiText-2, a key observation is that when we compare our results to the best prior reported results in [70], for the same number of passes over the training data (20), we obtain lower perplexity for \u03b5 = 0 . 2 than they report for \u03b5 = 3.\nClaim45: We adapt their techniques to the hyperparameter settings that we show are optimal for DP fine-tuning, and produce similar performance to non-private fine-tuning on the PersonaChat dataset.\nClaim46: By contrast our linear scaling rule needs only a small fraction of the overall privacy budget for hyperparameter search.\nClaim47: Our rule accounts for multiple hyperparameters (batch size, clipping norm, momentum, learning rate, number of iterations) and produces SOTA results.\nClaim48: We complement the existing body of work by introducing a new linear scaling rule to privately optimize hyperparameters.\nClaim49: We provide find that our method attains new state-of-the-art accuracy across 20 tasks, on benchmark image classification tasks, distribution shift datasets, and natural language modeling tasks.\nClaim50: We evaluate 20 datasets across multiple data modalities with multiple model architectures for two types of fine-tuning methods, linear probing and end-to-end training of deep ( > 100 M param) transformers.\nClaim51: We find that with privacy budgets as small as \u03b5 = 0 . 01 we can still effectively forecast the linear trend to determine the best hyperparameters for the main privacy budget we consider \u03b5 = 1.\n\nReview: Weakness: - The technique is restricted to optimising the learning rate and the length of the training. I wonder if similar extrapolation (perhaps more generally polynomial extrapolation) could be used to find optimal the optimal hyperparameter values for other hyperparameters.\n\n- The technical part could be written more carefully. It remains unclear whether you use RDP or GDP. The hyperparameter tuning cost of the method by Papernot and Steinke is in terms of RDP, but you list theoretical results in terms of GDP. In the end of Alg. 1 you write that the total cost is \""$\\varepsilon_f + \\varepsilon_0 + \\varepsilon_1$\"". Is that approximate DP? In case you use the classical composition result where you just add up the privacy parameters, what happens to the $\\delta$-parameters?\n\n- Some conclusions are a vaguely formulated/confusing. On p. 7 you have the subtitle \""Linear Scaling is robust to distribution shifts\"", but then you seem to show and also claim in the subsequent text that DP itself is robust to distribution shifts. Somehow the message is vague here.\n\n- The contribution remains too thin in my opinion. There is really no theoretical or even heuristic explanation for the proposed scaling rule. There two theoretical results given, a GDP composition result (which is well known and should be cited as such) and another result of which importance I find difficult to judge.\n\nQuestions: \n\nWhat does the following line in Alg. 1 mean: \""Use privacy loss variable accounting to calibrate noise parameter $\\sigma$ given $\\varepsilon$\"" ?\n\n- I somehow find it hard to believe that you would get 99 percent test accuracy for CIFAR-10 with $\\varepsilon=1.0$. The SOTA results by De at al. (\""Unlocking High-Accuracy Differentially Private Image Classification through Scale\"") are somewhere at 95 percent for  $\\varepsilon=1.0$. What made you to get this good results?\n\n- You mention in the description of the method that you scale up to $\\varepsilon=1.0$. Why is that? You seem to use varying $\\varepsilon$-values in the experiments.\n\nThere seem to be typos here and there, please go through the writing carefully. Here few examples:\nline 387-388: \"" treat < 10% of the private training dataset and public\"" -> \""as public\""\nline 394: \""We provide find that our method attains...\""\n\nI don't quite understand the following sentence:\n\""The key assumption in DP fine-tuning is that there is no privacy leakage between public data and private data.\""\nYou mean that there are not too big distribution shifts?\n\n\n\n"", ""id"": ""lENeWLXn4W1"", ""pdf"": ""openreview.net/pdf?id=lENeWLXn4W""}","8"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": """", ""Target claims"": [], ""Reasoning"": ""The review suggests a case study to provide detailed results for specific classes within the CIFAR-10 dataset, which could make conclusions more convincing. However, this does not directly challenge any specific claim made in the paper regarding its findings or contributions and does not argue that any findings are questionable. Therefore, no claim-related weakness is identified.""}, {""Weakness span"": """", ""Target claims"": [], ""Reasoning"": ""The review asks whether the method was tried in NLP datasets and notes the importance of pre-trained models in NLP. This question hints at potential limitations in the scope of the study but does not directly challenge any specific claim regarding its findings or contributions related to the experiments or results, indicating that it is not a claim-related weakness.""}]}, ""meta"": {""id"": ""kJHKkRAZ0W"", ""review_idx"": 0, ""title"": ""Title: NTKCPL: Active Learning on Top of Self-Supervised Model by Estimating True Coverage\nAbstract: High annotation cost has driven extensive research in active learning and self-supervised learning. Recent research has shown that in the context of supervised learning, when we have different numbers of labels, we need to apply different active learning strategies to ensure that it outperforms the random baseline. This number of annotations that change the suitable active learning strategy is called the phase transition point. We found, however, when combining active learning with self-supervised models to achieve improved performance, the phase transition point occurs earlier. It becomes challenging to determine which strategy should be used for previously unseen datasets. We argue that existing active learning algorithms are heavily influenced by the phase transition because the empirical risk over the entire active learning pool estimated by these algorithms is inaccurate and influenced by the number of labeled samples. To address this issue, we propose a novel active learning strategy, neural tangent kernel clustering-pseudo-labels (NTKCPL). It estimates empirical risk based on pseudo-labels and the model prediction with NTK approximation. We analyze the factors affecting this approximation error and design a pseudo-label clustering generation method to reduce the approximation error. Finally, our method was validated on five datasets, empirically demonstrating that it outperforms the baseline methods in most cases and is valid over a longer range of training budgets.\n\nTLDR:We propose an active learning strategy that performs effective on a broad range of annotation quantities, enabling us to confidently employ AL on top of a self-supervised model.\n\nKeywords:active learning, low budget, neural tangent kernel, pseudo-label"", ""claims"": [""Claim1: Finally, our method was validated on five datasets, empirically demonstrating that it outperforms the baseline methods in most cases and is valid over a longer range of training budgets."", ""Claim2: In this paper, we focus on designing an active learning strategy that works well in the training method with a self-supervised model."", ""Claim3: We note that when combining active learning with the self-supervised model, the cut-off point between low-budget and high-budget strategy occurs much earlier."", ""Claim4: In this paper we use this problem to motivate the design of an active learning strategy with a wider effective budget range."", ""Claim5: Extensive experimental results demonstrate that our method outperforms state-of-the-art approaches in most cases and has a wider effective budget range."", ""Claim6: As part of the results (sec. 4) we also show our method is effective for self-supervised features of different quality."", ""Claim7: Our contribution is summarized as follows: (1) We propose a novel active learning strategy, NTKCPL, by estimating empirical risk on the whole active learning pool based on pseudo-labels. (2) We analyze the approximation error of the empirical risk in the active learning pool when NTK and CPL are used to approximate networks and true labels. (3) Our method outperforms both lowand high-budget active learning strategies within a range of annotation quantities one order of magnitude larger than traditional low-budget active learning experiments."", ""Claim8: This means that our approach can be used more confidently for active learning on top of self-supervised models than existing low-budget strategies."", ""Claim9: The coverage estimated by NTKCPL is more consistent with the true coverage of the neural network than those estimated based on feature distances."", ""Claim10: To estimate the true coverage, we propose a new strategy, NTKCPL."", ""Claim11: That training method achieves better or equal performance than fine-tuning the whole network in the low-budget case while its prediction is more consistent with the results of NTK."", ""Claim12: The approximation error can be divided into two terms, the first one is the difference between NTK and neural network prediction, error NTK , and the second one is the difference caused by CPL during NTK estimation, error CPL."", ""Claim13: Our approach is validated on five datasets with various qualities of self-supervised features."", ""Claim14: We compare our proposed method with representative active learning strategies: (1) Random, (2) Entropy (uncertainty sampling, maximum entropy of output), (3) Coreset (diversity active learning strategy, greedy solution of minimum coverage radius), (4) BADGE (combination of uncertainty and diversity, kmeans++ sampling on grad embedding), where the scalable version, badge partition, is used in ImageNet-100, CIFAR-100 and Oxford-IIIT Pet because the huge dimension of grad embedding (5) Typiclust (designed for low-budget case), (6) Lookahead (maximum output change based on NTK)."", ""Claim15: We empirically found that fixed active learning query steps lead to much faster growth of classifier accuracy in the early stages of active learning (the amount of labels is about 10 times than the number of class) than in the later stages, so it is difficult to clearly observe the differences between different active learning strategies."", ""Claim16: NTKCPL outperforms SOTA. As shown in table 1, fig. 2. In most cases, our proposed method outperforms the baseline methods."", ""Claim17: For the few cases with only a small number of labels, our method shows comparable performance with the low-budget strategy, TypiClust, such as in CIFAR-100 with 500 and 1000 labeled samples, and Oxford-IIIT Pet with 80 and 100 labeled samples."", ""Claim18: NTKCPL still shows good performance when the self-supervised features do not correspond well to the label classes."", ""Claim19: Our method is similar to other baseline strategies at the beginning of active learning, but."", ""Claim20: Our method has similar accuracy in the first three rounds as the TypiClust and outperforms all baseline methods afterward."", ""Claim21: NTKCPL has a wider effective budget range than SOTA."", ""Claim22: The average accuracy of our method, NTKCPL(al), outperforms the random baseline at all quantities of labels."", ""Claim23: In contrast, both the typical high-budget strategy, BADGE, and low-budget strategy, TypiClust, appear to be worse than the random baseline over a range of annotation quantities."", ""Claim24: We show the effective budget range of our method, NTKCPL, as well as the typical high-budget strategy, BADGE, and the typical low-budget strategy, TypiClust, across all experiments in table 2."", ""Claim25: Our method approximates the true coverage well for most cases."", ""Claim26: The experimental results support our analysis in sec. 3.4 that too many or too few clusters will increase the approximation error, which affects the performance of active learning."", ""Claim27: From these experiments, we found that clustering on active learning features yields better results except for the case where the number of annotations is very small."", ""Claim28: Also, NTKCPL(self) is better than NTKCPL(al) in a wide range of annotation quantities (no more than 500), when self-supervised features are good such as experiment in the CIFAR-10."", ""Claim29: We study the active learning problem when training on top of a self-supervised model. In this case, an intensified phase transition is observed and it influences the application of active learning."", ""Claim30: We propose NTKCPL that approximates empirical risk on the whole pool more directly."", ""Claim31: Our method outperforms SOTA in most cases and has a wider effective budget range."", ""Claim32: The comprehensive experiments show that our method can work well on self-supervised features with different qualities."", ""Claim33: Our approach is limited to the fixed training approach, i.e., training the classifier on top of a frozen self-supervised training encoder, which is restricted to the low-budget scenario because the fine-tuning training approach provides higher accuracy in the high-budget case.""], ""review"": ""Review: Weakness: 1. A case study is suggested. For example, in CIFAR-10 dataset, which class or classes are much better than others or all the classes become better with the same scale? Providing detailed case study about the datasets together with improved metrics will make your conclusion further convincing.  \nQuestions: 1. Did authors try this method in NLP datasets? It will be very great if the proposed method can be evaluated in different fields. In NLP, pre-trained model seems to play a more important role than the one in Computer Vision tasks.  \n\n""}, ""review"": ""Title: NTKCPL: Active Learning on Top of Self-Supervised Model by Estimating True Coverage\nAbstract: High annotation cost has driven extensive research in active learning and self-supervised learning. Recent research has shown that in the context of supervised learning, when we have different numbers of labels, we need to apply different active learning strategies to ensure that it outperforms the random baseline. This number of annotations that change the suitable active learning strategy is called the phase transition point. We found, however, when combining active learning with self-supervised models to achieve improved performance, the phase transition point occurs earlier. It becomes challenging to determine which strategy should be used for previously unseen datasets. We argue that existing active learning algorithms are heavily influenced by the phase transition because the empirical risk over the entire active learning pool estimated by these algorithms is inaccurate and influenced by the number of labeled samples. To address this issue, we propose a novel active learning strategy, neural tangent kernel clustering-pseudo-labels (NTKCPL). It estimates empirical risk based on pseudo-labels and the model prediction with NTK approximation. We analyze the factors affecting this approximation error and design a pseudo-label clustering generation method to reduce the approximation error. Finally, our method was validated on five datasets, empirically demonstrating that it outperforms the baseline methods in most cases and is valid over a longer range of training budgets.\n\nClaim1: Finally, our method was validated on five datasets, empirically demonstrating that it outperforms the baseline methods in most cases and is valid over a longer range of training budgets.\nClaim2: In this paper, we focus on designing an active learning strategy that works well in the training method with a self-supervised model.\nClaim3: We note that when combining active learning with the self-supervised model, the cut-off point between low-budget and high-budget strategy occurs much earlier.\nClaim4: In this paper we use this problem to motivate the design of an active learning strategy with a wider effective budget range.\nClaim5: Extensive experimental results demonstrate that our method outperforms state-of-the-art approaches in most cases and has a wider effective budget range.\nClaim6: As part of the results (sec. 4) we also show our method is effective for self-supervised features of different quality.\nClaim7: Our contribution is summarized as follows: (1) We propose a novel active learning strategy, NTKCPL, by estimating empirical risk on the whole active learning pool based on pseudo-labels. (2) We analyze the approximation error of the empirical risk in the active learning pool when NTK and CPL are used to approximate networks and true labels. (3) Our method outperforms both lowand high-budget active learning strategies within a range of annotation quantities one order of magnitude larger than traditional low-budget active learning experiments.\nClaim8: This means that our approach can be used more confidently for active learning on top of self-supervised models than existing low-budget strategies.\nClaim9: The coverage estimated by NTKCPL is more consistent with the true coverage of the neural network than those estimated based on feature distances.\nClaim10: To estimate the true coverage, we propose a new strategy, NTKCPL.\nClaim11: That training method achieves better or equal performance than fine-tuning the whole network in the low-budget case while its prediction is more consistent with the results of NTK.\nClaim12: The approximation error can be divided into two terms, the first one is the difference between NTK and neural network prediction, error NTK , and the second one is the difference caused by CPL during NTK estimation, error CPL.\nClaim13: Our approach is validated on five datasets with various qualities of self-supervised features.\nClaim14: We compare our proposed method with representative active learning strategies: (1) Random, (2) Entropy (uncertainty sampling, maximum entropy of output), (3) Coreset (diversity active learning strategy, greedy solution of minimum coverage radius), (4) BADGE (combination of uncertainty and diversity, kmeans++ sampling on grad embedding), where the scalable version, badge partition, is used in ImageNet-100, CIFAR-100 and Oxford-IIIT Pet because the huge dimension of grad embedding (5) Typiclust (designed for low-budget case), (6) Lookahead (maximum output change based on NTK).\nClaim15: We empirically found that fixed active learning query steps lead to much faster growth of classifier accuracy in the early stages of active learning (the amount of labels is about 10 times than the number of class) than in the later stages, so it is difficult to clearly observe the differences between different active learning strategies.\nClaim16: NTKCPL outperforms SOTA. As shown in table 1, fig. 2. In most cases, our proposed method outperforms the baseline methods.\nClaim17: For the few cases with only a small number of labels, our method shows comparable performance with the low-budget strategy, TypiClust, such as in CIFAR-100 with 500 and 1000 labeled samples, and Oxford-IIIT Pet with 80 and 100 labeled samples.\nClaim18: NTKCPL still shows good performance when the self-supervised features do not correspond well to the label classes.\nClaim19: Our method is similar to other baseline strategies at the beginning of active learning, but.\nClaim20: Our method has similar accuracy in the first three rounds as the TypiClust and outperforms all baseline methods afterward.\nClaim21: NTKCPL has a wider effective budget range than SOTA.\nClaim22: The average accuracy of our method, NTKCPL(al), outperforms the random baseline at all quantities of labels.\nClaim23: In contrast, both the typical high-budget strategy, BADGE, and low-budget strategy, TypiClust, appear to be worse than the random baseline over a range of annotation quantities.\nClaim24: We show the effective budget range of our method, NTKCPL, as well as the typical high-budget strategy, BADGE, and the typical low-budget strategy, TypiClust, across all experiments in table 2.\nClaim25: Our method approximates the true coverage well for most cases.\nClaim26: The experimental results support our analysis in sec. 3.4 that too many or too few clusters will increase the approximation error, which affects the performance of active learning.\nClaim27: From these experiments, we found that clustering on active learning features yields better results except for the case where the number of annotations is very small.\nClaim28: Also, NTKCPL(self) is better than NTKCPL(al) in a wide range of annotation quantities (no more than 500), when self-supervised features are good such as experiment in the CIFAR-10.\nClaim29: We study the active learning problem when training on top of a self-supervised model. In this case, an intensified phase transition is observed and it influences the application of active learning.\nClaim30: We propose NTKCPL that approximates empirical risk on the whole pool more directly.\nClaim31: Our method outperforms SOTA in most cases and has a wider effective budget range.\nClaim32: The comprehensive experiments show that our method can work well on self-supervised features with different qualities.\nClaim33: Our approach is limited to the fixed training approach, i.e., training the classifier on top of a frozen self-supervised training encoder, which is restricted to the low-budget scenario because the fine-tuning training approach provides higher accuracy in the high-budget case.\n\nReview: Weakness: 1. A case study is suggested. For example, in CIFAR-10 dataset, which class or classes are much better than others or all the classes become better with the same scale? Providing detailed case study about the datasets together with improved metrics will make your conclusion further convincing.  \nQuestions: 1. Did authors try this method in NLP datasets? It will be very great if the proposed method can be evaluated in different fields. In NLP, pre-trained model seems to play a more important role than the one in Computer Vision tasks.  \n\n\n\n"", ""id"": ""kJHKkRAZ0W0"", ""pdf"": ""openreview.net/pdf?id=kJHKkRAZ0W""}","9"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""- The method needs to freeze the backbone to ensure the accuracy of the NTK approximation (line 151 - 154). However, according to the existing self-supervised learning literature [a], there is a substantial performance gap between fine-tuning the entire network and those that only train the classifier."", ""Target claims"": [11, 33], ""Reasoning"": ""The review points out a significant limitation related to Claim 11, which discusses the training method's effectiveness relative to entire network fine-tuning and mentions consistency with NTK results. It questions the validity of Claim 33, where the paper claims limitations to fixed training approaches. The review argues that such an approach is suboptimal compared to fine-tuning the network, directly challenging the claims that justify freezing the backbone for better NTK approximation and limiting methodology to low-budget scenarios.""}, {""Weakness span"": ""- Moreover, the proposed method appears to be dependent on the quality of the features learned through self-supervised learning. Although it is claimed that the active learning feature is used to improve clustering purity, the results in table 1 reveals a performance advantage for self-supervised features in low-budget situations."", ""Target claims"": [6, 18], ""Reasoning"": ""This part of the review challenges Claim 6 and Claim 18, which argue the method's ability to handle varying self-supervised feature qualities and its effectiveness despite this variability. The reviewer suggests that the dependence on feature quality is problematic and the method's purported benefits, as mentioned in these claims, may only hold under certain conditions, such as low-budget cases.""}]}, ""meta"": {""id"": ""kJHKkRAZ0W"", ""review_idx"": 1, ""title"": ""Title: NTKCPL: Active Learning on Top of Self-Supervised Model by Estimating True Coverage\nAbstract: High annotation cost has driven extensive research in active learning and self-supervised learning. Recent research has shown that in the context of supervised learning, when we have different numbers of labels, we need to apply different active learning strategies to ensure that it outperforms the random baseline. This number of annotations that change the suitable active learning strategy is called the phase transition point. We found, however, when combining active learning with self-supervised models to achieve improved performance, the phase transition point occurs earlier. It becomes challenging to determine which strategy should be used for previously unseen datasets. We argue that existing active learning algorithms are heavily influenced by the phase transition because the empirical risk over the entire active learning pool estimated by these algorithms is inaccurate and influenced by the number of labeled samples. To address this issue, we propose a novel active learning strategy, neural tangent kernel clustering-pseudo-labels (NTKCPL). It estimates empirical risk based on pseudo-labels and the model prediction with NTK approximation. We analyze the factors affecting this approximation error and design a pseudo-label clustering generation method to reduce the approximation error. Finally, our method was validated on five datasets, empirically demonstrating that it outperforms the baseline methods in most cases and is valid over a longer range of training budgets.\n\nTLDR:We propose an active learning strategy that performs effective on a broad range of annotation quantities, enabling us to confidently employ AL on top of a self-supervised model.\n\nKeywords:active learning, low budget, neural tangent kernel, pseudo-label"", ""claims"": [""Claim1: Finally, our method was validated on five datasets, empirically demonstrating that it outperforms the baseline methods in most cases and is valid over a longer range of training budgets."", ""Claim2: In this paper, we focus on designing an active learning strategy that works well in the training method with a self-supervised model."", ""Claim3: We note that when combining active learning with the self-supervised model, the cut-off point between low-budget and high-budget strategy occurs much earlier."", ""Claim4: In this paper we use this problem to motivate the design of an active learning strategy with a wider effective budget range."", ""Claim5: Extensive experimental results demonstrate that our method outperforms state-of-the-art approaches in most cases and has a wider effective budget range."", ""Claim6: As part of the results (sec. 4) we also show our method is effective for self-supervised features of different quality."", ""Claim7: Our contribution is summarized as follows: (1) We propose a novel active learning strategy, NTKCPL, by estimating empirical risk on the whole active learning pool based on pseudo-labels. (2) We analyze the approximation error of the empirical risk in the active learning pool when NTK and CPL are used to approximate networks and true labels. (3) Our method outperforms both lowand high-budget active learning strategies within a range of annotation quantities one order of magnitude larger than traditional low-budget active learning experiments."", ""Claim8: This means that our approach can be used more confidently for active learning on top of self-supervised models than existing low-budget strategies."", ""Claim9: The coverage estimated by NTKCPL is more consistent with the true coverage of the neural network than those estimated based on feature distances."", ""Claim10: To estimate the true coverage, we propose a new strategy, NTKCPL."", ""Claim11: That training method achieves better or equal performance than fine-tuning the whole network in the low-budget case while its prediction is more consistent with the results of NTK."", ""Claim12: The approximation error can be divided into two terms, the first one is the difference between NTK and neural network prediction, error NTK , and the second one is the difference caused by CPL during NTK estimation, error CPL."", ""Claim13: Our approach is validated on five datasets with various qualities of self-supervised features."", ""Claim14: We compare our proposed method with representative active learning strategies: (1) Random, (2) Entropy (uncertainty sampling, maximum entropy of output), (3) Coreset (diversity active learning strategy, greedy solution of minimum coverage radius), (4) BADGE (combination of uncertainty and diversity, kmeans++ sampling on grad embedding), where the scalable version, badge partition, is used in ImageNet-100, CIFAR-100 and Oxford-IIIT Pet because the huge dimension of grad embedding (5) Typiclust (designed for low-budget case), (6) Lookahead (maximum output change based on NTK)."", ""Claim15: We empirically found that fixed active learning query steps lead to much faster growth of classifier accuracy in the early stages of active learning (the amount of labels is about 10 times than the number of class) than in the later stages, so it is difficult to clearly observe the differences between different active learning strategies."", ""Claim16: NTKCPL outperforms SOTA. As shown in table 1, fig. 2. In most cases, our proposed method outperforms the baseline methods."", ""Claim17: For the few cases with only a small number of labels, our method shows comparable performance with the low-budget strategy, TypiClust, such as in CIFAR-100 with 500 and 1000 labeled samples, and Oxford-IIIT Pet with 80 and 100 labeled samples."", ""Claim18: NTKCPL still shows good performance when the self-supervised features do not correspond well to the label classes."", ""Claim19: Our method is similar to other baseline strategies at the beginning of active learning, but."", ""Claim20: Our method has similar accuracy in the first three rounds as the TypiClust and outperforms all baseline methods afterward."", ""Claim21: NTKCPL has a wider effective budget range than SOTA."", ""Claim22: The average accuracy of our method, NTKCPL(al), outperforms the random baseline at all quantities of labels."", ""Claim23: In contrast, both the typical high-budget strategy, BADGE, and low-budget strategy, TypiClust, appear to be worse than the random baseline over a range of annotation quantities."", ""Claim24: We show the effective budget range of our method, NTKCPL, as well as the typical high-budget strategy, BADGE, and the typical low-budget strategy, TypiClust, across all experiments in table 2."", ""Claim25: Our method approximates the true coverage well for most cases."", ""Claim26: The experimental results support our analysis in sec. 3.4 that too many or too few clusters will increase the approximation error, which affects the performance of active learning."", ""Claim27: From these experiments, we found that clustering on active learning features yields better results except for the case where the number of annotations is very small."", ""Claim28: Also, NTKCPL(self) is better than NTKCPL(al) in a wide range of annotation quantities (no more than 500), when self-supervised features are good such as experiment in the CIFAR-10."", ""Claim29: We study the active learning problem when training on top of a self-supervised model. In this case, an intensified phase transition is observed and it influences the application of active learning."", ""Claim30: We propose NTKCPL that approximates empirical risk on the whole pool more directly."", ""Claim31: Our method outperforms SOTA in most cases and has a wider effective budget range."", ""Claim32: The comprehensive experiments show that our method can work well on self-supervised features with different qualities."", ""Claim33: Our approach is limited to the fixed training approach, i.e., training the classifier on top of a frozen self-supervised training encoder, which is restricted to the low-budget scenario because the fine-tuning training approach provides higher accuracy in the high-budget case.""], ""review"": ""Review: Weakness: - Limited technical contribution\n    - An essential element of the proposed method stems from earlier work [26] that utilizes NTK approximation of DNN prediction for look-ahead active learning.\n    - The most prominent distinction from [26] is that this work utilize expected error reduction instead of expected model output change for active selection, and proposed a method for assigning pseudo labels to facilitate this.\n    - Given that both expected error reduction and pseudo labeling through feature vector clustering are widely used techniques, the technical contribution of the proposed method could be seen as incremental.\n- Concerns about practicality\n    - The method needs to freeze the backbone to ensure the accuracy of the NTK approximation (line 151 - 154). However, according to the existing self-supervised learning literature [a], there is a substantial performance gap between fine-tuning the entire network and those that only train the classifier.\n    - Moreover, the proposed method appears to be dependent on the quality of the features learned through self-supervised learning. Although it is claimed that the active learning feature is used to improve clustering purity, the results in table 1 reveals a performance advantage for self-supervised features in low-budget situations.\n- Need for comprehensive baseline comparisons\n    - It would be beneficial if the performance of [24] was also reported, given that the proposed method follows the training configuration of [24].\n    - It appears that the results using self-supervised features on Cifar 100 may be missing. It would be advantageous to see these results.\n    - Furthermore, I am curious as to whether the baseline methods were also trained with the frozen backbone, and how their performance would vary depending on this factor.\n- I encountered difficulties in smoothly following the provided script. I believe there is room for improvement in the writing.\n    - I found the paper's main point to be somewhat confusing. On lines 49 - 52, one of the stated goals of the proposed method is an active learning strategy with a wider effective budget range, yet on line 226, there is a focus on the low-budget regime.\n    - Furthermore, the notations in the method section have not been clearly defined, and they are somehow confusing. For instance, in the script, 'f' denotes a neural network (line 125), 'f_0' represents the network's output (line 127), 'f_self' and 'f_al' are features (line 1 of algorithm 1), 'f_t' signifies the classifier (line 21 of algorithm 1), and '\\hat(f)_cpl' is the prediction (line 170).\n\n[a] He, Kaiming, et al. \""Masked autoencoders are scalable vision learners.\""\u00a0*Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. 2022.\nQuestions: - I am interested in understanding the performance degradation when not using a frozen backbone.\n- I am curious about the upper bound of performance achieved through empirical risk estimation using true labels and NTK.\n\n""}, ""review"": ""Title: NTKCPL: Active Learning on Top of Self-Supervised Model by Estimating True Coverage\nAbstract: High annotation cost has driven extensive research in active learning and self-supervised learning. Recent research has shown that in the context of supervised learning, when we have different numbers of labels, we need to apply different active learning strategies to ensure that it outperforms the random baseline. This number of annotations that change the suitable active learning strategy is called the phase transition point. We found, however, when combining active learning with self-supervised models to achieve improved performance, the phase transition point occurs earlier. It becomes challenging to determine which strategy should be used for previously unseen datasets. We argue that existing active learning algorithms are heavily influenced by the phase transition because the empirical risk over the entire active learning pool estimated by these algorithms is inaccurate and influenced by the number of labeled samples. To address this issue, we propose a novel active learning strategy, neural tangent kernel clustering-pseudo-labels (NTKCPL). It estimates empirical risk based on pseudo-labels and the model prediction with NTK approximation. We analyze the factors affecting this approximation error and design a pseudo-label clustering generation method to reduce the approximation error. Finally, our method was validated on five datasets, empirically demonstrating that it outperforms the baseline methods in most cases and is valid over a longer range of training budgets.\n\nClaim1: Finally, our method was validated on five datasets, empirically demonstrating that it outperforms the baseline methods in most cases and is valid over a longer range of training budgets.\nClaim2: In this paper, we focus on designing an active learning strategy that works well in the training method with a self-supervised model.\nClaim3: We note that when combining active learning with the self-supervised model, the cut-off point between low-budget and high-budget strategy occurs much earlier.\nClaim4: In this paper we use this problem to motivate the design of an active learning strategy with a wider effective budget range.\nClaim5: Extensive experimental results demonstrate that our method outperforms state-of-the-art approaches in most cases and has a wider effective budget range.\nClaim6: As part of the results (sec. 4) we also show our method is effective for self-supervised features of different quality.\nClaim7: Our contribution is summarized as follows: (1) We propose a novel active learning strategy, NTKCPL, by estimating empirical risk on the whole active learning pool based on pseudo-labels. (2) We analyze the approximation error of the empirical risk in the active learning pool when NTK and CPL are used to approximate networks and true labels. (3) Our method outperforms both lowand high-budget active learning strategies within a range of annotation quantities one order of magnitude larger than traditional low-budget active learning experiments.\nClaim8: This means that our approach can be used more confidently for active learning on top of self-supervised models than existing low-budget strategies.\nClaim9: The coverage estimated by NTKCPL is more consistent with the true coverage of the neural network than those estimated based on feature distances.\nClaim10: To estimate the true coverage, we propose a new strategy, NTKCPL.\nClaim11: That training method achieves better or equal performance than fine-tuning the whole network in the low-budget case while its prediction is more consistent with the results of NTK.\nClaim12: The approximation error can be divided into two terms, the first one is the difference between NTK and neural network prediction, error NTK , and the second one is the difference caused by CPL during NTK estimation, error CPL.\nClaim13: Our approach is validated on five datasets with various qualities of self-supervised features.\nClaim14: We compare our proposed method with representative active learning strategies: (1) Random, (2) Entropy (uncertainty sampling, maximum entropy of output), (3) Coreset (diversity active learning strategy, greedy solution of minimum coverage radius), (4) BADGE (combination of uncertainty and diversity, kmeans++ sampling on grad embedding), where the scalable version, badge partition, is used in ImageNet-100, CIFAR-100 and Oxford-IIIT Pet because the huge dimension of grad embedding (5) Typiclust (designed for low-budget case), (6) Lookahead (maximum output change based on NTK).\nClaim15: We empirically found that fixed active learning query steps lead to much faster growth of classifier accuracy in the early stages of active learning (the amount of labels is about 10 times than the number of class) than in the later stages, so it is difficult to clearly observe the differences between different active learning strategies.\nClaim16: NTKCPL outperforms SOTA. As shown in table 1, fig. 2. In most cases, our proposed method outperforms the baseline methods.\nClaim17: For the few cases with only a small number of labels, our method shows comparable performance with the low-budget strategy, TypiClust, such as in CIFAR-100 with 500 and 1000 labeled samples, and Oxford-IIIT Pet with 80 and 100 labeled samples.\nClaim18: NTKCPL still shows good performance when the self-supervised features do not correspond well to the label classes.\nClaim19: Our method is similar to other baseline strategies at the beginning of active learning, but.\nClaim20: Our method has similar accuracy in the first three rounds as the TypiClust and outperforms all baseline methods afterward.\nClaim21: NTKCPL has a wider effective budget range than SOTA.\nClaim22: The average accuracy of our method, NTKCPL(al), outperforms the random baseline at all quantities of labels.\nClaim23: In contrast, both the typical high-budget strategy, BADGE, and low-budget strategy, TypiClust, appear to be worse than the random baseline over a range of annotation quantities.\nClaim24: We show the effective budget range of our method, NTKCPL, as well as the typical high-budget strategy, BADGE, and the typical low-budget strategy, TypiClust, across all experiments in table 2.\nClaim25: Our method approximates the true coverage well for most cases.\nClaim26: The experimental results support our analysis in sec. 3.4 that too many or too few clusters will increase the approximation error, which affects the performance of active learning.\nClaim27: From these experiments, we found that clustering on active learning features yields better results except for the case where the number of annotations is very small.\nClaim28: Also, NTKCPL(self) is better than NTKCPL(al) in a wide range of annotation quantities (no more than 500), when self-supervised features are good such as experiment in the CIFAR-10.\nClaim29: We study the active learning problem when training on top of a self-supervised model. In this case, an intensified phase transition is observed and it influences the application of active learning.\nClaim30: We propose NTKCPL that approximates empirical risk on the whole pool more directly.\nClaim31: Our method outperforms SOTA in most cases and has a wider effective budget range.\nClaim32: The comprehensive experiments show that our method can work well on self-supervised features with different qualities.\nClaim33: Our approach is limited to the fixed training approach, i.e., training the classifier on top of a frozen self-supervised training encoder, which is restricted to the low-budget scenario because the fine-tuning training approach provides higher accuracy in the high-budget case.\n\nReview: Weakness: - Limited technical contribution\n    - An essential element of the proposed method stems from earlier work [26] that utilizes NTK approximation of DNN prediction for look-ahead active learning.\n    - The most prominent distinction from [26] is that this work utilize expected error reduction instead of expected model output change for active selection, and proposed a method for assigning pseudo labels to facilitate this.\n    - Given that both expected error reduction and pseudo labeling through feature vector clustering are widely used techniques, the technical contribution of the proposed method could be seen as incremental.\n- Concerns about practicality\n    - The method needs to freeze the backbone to ensure the accuracy of the NTK approximation (line 151 - 154). However, according to the existing self-supervised learning literature [a], there is a substantial performance gap between fine-tuning the entire network and those that only train the classifier.\n    - Moreover, the proposed method appears to be dependent on the quality of the features learned through self-supervised learning. Although it is claimed that the active learning feature is used to improve clustering purity, the results in table 1 reveals a performance advantage for self-supervised features in low-budget situations.\n- Need for comprehensive baseline comparisons\n    - It would be beneficial if the performance of [24] was also reported, given that the proposed method follows the training configuration of [24].\n    - It appears that the results using self-supervised features on Cifar 100 may be missing. It would be advantageous to see these results.\n    - Furthermore, I am curious as to whether the baseline methods were also trained with the frozen backbone, and how their performance would vary depending on this factor.\n- I encountered difficulties in smoothly following the provided script. I believe there is room for improvement in the writing.\n    - I found the paper's main point to be somewhat confusing. On lines 49 - 52, one of the stated goals of the proposed method is an active learning strategy with a wider effective budget range, yet on line 226, there is a focus on the low-budget regime.\n    - Furthermore, the notations in the method section have not been clearly defined, and they are somehow confusing. For instance, in the script, 'f' denotes a neural network (line 125), 'f_0' represents the network's output (line 127), 'f_self' and 'f_al' are features (line 1 of algorithm 1), 'f_t' signifies the classifier (line 21 of algorithm 1), and '\\hat(f)_cpl' is the prediction (line 170).\n\n[a] He, Kaiming, et al. \""Masked autoencoders are scalable vision learners.\""\u00a0*Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. 2022.\nQuestions: - I am interested in understanding the performance degradation when not using a frozen backbone.\n- I am curious about the upper bound of performance achieved through empirical risk estimation using true labels and NTK.\n\n\n\n"", ""id"": ""kJHKkRAZ0W1"", ""pdf"": ""openreview.net/pdf?id=kJHKkRAZ0W""}","10"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""This paper asserts its realism and efficiency in generating new textures through optimization, yet its claims are evaluated solely on rendered images. It remains uncertain whether this method is applicable to in-the-wild objects."", ""Target claims"": [3, 17, 22], ""Reasoning"": ""This weakness questions the validity of the realism and efficiency claims of the paper because the evaluation is conducted solely on rendered rather than real-world objects, potentially limiting the applicability of the findings (Claim 3). It particularly challenges Claim 17, which asserts the effectiveness of the introduced optimization direction, and Claim 22, which claims superior quality in realism and text consistency, by suggesting that the results on rendered images may not translate to real-world performance.""}, {""Weakness span"": ""Furthermore, I find a lack of supportive material regarding its efficiency claims. Given that the method works based on optimization, I question whether it can truly generate high-quality textures swiftly, as stated in the introduction."", ""Target claims"": [3, 7, 17], ""Reasoning"": ""This excerpt challenges the claims related to the efficiency and speed of texture generation in the paper. Specifically, Claim 3 that mentions generating visually natural texture, Claim 7 about the efficient differentiable rasterization pipeline, and Claim 17 regarding the effectiveness of optimization, are targeted as the reviewer doubts whether high-quality textures can be generated quickly given the optimization-based approach.""}, {""Weakness span"": ""The proposed relative direction loss appears strikingly similar to that of NeRF-Art[1], albeit the latter employs the CLIP model to implement this constraint. The authors should highlight the distinguishing factors between their method and NeRF-Art in the introduction and add a direct comparison."", ""Target claims"": [9, 10, 28], ""Reasoning"": ""This weaknesses points to a lack of novelty in the proposed optimization direction by suggesting a similarity to an existing method (NeRF-Art), thus potentially undermining the claims of uniqueness and innovation in using relative direction (Claim 9 and 10) and replacing SDS with relative direction (Claim 28). The review suggests that the paper does not adequately distinguish itself from NeRF-Art, questioning its contribution to the field.""}]}, ""meta"": {""id"": ""cx8lw7WXY4"", ""review_idx"": 0, ""title"": ""Title: ITEM3D: Illumination-Aware Directional Texture Editing for 3D Models\nAbstract: Texture editing is a crucial task in 3D modeling that allows users to automatically manipulate the surface properties of 3D models. However, \nthe inherent complexity of 3D models and the ambiguous text description lead to the challenge in this task. \nTo address this challenge, we propose ITEM3D, an illumination-aware model for automatically 3D object editing according to the text prompts. \nLeveraging the power of diffusion model, ITEM3D takes the rendered images as the bridge of text and 3D representation and further optimizes the disentangled texture and environment map.\nPrevious methods adopt the absolute editing direction namely score distillation sampling (SDS) as the optimization objective, which unfortunately results in the noisy appearance and text inconsistency.\nTo solve the problem caused by the ambiguous text, we introduce a relative editing direction, a optimization objective defined by the noise difference between the source and target texts, to release the semantic ambiguity between the texts and images.\nAdditionally, we gradually adjust the direction during optimization to further address the unexpected deviation in texture domain. \nQualitative and quantitative experiments show that our ITEM3D outperforms SDS-based methods on various 3D objects. We also perform text-guided relighting to show the explicit control over lighting.\n\nTLDR:\n\nKeywords:Texture editing, diffusion model, relative direction, direction adjustment, relighting"", ""claims"": [""Claim1: Qualitative and quantitative experiments show that our ITEM3D outperforms SDS-based methods on various 3D objects."", ""Claim2: We also perform text-guided relighting to show the explicit control over lighting."", ""Claim3: To solve these problems, we present an efficient model, dubbed ITEM3D, which can generate visually natural texture corresponding to the text prompt generated by users."", ""Claim4: Thanks to the proposed solutions, our method overcomes the challenges of domain gap and learning misdirection, fulfilling all three requirements of texture editing."", ""Claim5: ITEM3D leverages an explicit mesh representation and optimizes texture and environment map."", ""Claim6: By supporting the decomposition of shape, materials and lighting, ITEM3D supports texture editing while preserving the topology by design."", ""Claim7: Additionally, ITEM3D employs an efficient differentiable rasterization pipeline for faster optimization."", ""Claim8: Unlike these methods that optimize in 2D image space, our ITEM3D utilizes the pre-trained diffusion model as a prior for 3D texture optimization."", ""Claim9: In contrast, our ITEM3D uses the relative direction to eliminate the semantic ambiguity of the target prompt towards the texture."", ""Claim10: To solve the problem of ambiguous and noisy details, we introduce the relative direction of source text and target text into the optimization."", ""Claim11: Moreover, we gradually adjust relative direction to address the challenges of deviation caused by the unbalanced optimization in the texture domain."", ""Claim12: Qualitative and quantitative experiments show that our ITEM3D outperforms SDS-based methods on various 3D objects."", ""Claim13: By adjusting the optimization direction step by step, we achieve more delicate and controllable editing, which can be seen in Sec. 4.3."", ""Claim14: We compare ITEM3D with the optimization method based on the SDS loss."", ""Claim15: While SDS-based method could edit textures along the direction of text prompt, their rendered images show the unrealistic appearance, sometimes overfitting to the text."", ""Claim16: In contrast, the texture edited by our ITEM3D can render realistic images with high quality, while remaining consistent with the input text prompt."", ""Claim17: The comparison indicates the effectiveness of the introduced relative direction of optimization and further direction adjustment."", ""Claim18: Besides, it can be noticed that our methods support segmentation-aware editing."", ""Claim19: Although the diffusion model lacks the capacity of recognizing the semantics in the texture map, it enables to edit the specific part of texture corresponding to a text prompt describing partial change."", ""Claim20: It proves that the gradients can accurately back-propagate to the corresponding parts of the texture map via the rendered images."", ""Claim21: As illustrated in Tab. 1, our method achieves better results than the SDS-based method."", ""Claim22: As presented in Tab. 2, the results demonstrate the superior quality with higher realism and more text consistency of our proposed method as compared to the baselines."", ""Claim23: The adjustment indeed helps to maintain the major semantics."", ""Claim24: The disentangled representation of environment map empowers ITEM3D to explicitly control the lighting under the guidance of a text prompt aiming to relight the 3D model."", ""Claim25: It is valuable to prove that the lighting condition of a 3D model can be learned solely from the text through the bridge of rendered 2D images."", ""Claim26: In conclusion, our ITEM3D model presents an efficient solution to the challenging task of texture editing for 3D models."", ""Claim27: By leveraging the power of diffusion models, ITEM3D is capable to optimize the texture and environment map under the guidance of text prompts."", ""Claim28: To address the semantic ambiguity between text prompts and images, we replace the traditional score distillation sampling (SDS) with a relative editing direction."", ""Claim29: We further propose a gradual direction adjustment during the optimization procedure, solving the unbalanced optimization in the texture."", ""Claim30: Despite the promising editing results, our ITEM3D still remains several limitations which should be solved in future work."", ""Claim31: The major limitation is that there remains irremovable noise in some samples."", ""Claim32: Because of the synthesis mechanism of the diffusion model, our ITEM3D extremely depends on the denoising ability of the pre-trained U-Net."", ""Claim33: Another limitation is that the adjustment by the source description is non-essential."", ""Claim34: Our further work aims to explore the learning scheme to solve the problem of unbalanced optimization in the texture.""], ""review"": ""Review: Weakness: 1. This paper asserts its realism and efficiency in generating new textures through optimization, yet its claims are evaluated solely on rendered images. It remains uncertain whether this method is applicable to in-the-wild objects. I suggest that the authors conduct additional experiments using real 3D objects, such as those in the DTU dataset. Furthermore, I find a lack of supportive material regarding its efficiency claims. Given that the method works based on optimization, I question whether it can truly generate high-quality textures swiftly, as stated in the introduction.\n\n2. The introduction lacks clarity and adequate context. The paper outlines two challenges when applying the diffusion model to 3D objects, but it doesn't discuss any related works aimed at addressing these challenges. This omission makes it difficult to gauge the novelty of the proposed method in relation to existing solutions.\n\n3. The proposed relative direction loss appears strikingly similar to that of NeRF-Art[1], albeit the latter employs the CLIP model to implement this constraint. The authors should highlight the distinguishing factors between their method and NeRF-Art in the introduction and add a direct comparison.\n\n\t[1] Wang, C., Jiang, R., Chai, M., He, M., Chen, D. and Liao, J., 2023. Nerf-art: Text-driven neural radiance fields stylization. IEEE Transactions on Visualization and Computer Graphics.\n\n4. In Figure 4, the examples provided are not representative enough. It would be beneficial to showcase results with brown, golden, and porcelain materials. The current examples make it difficult to evaluate the method's performance.\nQuestions: 1. The reconstruction results presented in Figure 5 seem inferior to those achieved by the original NeRF. The results appear quite coarse. Could the authors elaborate on the reasons for this?\n\n""}, ""review"": ""Title: ITEM3D: Illumination-Aware Directional Texture Editing for 3D Models\nAbstract: Texture editing is a crucial task in 3D modeling that allows users to automatically manipulate the surface properties of 3D models. However, \nthe inherent complexity of 3D models and the ambiguous text description lead to the challenge in this task. \nTo address this challenge, we propose ITEM3D, an illumination-aware model for automatically 3D object editing according to the text prompts. \nLeveraging the power of diffusion model, ITEM3D takes the rendered images as the bridge of text and 3D representation and further optimizes the disentangled texture and environment map.\nPrevious methods adopt the absolute editing direction namely score distillation sampling (SDS) as the optimization objective, which unfortunately results in the noisy appearance and text inconsistency.\nTo solve the problem caused by the ambiguous text, we introduce a relative editing direction, a optimization objective defined by the noise difference between the source and target texts, to release the semantic ambiguity between the texts and images.\nAdditionally, we gradually adjust the direction during optimization to further address the unexpected deviation in texture domain. \nQualitative and quantitative experiments show that our ITEM3D outperforms SDS-based methods on various 3D objects. We also perform text-guided relighting to show the explicit control over lighting.\n\nClaim1: Qualitative and quantitative experiments show that our ITEM3D outperforms SDS-based methods on various 3D objects.\nClaim2: We also perform text-guided relighting to show the explicit control over lighting.\nClaim3: To solve these problems, we present an efficient model, dubbed ITEM3D, which can generate visually natural texture corresponding to the text prompt generated by users.\nClaim4: Thanks to the proposed solutions, our method overcomes the challenges of domain gap and learning misdirection, fulfilling all three requirements of texture editing.\nClaim5: ITEM3D leverages an explicit mesh representation and optimizes texture and environment map.\nClaim6: By supporting the decomposition of shape, materials and lighting, ITEM3D supports texture editing while preserving the topology by design.\nClaim7: Additionally, ITEM3D employs an efficient differentiable rasterization pipeline for faster optimization.\nClaim8: Unlike these methods that optimize in 2D image space, our ITEM3D utilizes the pre-trained diffusion model as a prior for 3D texture optimization.\nClaim9: In contrast, our ITEM3D uses the relative direction to eliminate the semantic ambiguity of the target prompt towards the texture.\nClaim10: To solve the problem of ambiguous and noisy details, we introduce the relative direction of source text and target text into the optimization.\nClaim11: Moreover, we gradually adjust relative direction to address the challenges of deviation caused by the unbalanced optimization in the texture domain.\nClaim12: Qualitative and quantitative experiments show that our ITEM3D outperforms SDS-based methods on various 3D objects.\nClaim13: By adjusting the optimization direction step by step, we achieve more delicate and controllable editing, which can be seen in Sec. 4.3.\nClaim14: We compare ITEM3D with the optimization method based on the SDS loss.\nClaim15: While SDS-based method could edit textures along the direction of text prompt, their rendered images show the unrealistic appearance, sometimes overfitting to the text.\nClaim16: In contrast, the texture edited by our ITEM3D can render realistic images with high quality, while remaining consistent with the input text prompt.\nClaim17: The comparison indicates the effectiveness of the introduced relative direction of optimization and further direction adjustment.\nClaim18: Besides, it can be noticed that our methods support segmentation-aware editing.\nClaim19: Although the diffusion model lacks the capacity of recognizing the semantics in the texture map, it enables to edit the specific part of texture corresponding to a text prompt describing partial change.\nClaim20: It proves that the gradients can accurately back-propagate to the corresponding parts of the texture map via the rendered images.\nClaim21: As illustrated in Tab. 1, our method achieves better results than the SDS-based method.\nClaim22: As presented in Tab. 2, the results demonstrate the superior quality with higher realism and more text consistency of our proposed method as compared to the baselines.\nClaim23: The adjustment indeed helps to maintain the major semantics.\nClaim24: The disentangled representation of environment map empowers ITEM3D to explicitly control the lighting under the guidance of a text prompt aiming to relight the 3D model.\nClaim25: It is valuable to prove that the lighting condition of a 3D model can be learned solely from the text through the bridge of rendered 2D images.\nClaim26: In conclusion, our ITEM3D model presents an efficient solution to the challenging task of texture editing for 3D models.\nClaim27: By leveraging the power of diffusion models, ITEM3D is capable to optimize the texture and environment map under the guidance of text prompts.\nClaim28: To address the semantic ambiguity between text prompts and images, we replace the traditional score distillation sampling (SDS) with a relative editing direction.\nClaim29: We further propose a gradual direction adjustment during the optimization procedure, solving the unbalanced optimization in the texture.\nClaim30: Despite the promising editing results, our ITEM3D still remains several limitations which should be solved in future work.\nClaim31: The major limitation is that there remains irremovable noise in some samples.\nClaim32: Because of the synthesis mechanism of the diffusion model, our ITEM3D extremely depends on the denoising ability of the pre-trained U-Net.\nClaim33: Another limitation is that the adjustment by the source description is non-essential.\nClaim34: Our further work aims to explore the learning scheme to solve the problem of unbalanced optimization in the texture.\n\nReview: Weakness: 1. This paper asserts its realism and efficiency in generating new textures through optimization, yet its claims are evaluated solely on rendered images. It remains uncertain whether this method is applicable to in-the-wild objects. I suggest that the authors conduct additional experiments using real 3D objects, such as those in the DTU dataset. Furthermore, I find a lack of supportive material regarding its efficiency claims. Given that the method works based on optimization, I question whether it can truly generate high-quality textures swiftly, as stated in the introduction.\n\n2. The introduction lacks clarity and adequate context. The paper outlines two challenges when applying the diffusion model to 3D objects, but it doesn't discuss any related works aimed at addressing these challenges. This omission makes it difficult to gauge the novelty of the proposed method in relation to existing solutions.\n\n3. The proposed relative direction loss appears strikingly similar to that of NeRF-Art[1], albeit the latter employs the CLIP model to implement this constraint. The authors should highlight the distinguishing factors between their method and NeRF-Art in the introduction and add a direct comparison.\n\n\t[1] Wang, C., Jiang, R., Chai, M., He, M., Chen, D. and Liao, J., 2023. Nerf-art: Text-driven neural radiance fields stylization. IEEE Transactions on Visualization and Computer Graphics.\n\n4. In Figure 4, the examples provided are not representative enough. It would be beneficial to showcase results with brown, golden, and porcelain materials. The current examples make it difficult to evaluate the method's performance.\nQuestions: 1. The reconstruction results presented in Figure 5 seem inferior to those achieved by the original NeRF. The results appear quite coarse. Could the authors elaborate on the reasons for this?\n\n\n\n"", ""id"": ""cx8lw7WXY40"", ""pdf"": ""openreview.net/pdf?id=cx8lw7WXY4""}","11"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""The motivation is not convincing. Rank(A + B) < Rank (A) + Rank(B) doesn't guarantee Ranks(A + B) will be larger than min( Rank(A) + Rank(B)). Without any further constraints, there is no guarantee L93 is true. We just know the upper bound is higher but to claim it, we need to bound lower-bound, which is not discussed at all. The idea itself is interesting but rationale is wrong."", ""Target claims"": [24], ""Reasoning"": ""This span argues that the rationale behind Claim24 is questionable. Claim24 proposes that ReLoRA leverages 'the rank of sum property to train a high-rank network through multiple low-rank updates.' The reviewer here is questioning the validity of the claim by stating that the rank properties described do not guarantee the intended effect unless further conditions are specified, thus directly targeting the claim on the method's mathematical basis.""}, {""Weakness span"": ""Experiments are limited, as shown in Question sections."", ""Target claims"": [1, 2, 17], ""Reasoning"": ""The reviewer claims that the experiments are limited, which could question the evidence provided for Claim1, Claim2, and Claim17, where the paper claims comparable performance of ReLoRA to regular training and its increased efficiency with model size. Without adequate experiments, such results may not be fully convincing.""}, {""Weakness span"": ""A fundamental problem with the setup is that LoRA is used for fine-tune a pre-trained LLM 'for downstream tasks'. Thus, comparing PEFT methods on pre-training performance seems to be meaningless. Authors should compare ReLORA for both pre-train + fine-tuning vs full train + LoRA fine-tune and analyze the ranks or so to make readers understand what's going under the hood. Without this type of analysis (no matter ReLORA performs better or worse), the insights are limited."", ""Target claims"": [15, 17], ""Reasoning"": ""This weakness raises concerns about the adequacy of the experimental design, which undermines claims about the effectiveness of ReLoRA demonstrated on the C4 dataset and similar performance to full-rank training (Claim15 and Claim17). The reviewer suggests that the current comparison setup is inadequate for understanding the full capability of the proposed ReLoRA method.""}]}, ""meta"": {""id"": ""biaOpY5gAo"", ""review_idx"": 0, ""title"": ""Title: Stack More Layers Differently: High-Rank Training Through Low-Rank Updates\nAbstract: Despite the dominance and effectiveness of scaling, resulting in large networks with hundreds of billions of parameters,\nthe necessity to train overparametrized models remains poorly understood, and alternative approaches do not necessarily make it cheaper to train high-performance models.\nIn this paper, we explore low-rank training techniques as an alternative approach to training large neural networks. We introduce a novel method called ReLoRA, which utilizes low-rank updates to train high-rank networks.\nWe apply ReLoRA to pre-training transformer language models with up to 350M parameters and demonstrate comparable performance to regular neural network training. Furthermore, we observe that the efficiency of ReLoRA increases with model size, making it a promising approach for training multi-billion-parameter networks efficiently. Our findings shed light on the potential of low-rank training techniques and their implications for model scaling.\n\nTLDR:ReLoRA uses low-rank updates to efficiently train high-rank neural networks, demonstrating comparable performance to regular training, with increasing efficiency as model size grows.\n\nKeywords:efficient training, scaling laws, language models, low-rank training, parameter-efficient fine-tuning"", ""claims"": [""Claim1: We apply ReLoRA to pre-training transformer language models with up to 350M parameters and demonstrate comparable performance to regular neural network training."", ""Claim2: Furthermore, we observe that the efficiency of ReLoRA increases with model size, making it a promising approach for training multi-billion-parameter networks efficiently."", ""Claim3: Our findings shed light on the potential of low-rank training techniques and their implications for model scaling."", ""Claim4: We empirically demonstrate that ReLoRA performs a high-rank update and achieves performance similar to regular neural network training."", ""Claim5: Finally, we observe that the efficiency of ReLoRA increases with model size, making it a viable option for efficient training of multi-billion-parameter networks."", ""Claim6: Our own experiments (Section 3) also demonstrate that low-rank transformers perform significantly worse compared to the full-rank baseline and ReLoRA."", ""Claim7: To reiterate, ReLoRA is a low-rank training method inspired by LoRA that uses restarts to increase the effective rank of the update, uses partial optimizer reset, and a jagged scheduler to stabilize training and warm starts."", ""Claim8: All of this allows ReLoRA to achieve performance comparable to full-rank training, especially in large transformer networks, by only training a small set of parameters at a time."", ""Claim9: Enhancing computational efficiency Unlike other low-rank training techniques [44, 49], ReLoRA follows the LoRA approach by maintaining the frozen weights of the original network and adding new trainable parameters."", ""Claim10: These methods achieve significant improvements in training time and memory efficiency by reducing the size of the gradients and the optimizer states."", ""Claim11: By substantially reducing the number of trainable parameters, ReLoRA enables the utilization of larger batch sizes, maximizing hardware efficiency."", ""Claim12: Additionally, it reduces the bandwidth requirements in distributed setups, which are often the limiting factor in large-scale training."", ""Claim13: Furthermore, since the frozen parameters are not being updated between restarts, they can be kept in a low-precision quantized format, further reducing their memory and computational impact."", ""Claim14: This additional optimization contributes to overall improved efficiency in terms of memory utilization and computational resources of ReLoRA and increases at scale."", ""Claim15: To evaluate the effectiveness of ReLoRA, we apply it to train a transformer language model on the C4 dataset [41] using various model sizes: 60M, 130M, 250M, and 350M."", ""Claim16: ReLoRA significantly outperforms low-rank LoRA training demonstrating the effectiveness of our proposed modifications (ablated in Section 3)."", ""Claim17: Furthermore, ReLoRA achieves similar performance to full-rank training, and the performance gap diminishes as network size increases."", ""Claim18: Interestingly, the only model in which ReLoRA couldn\u2019t surpass the Control baseline was our smallest model with 60M parameters."", ""Claim19: This observation suggests that ReLoRA is particularly effective in improving the training of large networks, which aligns with our goal of developing a method that improves large-network training."", ""Claim20: While most of the singular values for LoRA are zero (Figure 4) with a noticeable number of exceptionally high values above 1.5, ReLoRA exhibits a higher distribution mass between 0.1 and 1.0, reminiscent of full-rank training."", ""Claim21: This observation emphasizes the significance of high-rank updates and demonstrates the qualitative efficacy of ReLoRA, which accomplishes a high-rank update by performing multiple low-rank updates."", ""Claim22: In this paper, we investigated low-rank training techniques for large transformer language models."", ""Claim23: We first examined the limitations of a simple low-rank matrix factorization (LoRA) approach and observed that it struggles to effectively train high-performing transformer models."", ""Claim24: To address this issue, we proposed a novel method called ReLoRA, which leverages the rank of sum property to train a high-rank network through multiple low-rank updates."", ""Claim25: Similar to the lottery ticket hypothesis with rewinding, ReLoRA employs a full-rank training warm start before transitioning to ReLoRA."", ""Claim26: Additionally, ReLoRA introduces a merge-and-reinit (restart) strategy, a jagged learning rate scheduler, and partial optimizer resets, which collectively enhance the efficiency of ReLoRA and bring it closer to full-rank training, particularly in large networks."", ""Claim27: ReLoRA efficiency increases with the network size making it a viable candidate for multi-billion-scale training."", ""Claim28: We firmly believe that the development of low-rank training methods holds great promise for improving the efficiency of training large language models and neural networks in general."", ""Claim29: Furthermore, low-rank training has the potential to provide valuable insights for the advancement of deep learning theories, aiding our understanding of neural network trainability through gradient descent and their exceptional generalization capabilities in the overparametrized regime."", ""Claim30: Nonetheless, ReLoRA already demonstrates promising results at this scale."", ""Claim31: Additionally, while the 350M model outperforms the Control baseline, it does not continue the trend of narrowing the gap between ReLoRA and full-rank training."", ""Claim32: At this scale, we observe 30% memory consumption reduction and 52% training throughput increase."", ""Claim33: We expect to observe even bigger improvements over the full-training baseline for larger networks since the number of trainable parameters for ReLoRA, similar to LoRA, increases at a much slower rate compared to the number of frozen parameters."", ""Claim34: Comparison to other low-rank training methods A number of approaches to low-rank training have been explored with other model architectures in earlier work.""], ""review"": ""Review: Weakness: Overall, I like the idea but somehow there are few concerns that I think needs to be improved in order to accept it.\n\n1. It's not well written as many there are many notation unexplained (L85 r), linkage error (e.g., L106 sec 3), and the algorithm is not well explained in a line by line fashion. Many details are skipped, which leads to the problem that reading the code is the only way to fully capture the proposed method.\n\n2. The motivation is not convincing. Rank(A + B) < Rank (A) + Rank(B) doesn't guarantee Ranks(A + B) will be larger than min( Rank(A) + Rank(B)). Without any further constraints, there is no guarantee L93 is true. We just know the upper bound is higher but to claim it, we need to bound lower-bound, which is not discussed at all. The idea itself is interesting but rationale is wrong. \n\n3. Experiments are limited, as shown in Question sections.\nQuestions: 1. A fundamental problem with the setup is that LoRA is used for fine-tune a pre-trained LLM \""for downstream tasks\"". Thus, comparing PEFT methods on pre-training performance seems to be meaningless. Authors should compare ReLORA for both pre-train + fine-tuning vs full train + LoRA fine-tune and analyze the ranks or so to make readers understand what's going under the hood. Without this type of analysis (no matter ReLORA performs better or worse), the insights are limited.\n\n2. Following 1, I'd like to see some analysis/experiments on downstream tasks like GLUE/super-GLUE tasks.\n\n3. Jagged Schedule is rather arbitrary. Not sure what's going on if I switch to another dataset, switch models and how it should be combined with fine-tuning. There is no theoretical analysis of the stability, nor experimental experiments to demonstrate it. \n\n4. Another baseline which should be included is training using plain SGD with Jagged Schedule. The above mentioned problems with Adam might totally be redundant given such a complex schedule designed. \n\n5. What's the formal definition of Control? It's not clear how it's performed and frankly speaking, the performance of control is fairly comparable to ReLoRAin Table 2. \n\n6. Selection of r needs to be discussed.\n\n""}, ""review"": ""Title: Stack More Layers Differently: High-Rank Training Through Low-Rank Updates\nAbstract: Despite the dominance and effectiveness of scaling, resulting in large networks with hundreds of billions of parameters,\nthe necessity to train overparametrized models remains poorly understood, and alternative approaches do not necessarily make it cheaper to train high-performance models.\nIn this paper, we explore low-rank training techniques as an alternative approach to training large neural networks. We introduce a novel method called ReLoRA, which utilizes low-rank updates to train high-rank networks.\nWe apply ReLoRA to pre-training transformer language models with up to 350M parameters and demonstrate comparable performance to regular neural network training. Furthermore, we observe that the efficiency of ReLoRA increases with model size, making it a promising approach for training multi-billion-parameter networks efficiently. Our findings shed light on the potential of low-rank training techniques and their implications for model scaling.\n\nClaim1: We apply ReLoRA to pre-training transformer language models with up to 350M parameters and demonstrate comparable performance to regular neural network training.\nClaim2: Furthermore, we observe that the efficiency of ReLoRA increases with model size, making it a promising approach for training multi-billion-parameter networks efficiently.\nClaim3: Our findings shed light on the potential of low-rank training techniques and their implications for model scaling.\nClaim4: We empirically demonstrate that ReLoRA performs a high-rank update and achieves performance similar to regular neural network training.\nClaim5: Finally, we observe that the efficiency of ReLoRA increases with model size, making it a viable option for efficient training of multi-billion-parameter networks.\nClaim6: Our own experiments (Section 3) also demonstrate that low-rank transformers perform significantly worse compared to the full-rank baseline and ReLoRA.\nClaim7: To reiterate, ReLoRA is a low-rank training method inspired by LoRA that uses restarts to increase the effective rank of the update, uses partial optimizer reset, and a jagged scheduler to stabilize training and warm starts.\nClaim8: All of this allows ReLoRA to achieve performance comparable to full-rank training, especially in large transformer networks, by only training a small set of parameters at a time.\nClaim9: Enhancing computational efficiency Unlike other low-rank training techniques [44, 49], ReLoRA follows the LoRA approach by maintaining the frozen weights of the original network and adding new trainable parameters.\nClaim10: These methods achieve significant improvements in training time and memory efficiency by reducing the size of the gradients and the optimizer states.\nClaim11: By substantially reducing the number of trainable parameters, ReLoRA enables the utilization of larger batch sizes, maximizing hardware efficiency.\nClaim12: Additionally, it reduces the bandwidth requirements in distributed setups, which are often the limiting factor in large-scale training.\nClaim13: Furthermore, since the frozen parameters are not being updated between restarts, they can be kept in a low-precision quantized format, further reducing their memory and computational impact.\nClaim14: This additional optimization contributes to overall improved efficiency in terms of memory utilization and computational resources of ReLoRA and increases at scale.\nClaim15: To evaluate the effectiveness of ReLoRA, we apply it to train a transformer language model on the C4 dataset [41] using various model sizes: 60M, 130M, 250M, and 350M.\nClaim16: ReLoRA significantly outperforms low-rank LoRA training demonstrating the effectiveness of our proposed modifications (ablated in Section 3).\nClaim17: Furthermore, ReLoRA achieves similar performance to full-rank training, and the performance gap diminishes as network size increases.\nClaim18: Interestingly, the only model in which ReLoRA couldn\u2019t surpass the Control baseline was our smallest model with 60M parameters.\nClaim19: This observation suggests that ReLoRA is particularly effective in improving the training of large networks, which aligns with our goal of developing a method that improves large-network training.\nClaim20: While most of the singular values for LoRA are zero (Figure 4) with a noticeable number of exceptionally high values above 1.5, ReLoRA exhibits a higher distribution mass between 0.1 and 1.0, reminiscent of full-rank training.\nClaim21: This observation emphasizes the significance of high-rank updates and demonstrates the qualitative efficacy of ReLoRA, which accomplishes a high-rank update by performing multiple low-rank updates.\nClaim22: In this paper, we investigated low-rank training techniques for large transformer language models.\nClaim23: We first examined the limitations of a simple low-rank matrix factorization (LoRA) approach and observed that it struggles to effectively train high-performing transformer models.\nClaim24: To address this issue, we proposed a novel method called ReLoRA, which leverages the rank of sum property to train a high-rank network through multiple low-rank updates.\nClaim25: Similar to the lottery ticket hypothesis with rewinding, ReLoRA employs a full-rank training warm start before transitioning to ReLoRA.\nClaim26: Additionally, ReLoRA introduces a merge-and-reinit (restart) strategy, a jagged learning rate scheduler, and partial optimizer resets, which collectively enhance the efficiency of ReLoRA and bring it closer to full-rank training, particularly in large networks.\nClaim27: ReLoRA efficiency increases with the network size making it a viable candidate for multi-billion-scale training.\nClaim28: We firmly believe that the development of low-rank training methods holds great promise for improving the efficiency of training large language models and neural networks in general.\nClaim29: Furthermore, low-rank training has the potential to provide valuable insights for the advancement of deep learning theories, aiding our understanding of neural network trainability through gradient descent and their exceptional generalization capabilities in the overparametrized regime.\nClaim30: Nonetheless, ReLoRA already demonstrates promising results at this scale.\nClaim31: Additionally, while the 350M model outperforms the Control baseline, it does not continue the trend of narrowing the gap between ReLoRA and full-rank training.\nClaim32: At this scale, we observe 30% memory consumption reduction and 52% training throughput increase.\nClaim33: We expect to observe even bigger improvements over the full-training baseline for larger networks since the number of trainable parameters for ReLoRA, similar to LoRA, increases at a much slower rate compared to the number of frozen parameters.\nClaim34: Comparison to other low-rank training methods A number of approaches to low-rank training have been explored with other model architectures in earlier work.\n\nReview: Weakness: Overall, I like the idea but somehow there are few concerns that I think needs to be improved in order to accept it.\n\n1. It's not well written as many there are many notation unexplained (L85 r), linkage error (e.g., L106 sec 3), and the algorithm is not well explained in a line by line fashion. Many details are skipped, which leads to the problem that reading the code is the only way to fully capture the proposed method.\n\n2. The motivation is not convincing. Rank(A + B) < Rank (A) + Rank(B) doesn't guarantee Ranks(A + B) will be larger than min( Rank(A) + Rank(B)). Without any further constraints, there is no guarantee L93 is true. We just know the upper bound is higher but to claim it, we need to bound lower-bound, which is not discussed at all. The idea itself is interesting but rationale is wrong. \n\n3. Experiments are limited, as shown in Question sections.\nQuestions: 1. A fundamental problem with the setup is that LoRA is used for fine-tune a pre-trained LLM \""for downstream tasks\"". Thus, comparing PEFT methods on pre-training performance seems to be meaningless. Authors should compare ReLORA for both pre-train + fine-tuning vs full train + LoRA fine-tune and analyze the ranks or so to make readers understand what's going under the hood. Without this type of analysis (no matter ReLORA performs better or worse), the insights are limited.\n\n2. Following 1, I'd like to see some analysis/experiments on downstream tasks like GLUE/super-GLUE tasks.\n\n3. Jagged Schedule is rather arbitrary. Not sure what's going on if I switch to another dataset, switch models and how it should be combined with fine-tuning. There is no theoretical analysis of the stability, nor experimental experiments to demonstrate it. \n\n4. Another baseline which should be included is training using plain SGD with Jagged Schedule. The above mentioned problems with Adam might totally be redundant given such a complex schedule designed. \n\n5. What's the formal definition of Control? It's not clear how it's performed and frankly speaking, the performance of control is fairly comparable to ReLoRAin Table 2. \n\n6. Selection of r needs to be discussed.\n\n\n\n"", ""id"": ""biaOpY5gAo0"", ""pdf"": ""openreview.net/pdf?id=biaOpY5gAo""}","12"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""The biggest weakness of this paper is the small number of experiments, which also are conducted on a single task (sudoku). In the text, many different versions of ToT are discussed, however, experiments are done only for a single setup and a single task. ToT was not tested on any other task, thus we cannot know if it really generalizes at all."", ""Target claims"": [21], ""Reasoning"": ""The reviewer points out the limited scope of experimentation, which questions the generalizability of the ToT framework. This directly relates to Claim21, which suggests improved performance for mathematical problem-solving tasks broadly. Without broader experimental validation, the claim regarding the general applicability and improvement across various tasks is weakened.""}, {""Weakness span"": ""There are far too few experimental results and data. What is missing: ... If it was 10 boards then the results cannot be trusted at all. In such a case for the 0.4 success rate, the 90% confidence interval is (0.15 - 0.7), which tells nothing about the real results. Results on 10 testing boards have no scientific meaning and this is the main reason for such a low score I gave."", ""Target claims"": [3, 16, 17, 18, 19, 20], ""Reasoning"": ""The reviewer highlights the insufficient number of test cases, particularly for Sudoku, raising concerns about the reliability and scientific validity of the reported success rates and improvements. This directly questions Claim3 as it refers to the evaluation using Sudoku puzzle benchmarks. Similarly, Claims16 to 20 address the success rates and comparisons with other solvers, which are undermined by the lack of adequate experimental data.""}, {""Weakness span"": ""Authors claim that: \""If the ToT framework can solve instances of the generalized Sudoku [...] in principle it can handle many other mathematical and logical reasoning tasks.\"". The claim that ToT should be able to handle other complex problems is based on the idea that complex tasks require a similar way of thinking More advanced mathematical problems like automated theorem proving have their own sources of complexity (e.g. choosing the appropriate lemmas to consider or how to represent the state in a compact form, which fits to the transformer). I know that authors do not claim that for sure ToT works in such tasks, but after reading the paper it seems that the significance of the paper is built upon a promise that ToT can be easily adapted to more serious problems. Since there are no experiments to support this claim I think that the significance of this paper is limited."", ""Target claims"": [14], ""Reasoning"": ""The review questions the claim regarding the adaptability of the ToT framework to more complex tasks due to the lack of experimentation beyond Sudoku. This specifically challenges Claim14 since it suggests an expansion of problem-solving capabilities, drawing on theoretical justification rather than empirical evidence. Hence, the significance and general adaptability suggested by this claim are deemed speculative.""}]}, ""meta"": {""id"": ""a648X9AoL4"", ""review_idx"": 0, ""title"": ""Title: Large Language Model Guided Tree-of-Thought\nAbstract: In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implement a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving.\n\nTLDR:This paper introduces the concept of Tree-of-Thought (ToT) which enhances the problem solving capability of LLMs by allowing LLM to solve a given problem through trial and error via a multi-round conversation with a prompter agent.\n\nKeywords:LLM, tree of thought, problem solving"", ""claims"": [""Claim1: Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving."", ""Claim2: Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving."", ""Claim3: To evaluate the effectiveness of the ToT framework, we implemented a ToT-based Sudoku puzzle solver and evaluate it on a suite of Sudoku puzzle benchmarks we created."", ""Claim4: As shown by the experimental results in Section 4.2, the ToT framework can significantly increase the success rate of Sudoku puzzle solving."", ""Claim5: Our proposal shares some commonalities with these approaches, for example, the use of a memory module and additional agents for automatic prompt generation."", ""Claim6: However, our approach is unique in that it introduces a ToT controller which can explicitly conduct backtracking when necessary."", ""Claim7: This not only allows the system to recover from mistakes, but potentially can also enlarge the solution search space."", ""Claim8: The ToT strategy can be viewed as a tree-search algorithm using an LLM as a heuristic for generating the search steps."", ""Claim9: In this setting, the LLM is only used for the \u201cshort-range reasoning\u201d tasks, i.e deriving the next intermediate solution, which is a type of tasks that have been shown to have a high success rate for LLMs."", ""Claim10: On the other hand, by introducing the checker, the system have a higher likelihood to discover the mistakes it makes as it generates the solutions."", ""Claim11: Moreover, by allowing the system to backtrack from a valid but somewhat \u201chopeless\u201d intermediate solution, the system is able to explore a larger solution space, which enhances the \u201clong-range reasoning\u201d capability of the system as a whole."", ""Claim12: The ToT framework thus combines the best of both world."", ""Claim13: Furthermore, this multi-round conversation technique increases the number of computation steps the system can perform."", ""Claim14: Thus, based on the time hierarchy theorem in computational complexity theory, the ToT framework can expand the range of problems that can potentially be solved compared to relying solely on a single round of conversation with an LLM."", ""Claim15: Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving."", ""Claim16: In comparison, the ToT-based solver demonstrates superior performance when compared to the other solvers."", ""Claim17: For the 3x3 benchmark set, it was able to solve all the puzzles."", ""Claim18: The success rate improves by 11% compared to the second best for the two benchmark sets."", ""Claim19: Despite this, the success rate of the ToT based solver is still 80% higher compared to that of the one-shot and few-shot based solvers."", ""Claim20: Nonetheless, the success rate is 60% higher compared to that of the one-shot and few-shot based solvers."", ""Claim21: In this paper, we presented the Tree-of-Thought framework, which enhances LLMs with additional agents and memory modules, resulting in improved performance for mathematical problem-solving tasks."", ""Claim22: To evaluate the performance of this technique, we implemented a Sudoku puzzle solver based on the ToT framework.""], ""review"": ""Review: Weakness: The biggest weakness of this paper is the small number of experiments, which also are conducted on a single task (sudoku). In the text, many different versions of ToT are discussed, however, experiments are done only for a single setup and a single task. ToT was not tested on any other task, thus we cannot know if it really generalizes at all.\n\nThere are far too few experimental results and data. What is missing:\n- How many nodes ToT needs on average to solve a given task?\n- How many steps baseline needs to solve a given task?\n- There are only 3 versions of the Sudoku: for n=3, 4, and 5. (this would be ok if there was more tasks). What happens for n >= 6? If ToT is still the best then it would be great for the method. If not, we will clearly know where is the limit for ToT. If the method is too slow for n>=6 it is important to know.\n- What is the price (or number of tokens needed) on average for a single ToT run?\n- What was the number of Sudoku puzzles used for evaluation? It is not clearly stated in the text, however, I guess it was 10 boards for each n=3,4,5. If I am wrong please correct me. If it was 10 boards then the results cannot be trusted at all. In such a case for the 0.4 success rate, the 90% confidence interval is (0.15 - 0.7), which tells nothing about the real results. Results on 10 testing boards have no scientific meaning and this is the main reason for such a low score I gave. If I am wrong (and the number is higher I will be happy to improve the score).\n-There are no error bars in Figure 2.\n\n\nAuthors claim that: \""If the ToT framework can solve instances of the generalized Sudoku [...] in principle it can handle many other mathematical and logical reasoning tasks.\"". The claim that ToT should be able to handle other complex problems is based on the idea that complex tasks require a similar way of thinking  More advanced mathematical problems like automated theorem proving have their own sources of complexity (e.g. choosing the appropriate lemmas to consider or how to represent the state in a compact form, which fits to the transformer). I know that authors do not claim that for sure ToT works in such tasks, but after reading the paper it seems that the significance of the paper is built upon a promise that ToT can be easily adapted to more serious problems. Since there are no experiments to support this claim I think that the significance of this paper is limited. \n\nThere are no experiments concerning other variants of ToT: for example with neural network checker or rule-based policy. \n\nNotation in Algorithm 1 is hard to understand. I had trouble understanding which \\pi stands for ToT policy and which for prompt policy. Please consider more natural notation like \\pi_{tot} \\pi_{prompt} or similar. \n\nAlgorithm 2. The meaning of (nil) is not introduced in the algorithm, it is only later in the text. \n\nI think that the version of the text review should use specific LaTeX options: for example line numbers. It is hard for me to refer to concrete lines without them.\n\nIt makes no sense for me to describe the procedure of training ToT policy if it was not used in the experiments.\nQuestions: The most important: what was the number of test boards for each n?\n\nWhat is the hierarchy theorem? You should briefly explain it in the paper for readers who are not familiar with complexity theory.\n\nEquations (1) and (3): what is s_i precisely? How it is represented?\n\nWhat networks were trained in the experiments? It is hard to find the text.\n\n""}, ""review"": ""Title: Large Language Model Guided Tree-of-Thought\nAbstract: In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implement a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving.\n\nClaim1: Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving.\nClaim2: Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving.\nClaim3: To evaluate the effectiveness of the ToT framework, we implemented a ToT-based Sudoku puzzle solver and evaluate it on a suite of Sudoku puzzle benchmarks we created.\nClaim4: As shown by the experimental results in Section 4.2, the ToT framework can significantly increase the success rate of Sudoku puzzle solving.\nClaim5: Our proposal shares some commonalities with these approaches, for example, the use of a memory module and additional agents for automatic prompt generation.\nClaim6: However, our approach is unique in that it introduces a ToT controller which can explicitly conduct backtracking when necessary.\nClaim7: This not only allows the system to recover from mistakes, but potentially can also enlarge the solution search space.\nClaim8: The ToT strategy can be viewed as a tree-search algorithm using an LLM as a heuristic for generating the search steps.\nClaim9: In this setting, the LLM is only used for the \u201cshort-range reasoning\u201d tasks, i.e deriving the next intermediate solution, which is a type of tasks that have been shown to have a high success rate for LLMs.\nClaim10: On the other hand, by introducing the checker, the system have a higher likelihood to discover the mistakes it makes as it generates the solutions.\nClaim11: Moreover, by allowing the system to backtrack from a valid but somewhat \u201chopeless\u201d intermediate solution, the system is able to explore a larger solution space, which enhances the \u201clong-range reasoning\u201d capability of the system as a whole.\nClaim12: The ToT framework thus combines the best of both world.\nClaim13: Furthermore, this multi-round conversation technique increases the number of computation steps the system can perform.\nClaim14: Thus, based on the time hierarchy theorem in computational complexity theory, the ToT framework can expand the range of problems that can potentially be solved compared to relying solely on a single round of conversation with an LLM.\nClaim15: Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving.\nClaim16: In comparison, the ToT-based solver demonstrates superior performance when compared to the other solvers.\nClaim17: For the 3x3 benchmark set, it was able to solve all the puzzles.\nClaim18: The success rate improves by 11% compared to the second best for the two benchmark sets.\nClaim19: Despite this, the success rate of the ToT based solver is still 80% higher compared to that of the one-shot and few-shot based solvers.\nClaim20: Nonetheless, the success rate is 60% higher compared to that of the one-shot and few-shot based solvers.\nClaim21: In this paper, we presented the Tree-of-Thought framework, which enhances LLMs with additional agents and memory modules, resulting in improved performance for mathematical problem-solving tasks.\nClaim22: To evaluate the performance of this technique, we implemented a Sudoku puzzle solver based on the ToT framework.\n\nReview: Weakness: The biggest weakness of this paper is the small number of experiments, which also are conducted on a single task (sudoku). In the text, many different versions of ToT are discussed, however, experiments are done only for a single setup and a single task. ToT was not tested on any other task, thus we cannot know if it really generalizes at all.\n\nThere are far too few experimental results and data. What is missing:\n- How many nodes ToT needs on average to solve a given task?\n- How many steps baseline needs to solve a given task?\n- There are only 3 versions of the Sudoku: for n=3, 4, and 5. (this would be ok if there was more tasks). What happens for n >= 6? If ToT is still the best then it would be great for the method. If not, we will clearly know where is the limit for ToT. If the method is too slow for n>=6 it is important to know.\n- What is the price (or number of tokens needed) on average for a single ToT run?\n- What was the number of Sudoku puzzles used for evaluation? It is not clearly stated in the text, however, I guess it was 10 boards for each n=3,4,5. If I am wrong please correct me. If it was 10 boards then the results cannot be trusted at all. In such a case for the 0.4 success rate, the 90% confidence interval is (0.15 - 0.7), which tells nothing about the real results. Results on 10 testing boards have no scientific meaning and this is the main reason for such a low score I gave. If I am wrong (and the number is higher I will be happy to improve the score).\n-There are no error bars in Figure 2.\n\n\nAuthors claim that: \""If the ToT framework can solve instances of the generalized Sudoku [...] in principle it can handle many other mathematical and logical reasoning tasks.\"". The claim that ToT should be able to handle other complex problems is based on the idea that complex tasks require a similar way of thinking  More advanced mathematical problems like automated theorem proving have their own sources of complexity (e.g. choosing the appropriate lemmas to consider or how to represent the state in a compact form, which fits to the transformer). I know that authors do not claim that for sure ToT works in such tasks, but after reading the paper it seems that the significance of the paper is built upon a promise that ToT can be easily adapted to more serious problems. Since there are no experiments to support this claim I think that the significance of this paper is limited. \n\nThere are no experiments concerning other variants of ToT: for example with neural network checker or rule-based policy. \n\nNotation in Algorithm 1 is hard to understand. I had trouble understanding which \\pi stands for ToT policy and which for prompt policy. Please consider more natural notation like \\pi_{tot} \\pi_{prompt} or similar. \n\nAlgorithm 2. The meaning of (nil) is not introduced in the algorithm, it is only later in the text. \n\nI think that the version of the text review should use specific LaTeX options: for example line numbers. It is hard for me to refer to concrete lines without them.\n\nIt makes no sense for me to describe the procedure of training ToT policy if it was not used in the experiments.\nQuestions: The most important: what was the number of test boards for each n?\n\nWhat is the hierarchy theorem? You should briefly explain it in the paper for readers who are not familiar with complexity theory.\n\nEquations (1) and (3): what is s_i precisely? How it is represented?\n\nWhat networks were trained in the experiments? It is hard to find the text.\n\n\n\n"", ""id"": ""a648X9AoL40"", ""pdf"": ""openreview.net/pdf?id=a648X9AoL4""}","13"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""One of the biggest issues of this paper is the mismatch between the described method and the actual one used in the experiments. The paper spends lots of space talking about how the ToT controller and prompter agent can be modeled by a policy network and trained via multi-agent RL. But it never tried such formulation and training in the experiments and only presented them as some kind of future work. Without valid evidence, empirically or theoretically, the method section is largely questionable."", ""Target claims"": [], ""Reasoning"": ""This span questions the validity of the method due to a mismatch between the description and the actual implementation in experiments. However, it does not directly challenge whether the claims about the experimental results (such as improved Sudoku solving success rate) are true or questionable, but rather criticizes the presentation of the proposed technique's novelty and implementation. Hence, this is not directly linked to specific claims about experimental results or the technique's effectiveness as declared in the claims.""}, {""Weakness span"": ""Another issue is the novelty of this work probably is not as big as the paper claims. The formulation of multi-agent RL for controller and agent probably is overcomplicated, and I encourage the authors to think of reformulating them all as LLMs to make things easier. Also, there are many missing related works [1, 2, 3, 4] that have a similar tree search/reasoning formulation with more operational and rigorous experiments."", ""Target claims"": [], ""Reasoning"": ""The span argues that the novelty of the method is overstated and points out missing related works. However, this critique is directed more towards the novelty and complexity of the approach rather than disputing the claimed results or successes of the ToT framework presented in claims. Therefore, it doesn't directly counter any individual claims about the results or contributions.""}, {""Weakness span"": ""The experiment scope is limited. The proposed method is only demonstrated in one task with the simple formulation of the controller and agent (discussed above). This isn't ok for NeurIPS papers, and we need to better figure out why and how the proposed method can or cannot be applied to more general tasks."", ""Target claims"": [1, 3, 21], ""Reasoning"": ""This weakness relates to the limitation of experimental validation to a single task (Sudoku) which may undermine the claims related to the general effectiveness and applicability of the ToT framework for problem-solving tasks, as mentioned in Claim 1, Claim 3, and Claim 21. The claims suggest improved problem-solving capabilities and evaluation on several Sudoku puzzle benchmarks, but the reviewer points out the narrow application scope, suggesting that such claims may not have comprehensive empirical support.""}]}, ""meta"": {""id"": ""a648X9AoL4"", ""review_idx"": 1, ""title"": ""Title: Large Language Model Guided Tree-of-Thought\nAbstract: In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implement a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving.\n\nTLDR:This paper introduces the concept of Tree-of-Thought (ToT) which enhances the problem solving capability of LLMs by allowing LLM to solve a given problem through trial and error via a multi-round conversation with a prompter agent.\n\nKeywords:LLM, tree of thought, problem solving"", ""claims"": [""Claim1: Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving."", ""Claim2: Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving."", ""Claim3: To evaluate the effectiveness of the ToT framework, we implemented a ToT-based Sudoku puzzle solver and evaluate it on a suite of Sudoku puzzle benchmarks we created."", ""Claim4: As shown by the experimental results in Section 4.2, the ToT framework can significantly increase the success rate of Sudoku puzzle solving."", ""Claim5: Our proposal shares some commonalities with these approaches, for example, the use of a memory module and additional agents for automatic prompt generation."", ""Claim6: However, our approach is unique in that it introduces a ToT controller which can explicitly conduct backtracking when necessary."", ""Claim7: This not only allows the system to recover from mistakes, but potentially can also enlarge the solution search space."", ""Claim8: The ToT strategy can be viewed as a tree-search algorithm using an LLM as a heuristic for generating the search steps."", ""Claim9: In this setting, the LLM is only used for the \u201cshort-range reasoning\u201d tasks, i.e deriving the next intermediate solution, which is a type of tasks that have been shown to have a high success rate for LLMs."", ""Claim10: On the other hand, by introducing the checker, the system have a higher likelihood to discover the mistakes it makes as it generates the solutions."", ""Claim11: Moreover, by allowing the system to backtrack from a valid but somewhat \u201chopeless\u201d intermediate solution, the system is able to explore a larger solution space, which enhances the \u201clong-range reasoning\u201d capability of the system as a whole."", ""Claim12: The ToT framework thus combines the best of both world."", ""Claim13: Furthermore, this multi-round conversation technique increases the number of computation steps the system can perform."", ""Claim14: Thus, based on the time hierarchy theorem in computational complexity theory, the ToT framework can expand the range of problems that can potentially be solved compared to relying solely on a single round of conversation with an LLM."", ""Claim15: Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving."", ""Claim16: In comparison, the ToT-based solver demonstrates superior performance when compared to the other solvers."", ""Claim17: For the 3x3 benchmark set, it was able to solve all the puzzles."", ""Claim18: The success rate improves by 11% compared to the second best for the two benchmark sets."", ""Claim19: Despite this, the success rate of the ToT based solver is still 80% higher compared to that of the one-shot and few-shot based solvers."", ""Claim20: Nonetheless, the success rate is 60% higher compared to that of the one-shot and few-shot based solvers."", ""Claim21: In this paper, we presented the Tree-of-Thought framework, which enhances LLMs with additional agents and memory modules, resulting in improved performance for mathematical problem-solving tasks."", ""Claim22: To evaluate the performance of this technique, we implemented a Sudoku puzzle solver based on the ToT framework.""], ""review"": ""Review: Weakness: 1. One of the biggest issues of this paper is the mismatch between the described method and the actual one used in the experiments. The paper spends lots of space talking about how the ToT controller and prompter agent can be modeled by a policy network and trained via multi-agent RL. But it never tried such formulation and training in the experiments and only presented them as some kind of future work. Without valid evidence, empirically or theoretically, the method section is largely questionable. \n2. Another issue is the novelty of this work probably is not as big as the paper claims. The formulation of multi-agent RL for controller and agent probably is overcomplicated, and I encourage the authors to think of reformulating them all as LLMs to make things easier. Also, there are many missing related works [1, 2, 3, 4] that have a similar tree search/reasoning formulation with more operational and rigorous experiments.\n3. The experiment scope is limited. The proposed method is only demonstrated in one task with the simple formulation of the controller and agent (discussed above). This isn't ok for NeurIPS papers, and we need to better figure out why and how the proposed method can or cannot be applied to more general tasks. \nQuestions: I raised some questions in the weaknesses section, and there could be lots of improvement space for the authors to make and answer those questions. The following are some minor comments:\n\n1. It seems the catchy name of Tree-of-though has been popularized by another work [5], which draws far more attention than this one; I'd suggest the authors rethink the core contributions of the proposed method and position it in a different and unique way \n2. I wonder whether the authors could explicitly explain what kind of search algorithms are used in the proposed method for a better understanding of the method.\n\n[1]. Xie, Yuxi, et al. \""Decomposition enhances reasoning via self-evaluation guided decoding.\"" arXiv preprint arXiv:2305.00633 (2023). \n\n[2]. Jung, Jaehun, et al. \""Maieutic prompting: Logically consistent reasoning with recursive explanations.\"" arXiv preprint arXiv:2205.11822 (2022). \n\n[3]. Zhu, Xinyu, et al. \""Solving math word problem via cooperative reasoning induced language models.\"" arXiv preprint arXiv:2210.16257 (2022). \n\n[4]. Hao, Shibo, et al. \""Reasoning with language model is planning with world model.\"" arXiv preprint arXiv:2305.14992 (2023).\n\n[5]. Yao, Shunyu, et al. \""Tree of thoughts: Deliberate problem solving with large language models.\"" arXiv preprint arXiv:2305.10601 (2023).\n\n""}, ""review"": ""Title: Large Language Model Guided Tree-of-Thought\nAbstract: In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implement a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving.\n\nClaim1: Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving.\nClaim2: Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving.\nClaim3: To evaluate the effectiveness of the ToT framework, we implemented a ToT-based Sudoku puzzle solver and evaluate it on a suite of Sudoku puzzle benchmarks we created.\nClaim4: As shown by the experimental results in Section 4.2, the ToT framework can significantly increase the success rate of Sudoku puzzle solving.\nClaim5: Our proposal shares some commonalities with these approaches, for example, the use of a memory module and additional agents for automatic prompt generation.\nClaim6: However, our approach is unique in that it introduces a ToT controller which can explicitly conduct backtracking when necessary.\nClaim7: This not only allows the system to recover from mistakes, but potentially can also enlarge the solution search space.\nClaim8: The ToT strategy can be viewed as a tree-search algorithm using an LLM as a heuristic for generating the search steps.\nClaim9: In this setting, the LLM is only used for the \u201cshort-range reasoning\u201d tasks, i.e deriving the next intermediate solution, which is a type of tasks that have been shown to have a high success rate for LLMs.\nClaim10: On the other hand, by introducing the checker, the system have a higher likelihood to discover the mistakes it makes as it generates the solutions.\nClaim11: Moreover, by allowing the system to backtrack from a valid but somewhat \u201chopeless\u201d intermediate solution, the system is able to explore a larger solution space, which enhances the \u201clong-range reasoning\u201d capability of the system as a whole.\nClaim12: The ToT framework thus combines the best of both world.\nClaim13: Furthermore, this multi-round conversation technique increases the number of computation steps the system can perform.\nClaim14: Thus, based on the time hierarchy theorem in computational complexity theory, the ToT framework can expand the range of problems that can potentially be solved compared to relying solely on a single round of conversation with an LLM.\nClaim15: Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving.\nClaim16: In comparison, the ToT-based solver demonstrates superior performance when compared to the other solvers.\nClaim17: For the 3x3 benchmark set, it was able to solve all the puzzles.\nClaim18: The success rate improves by 11% compared to the second best for the two benchmark sets.\nClaim19: Despite this, the success rate of the ToT based solver is still 80% higher compared to that of the one-shot and few-shot based solvers.\nClaim20: Nonetheless, the success rate is 60% higher compared to that of the one-shot and few-shot based solvers.\nClaim21: In this paper, we presented the Tree-of-Thought framework, which enhances LLMs with additional agents and memory modules, resulting in improved performance for mathematical problem-solving tasks.\nClaim22: To evaluate the performance of this technique, we implemented a Sudoku puzzle solver based on the ToT framework.\n\nReview: Weakness: 1. One of the biggest issues of this paper is the mismatch between the described method and the actual one used in the experiments. The paper spends lots of space talking about how the ToT controller and prompter agent can be modeled by a policy network and trained via multi-agent RL. But it never tried such formulation and training in the experiments and only presented them as some kind of future work. Without valid evidence, empirically or theoretically, the method section is largely questionable. \n2. Another issue is the novelty of this work probably is not as big as the paper claims. The formulation of multi-agent RL for controller and agent probably is overcomplicated, and I encourage the authors to think of reformulating them all as LLMs to make things easier. Also, there are many missing related works [1, 2, 3, 4] that have a similar tree search/reasoning formulation with more operational and rigorous experiments.\n3. The experiment scope is limited. The proposed method is only demonstrated in one task with the simple formulation of the controller and agent (discussed above). This isn't ok for NeurIPS papers, and we need to better figure out why and how the proposed method can or cannot be applied to more general tasks. \nQuestions: I raised some questions in the weaknesses section, and there could be lots of improvement space for the authors to make and answer those questions. The following are some minor comments:\n\n1. It seems the catchy name of Tree-of-though has been popularized by another work [5], which draws far more attention than this one; I'd suggest the authors rethink the core contributions of the proposed method and position it in a different and unique way \n2. I wonder whether the authors could explicitly explain what kind of search algorithms are used in the proposed method for a better understanding of the method.\n\n[1]. Xie, Yuxi, et al. \""Decomposition enhances reasoning via self-evaluation guided decoding.\"" arXiv preprint arXiv:2305.00633 (2023). \n\n[2]. Jung, Jaehun, et al. \""Maieutic prompting: Logically consistent reasoning with recursive explanations.\"" arXiv preprint arXiv:2205.11822 (2022). \n\n[3]. Zhu, Xinyu, et al. \""Solving math word problem via cooperative reasoning induced language models.\"" arXiv preprint arXiv:2210.16257 (2022). \n\n[4]. Hao, Shibo, et al. \""Reasoning with language model is planning with world model.\"" arXiv preprint arXiv:2305.14992 (2023).\n\n[5]. Yao, Shunyu, et al. \""Tree of thoughts: Deliberate problem solving with large language models.\"" arXiv preprint arXiv:2305.10601 (2023).\n\n\n\n"", ""id"": ""a648X9AoL41"", ""pdf"": ""openreview.net/pdf?id=a648X9AoL4""}","14"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""Although the high-level idea of tree-of-thought is promising, with corresponding ToT controller, agent, and memory, the paper is only evaluated on one Sudoku task, especially when the details of evaluation (e.g., number of games evaluated, and computational cost and prompts used compared to the baselines) are not specified. This makes the evaluation results less convincing."", ""Target claims"": [1, 2, 3, 4, 15], ""Reasoning"": ""This span questions the validity of the claims regarding the effectiveness of the ToT framework due to the limited scope of the evaluation, which is only conducted on one Sudoku task, without detailed information on evaluation setup. This is directly related to Claim1, Claim2, Claim3, Claim4, and Claim15, which assert the framework's effectiveness on Sudoku solving.""}, {""Weakness span"": ""Moreover, despite that the method sounds generalizable, there is no strong evidence on how each module in the framework should be actually implemented to be effective and demonstrated (apart from simply mentioning some future work in the end)."", ""Target claims"": [5, 6, 10, 11], ""Reasoning"": ""This span criticizes the lack of strong evidence on the implementation of the framework's modules, which relates directly to claims discussing these modules' roles and functionalities: Claim5 (commonalities with other approaches), Claim6 (uniqueness of ToT controller), Claim10 (success attributed to the checker), and Claim11 (enhanced reasoning capability through backtracking).""}, {""Weakness span"": ""It is not clear how each module in the ToT framework should work in details. For example, the memory module seems to be compelling where the LLM can retrieve previous configuration when backtracking, there is no explicit demonstration of how the memory is maintained, and how backtracking would work."", ""Target claims"": [7, 9, 10], ""Reasoning"": ""This span highlights the lack of clarity in how key components like the memory module and backtracking functionality are implemented, which directly targets claims: Claim7 (recovery and enlarged search space via backtracking), Claim9 (LLM short-range reasoning role), and Claim10 (discovery of mistakes through checker).""}]}, ""meta"": {""id"": ""a648X9AoL4"", ""review_idx"": 2, ""title"": ""Title: Large Language Model Guided Tree-of-Thought\nAbstract: In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implement a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving.\n\nTLDR:This paper introduces the concept of Tree-of-Thought (ToT) which enhances the problem solving capability of LLMs by allowing LLM to solve a given problem through trial and error via a multi-round conversation with a prompter agent.\n\nKeywords:LLM, tree of thought, problem solving"", ""claims"": [""Claim1: Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving."", ""Claim2: Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving."", ""Claim3: To evaluate the effectiveness of the ToT framework, we implemented a ToT-based Sudoku puzzle solver and evaluate it on a suite of Sudoku puzzle benchmarks we created."", ""Claim4: As shown by the experimental results in Section 4.2, the ToT framework can significantly increase the success rate of Sudoku puzzle solving."", ""Claim5: Our proposal shares some commonalities with these approaches, for example, the use of a memory module and additional agents for automatic prompt generation."", ""Claim6: However, our approach is unique in that it introduces a ToT controller which can explicitly conduct backtracking when necessary."", ""Claim7: This not only allows the system to recover from mistakes, but potentially can also enlarge the solution search space."", ""Claim8: The ToT strategy can be viewed as a tree-search algorithm using an LLM as a heuristic for generating the search steps."", ""Claim9: In this setting, the LLM is only used for the \u201cshort-range reasoning\u201d tasks, i.e deriving the next intermediate solution, which is a type of tasks that have been shown to have a high success rate for LLMs."", ""Claim10: On the other hand, by introducing the checker, the system have a higher likelihood to discover the mistakes it makes as it generates the solutions."", ""Claim11: Moreover, by allowing the system to backtrack from a valid but somewhat \u201chopeless\u201d intermediate solution, the system is able to explore a larger solution space, which enhances the \u201clong-range reasoning\u201d capability of the system as a whole."", ""Claim12: The ToT framework thus combines the best of both world."", ""Claim13: Furthermore, this multi-round conversation technique increases the number of computation steps the system can perform."", ""Claim14: Thus, based on the time hierarchy theorem in computational complexity theory, the ToT framework can expand the range of problems that can potentially be solved compared to relying solely on a single round of conversation with an LLM."", ""Claim15: Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving."", ""Claim16: In comparison, the ToT-based solver demonstrates superior performance when compared to the other solvers."", ""Claim17: For the 3x3 benchmark set, it was able to solve all the puzzles."", ""Claim18: The success rate improves by 11% compared to the second best for the two benchmark sets."", ""Claim19: Despite this, the success rate of the ToT based solver is still 80% higher compared to that of the one-shot and few-shot based solvers."", ""Claim20: Nonetheless, the success rate is 60% higher compared to that of the one-shot and few-shot based solvers."", ""Claim21: In this paper, we presented the Tree-of-Thought framework, which enhances LLMs with additional agents and memory modules, resulting in improved performance for mathematical problem-solving tasks."", ""Claim22: To evaluate the performance of this technique, we implemented a Sudoku puzzle solver based on the ToT framework.""], ""review"": ""Review: Weakness: 1. Although the high-level idea of tree-of-thought is promising, with corresponding ToT controller, agent, and memory, the paper is only evaluated on one Sudoku task, especially when the details of evaluation (e.g., number of games evaluated, and computational cost and prompts used compared to the baselines) are not specified. This makes the evaluation results less convincing. Moreover, despite that the method sounds generalizable, there is no strong evidence on how each module in the framework should be actually implemented to be effective and demonstrated (apart from simply mentioning some future work in the end). \n2. It is not clear how each module in the ToT framework should work in details. For example, the memory module seems to be compelling where the LLM can retrieve previous configuration when backtracking, there is no explicit demonstration of how the memory is maintained, and how backtracking would work. Furthermore, the ToT controller is rule-based in the experiment, but Section 3.2 and 3.3 mostly explains how the ToT controller should be trained similar to a policy network. This makes it very confusing to judge the proposed method. I would suggest the authors to add more detailed illustrations using specific examples in the paper revision.\nQuestions: 1. Can you provide more evaluation details for the Sudoku setup? For example, how many games are used for evaluation, and what prompts are employed to the LLM (especially when comparing to the baselines).\n2. Why do you use a rule-based controller for backtracking? How do you derive the rules? \n3. Can you provide more details in terms of how backtracking interact with the memory?\n\n""}, ""review"": ""Title: Large Language Model Guided Tree-of-Thought\nAbstract: In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implement a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving.\n\nClaim1: Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving.\nClaim2: Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving.\nClaim3: To evaluate the effectiveness of the ToT framework, we implemented a ToT-based Sudoku puzzle solver and evaluate it on a suite of Sudoku puzzle benchmarks we created.\nClaim4: As shown by the experimental results in Section 4.2, the ToT framework can significantly increase the success rate of Sudoku puzzle solving.\nClaim5: Our proposal shares some commonalities with these approaches, for example, the use of a memory module and additional agents for automatic prompt generation.\nClaim6: However, our approach is unique in that it introduces a ToT controller which can explicitly conduct backtracking when necessary.\nClaim7: This not only allows the system to recover from mistakes, but potentially can also enlarge the solution search space.\nClaim8: The ToT strategy can be viewed as a tree-search algorithm using an LLM as a heuristic for generating the search steps.\nClaim9: In this setting, the LLM is only used for the \u201cshort-range reasoning\u201d tasks, i.e deriving the next intermediate solution, which is a type of tasks that have been shown to have a high success rate for LLMs.\nClaim10: On the other hand, by introducing the checker, the system have a higher likelihood to discover the mistakes it makes as it generates the solutions.\nClaim11: Moreover, by allowing the system to backtrack from a valid but somewhat \u201chopeless\u201d intermediate solution, the system is able to explore a larger solution space, which enhances the \u201clong-range reasoning\u201d capability of the system as a whole.\nClaim12: The ToT framework thus combines the best of both world.\nClaim13: Furthermore, this multi-round conversation technique increases the number of computation steps the system can perform.\nClaim14: Thus, based on the time hierarchy theorem in computational complexity theory, the ToT framework can expand the range of problems that can potentially be solved compared to relying solely on a single round of conversation with an LLM.\nClaim15: Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving.\nClaim16: In comparison, the ToT-based solver demonstrates superior performance when compared to the other solvers.\nClaim17: For the 3x3 benchmark set, it was able to solve all the puzzles.\nClaim18: The success rate improves by 11% compared to the second best for the two benchmark sets.\nClaim19: Despite this, the success rate of the ToT based solver is still 80% higher compared to that of the one-shot and few-shot based solvers.\nClaim20: Nonetheless, the success rate is 60% higher compared to that of the one-shot and few-shot based solvers.\nClaim21: In this paper, we presented the Tree-of-Thought framework, which enhances LLMs with additional agents and memory modules, resulting in improved performance for mathematical problem-solving tasks.\nClaim22: To evaluate the performance of this technique, we implemented a Sudoku puzzle solver based on the ToT framework.\n\nReview: Weakness: 1. Although the high-level idea of tree-of-thought is promising, with corresponding ToT controller, agent, and memory, the paper is only evaluated on one Sudoku task, especially when the details of evaluation (e.g., number of games evaluated, and computational cost and prompts used compared to the baselines) are not specified. This makes the evaluation results less convincing. Moreover, despite that the method sounds generalizable, there is no strong evidence on how each module in the framework should be actually implemented to be effective and demonstrated (apart from simply mentioning some future work in the end). \n2. It is not clear how each module in the ToT framework should work in details. For example, the memory module seems to be compelling where the LLM can retrieve previous configuration when backtracking, there is no explicit demonstration of how the memory is maintained, and how backtracking would work. Furthermore, the ToT controller is rule-based in the experiment, but Section 3.2 and 3.3 mostly explains how the ToT controller should be trained similar to a policy network. This makes it very confusing to judge the proposed method. I would suggest the authors to add more detailed illustrations using specific examples in the paper revision.\nQuestions: 1. Can you provide more evaluation details for the Sudoku setup? For example, how many games are used for evaluation, and what prompts are employed to the LLM (especially when comparing to the baselines).\n2. Why do you use a rule-based controller for backtracking? How do you derive the rules? \n3. Can you provide more details in terms of how backtracking interact with the memory?\n\n\n\n"", ""id"": ""a648X9AoL42"", ""pdf"": ""openreview.net/pdf?id=a648X9AoL4""}","15"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""- **The efficiency of the proposed method is stated but not experimentally proven.** In section 3.3, the authors claim that training the graph adapters can be achieved with very few computing resources. However, to support this claim, it is necessary to provide a comparison with the compared methods (w.r.t. training parameters, memory usage, training speed, and training time, etc). This empirical evidence would provide stronger support for the efficiency of the proposed method. Furthermore, it is worth noting that the training epoch for the graph adapter is 50 for ogbn-arxiv, while typically fine-tuning it for 3-5 epochs is sufficient[1]. Therefore, it raises concerns whether the additional training epoch may negate the benefits introduced by training only the graph adapter."", ""Target claims"": [], ""Reasoning"": ""This feedback questions the efficiency claim made by the authors regarding their method, particularly focusing on the resource requirements for training the graph adapters. While there is a claim in the method about being able to train with few resources, this is a statement about practical applicability rather than a core claim related to experimental results or comparisons with specific baselines stated in the claims. Therefore, no claims are directly affected here as the claims focus on performance improvements and interpretability which are not addressed in the weakness.""}]}, ""meta"": {""id"": ""ZHMYXfk4b1"", ""review_idx"": 0, ""title"": ""Title: Prompt-based Node Feature Extractor for Few-shot Learning on Text-Attributed Graph\nAbstract: Text-attributed Graphs (TAGs) are commonly found in the real world, such as social networks and citation networks, which consist of nodes represented by textual descriptions. \nCurrently, mainstream machine learning methods on TAGs involve a two-stage modeling approach: (1) unsupervised node feature extraction with pre-trained language models (PLMs); and (2) supervised learning using Graph Neural Networks (GNNs). \nHowever, we observe that these representations, which have undergone large-scale pre-training, do not significantly improve performance with limited amount of training samples. \nThe main issue is that existing methods have not effectively integrated information from the graph and downstream tasks simultaneously. \nFirst, G-Prompt introduces a learnable GNN layer (i.e., adaptor) at the end of PLMs, which is fine-tuned to better capture the masked tokens considering graph neighborhood information.\nAfter the adapter is trained, G-Prompt incorporates task-specific prompts to obtain interpretable node representations for the downstream task.\nOur experiment results demonstrate that our proposed method outperforms current state-of-the-art (SOTA) methods on few-shot node classification. More importantly, in zero-shot settings, the G-Prompt embeddings can not only provide better task interpretability than vanilla PLMs\nbut also achieve comparable performance with fully-supervised baselines.\n\nTLDR:\n\nKeywords:Text-attributed graph; graph neural network; language model"", ""claims"": [""Claim1: Our experiment results demonstrate that our proposed method outperforms current state-of-the-art (SOTA) methods on few-shot node classification."", ""Claim2: More importantly, in zero-shot settings, the G-Prompt embeddings can not only provide better task interpretability than vanilla PLMs but also achieve comparable performance with fully-supervised baselines."", ""Claim3: The results of our experiments show that G-Prompt achieves state-of-the-art performance in few-shot learning, with an average improvement of avg. 4.1% compared to the best baseline."", ""Claim4: Besides, our G-Prompt embeddings are also highly robust in zero-shot settings, outperforming PLMs by avg. 2.7%."", ""Claim5: Furthermore, we conduct an analysis of the representations generated by G-Prompt and found that they have high interpretability with respect to task performance."", ""Claim6: Therefore, this paper proposes a novel self-supervised method, G-Prompt, capable of extracting task-specific and graph-aware node representations."", ""Claim7: While previous works have frequently employed PLMs to process TAGs, these investigations have been constrained in extracting a broad node representation from the text-based characteristics and have not incorporated task-specific prior knowledge."", ""Claim8: Consequently, additional learning supervision via GNNs is needed to enable the effective adaptation of these node representations to downstream tasks."", ""Claim9: Therefore, the proposed G-Prompt leverages a self-supervised based graph adapter and prompts to make PLMs aware of the graph information and downstream task."", ""Claim10: Additionally, the graph adapter is only trained on a given TAG, generalizing to prompt tokens can also be quite difficult."", ""Claim11: Our experiment results demonstrate that our proposed method outperforms current state-of-the-art (SOTA) methods on few-shot node classification."", ""Claim12: More importantly, in zero-shot settings, the G-Prompt embeddings can not only provide better task interpretability than vanilla PLMs but also achieve comparable performance with fully-supervised baselines."", ""Claim13: The experiment shows that: (1) Graph-aware can improve the performance of node representation."", ""Claim14: In general, approaches that use sentence representations or those that involve self-supervised training with graph information tend to outperform non-trained representations."", ""Claim15: For example, GAE shows an average improvement of avg. 6.2% compared to RoBERTa\u2019s [cls], and GIANT shows avg. 6.2% improvement over cls representation."", ""Claim16: For graph-based self-supervised tasks, fine-tuning language models might be more suitable for larger datasets."", ""Claim17: GIANT outperforms GAE by avg. 3.0% on Arxiv, but lags behind by avg. 1.4% on Instagram and Reddit."", ""Claim18: (2) Downstream task-related prompts can improve performance for all models."", ""Claim19: For graph-free language models, prompt-based representations can improve performance by avg. 5.7%, and the overall performance of prediction values and hidden states corresponding to prompts is similar."", ""Claim20: For graph-based methods, prompts in GAE improve performance by avg. 1.3%, while prompts in GIANT lead to an average improvement of avg. 1.2%."", ""Claim21: However, we note that prompts are unstable for graph-based pre-trained models. GAE shows a decline in 4 experiments, while prompts only bring a slight improvement in GIANT (compared to language models)."", ""Claim22: (3) Our method is capable of utilizing both graph perception and downstream task prompts simultaneously, achieving state-of-the-art performance."", ""Claim23: Compared to PLM representations without prompts, our method improves by avg. 10.6%."", ""Claim24: Compared to PLM-prompt, it improves by avg. 4.6%, and compared to GIANT, it improves by avg. 4.1%."", ""Claim25: Besides, as Figure 2 shows, the node representation extracted by G-Prompt in different GNN backbone also achieves the SOTA performance compared to other baseline methods."", ""Claim26: The experimental results show that all variants perform worse than G-Prompt."", ""Claim27: Specifically, removing the Graph-Adapter training process leads to avg. 2.8% decrease in performance, which demonstrates the effectiveness of training graph-adapter through the fillmask task."", ""Claim28: After removing the graph input, the performance of G-Prompt decreases by avg. 3.8%, which further confirms that the improvement provided by G-Prompt, compared to using language model prompts directly, stems from the graph adapter\u2019s ability to assist language models in comprehending graph structures."", ""Claim29: Moreover, removing the gate mechanism results in a avg. 1.8% decrease in performance, indicating that the design of the graph-adapter structure is reasonable."", ""Claim30: Our experiment results demonstrate that our proposed method outperforms current state-of-the-art (SOTA) methods on few-shot node classification."", ""Claim31: More importantly, in zero-shot settings, the G-Prompt embeddings can not only provide better task interpretability than vanilla PLMs but also achieve comparable performance with fully-supervised baselines."", ""Claim32: According to the results shown in Table 3. (1) The bag-of-words method has almost no predictive ability."", ""Claim33: The PLM through Prompts has predictive ability on different tasks (improvement compared to BOW by avg. 13%)."", ""Claim34: But there is a performance difference between base and large even with the same prompt due to the sensitivity of language models to prompts [22]."", ""Claim35: Compared to a language model, G-Prompt shows significant performance improvement."", ""Claim36: Specifically, G-Prompt-base improved avg. 2.7% compared to the language model."", ""Claim37: Moreover, selecting more candidate words through prior knowledge can effectively help G-Prompt improve its zero-shot capability, with an average improvement of avg. 4.8% for the base and avg. 5.3% for the large."", ""Claim38: However, there is no significant improvement for language models and bag-of-words."", ""Claim39: Surprisingly, by adding a small number of candidate words, G-Prompt\u2019s zero-shot performance is already close to or even sometimes surpasses supervised training with 100 shots."", ""Claim40: This result indicates that combining language models and graphs for zero-shot learning on TAG is feasible."", ""Claim41: Overall, the top 7 tokens given by our model have considerably higher ROC scores than RoBERTa-Large resulting in avg. 7.0% improvement."", ""Claim42: Additionally, our results are intuitive and can even help explain the task, for example, \u201cpremium.\u201d"", ""Claim43: Experiments with different shot settings using three datasets demonstrate that the proposed model can effectively capture both text and graph information, resulting in improved performance for few-shot learning."", ""Claim44: In zero-shot learning, our model achieves comparable performance with supervised baselines and has huge potential for future work."", ""Claim45: Furthermore, our model provides useful interpretations, which is essential for understanding the tasks and TAGs.""], ""review"": ""Review: Weakness: - **Some tables and figures are not self-contained, such as Figure 1 and Table 2.** The caption is too concise for the readers to follow. For example, in figure 1, the authors are expected to summarize the proposed framework in one or two sentences to aid comprehension; For table 2, the meaning of the bold and underline should be explicitly specified to ensure clarity and understanding.\n- **Background, method, and implementation of few-shot learning is missing.** As few-shot learning is less common in the graph community compared to supervised learning, it is essential to provide additional explanations and context for unfamiliar readers. \nMoreover, when comparing to some existing approaches, such as GIANT, which were originally designed for supervised learning, it should explicitly discuss how these methods can be adapted or extended for few-shot learning scenarios. Also, some compared methods, such as GIANT, were intended for supervised learning. How to adopt them for the few-shot learning is not straight-forward and should be introduced.\n- **The efficiency of the proposed method is stated but not experimentally proven.**\nIn section 3.3, the authors claim that training the graph adapters can be achieved with very few computing resources. However, to support this claim, it is necessary to provide a comparison with the compared methods (w.r.t. training parameters, memory usage, training speed, and training time, etc). This empirical evidence would provide stronger support for the efficiency of the proposed method. Furthermore, it is worth noting that the training epoch for the graph adapter is 50 for ogbn-arxiv, while typically fine-tuning it for 3-5 epochs is sufficient[1]. Therefore, it raises concerns whether the additional training epoch may negate the benefits introduced by training only the graph adapter.\n\n\n\n**Reference:**\n[1] Learning on Large-scale Text-attributed Graphs via Variational Inference, ICLR 2023.\nQuestions: - According to section 3.4, the node features are fed into subsequent GNNs but in Figure 1, the GNN encoder is not part of the framework, which is a little bit confusing.\n- The dimention of the node features need to reduced for practical usage (section 3.4). And it is heuristic to select the optimal M. I am wondering how sensitive the graph prompter is to the value of M, and if there is any discussion on the selection of the optimal M.\n\n\n""}, ""review"": ""Title: Prompt-based Node Feature Extractor for Few-shot Learning on Text-Attributed Graph\nAbstract: Text-attributed Graphs (TAGs) are commonly found in the real world, such as social networks and citation networks, which consist of nodes represented by textual descriptions. \nCurrently, mainstream machine learning methods on TAGs involve a two-stage modeling approach: (1) unsupervised node feature extraction with pre-trained language models (PLMs); and (2) supervised learning using Graph Neural Networks (GNNs). \nHowever, we observe that these representations, which have undergone large-scale pre-training, do not significantly improve performance with limited amount of training samples. \nThe main issue is that existing methods have not effectively integrated information from the graph and downstream tasks simultaneously. \nFirst, G-Prompt introduces a learnable GNN layer (i.e., adaptor) at the end of PLMs, which is fine-tuned to better capture the masked tokens considering graph neighborhood information.\nAfter the adapter is trained, G-Prompt incorporates task-specific prompts to obtain interpretable node representations for the downstream task.\nOur experiment results demonstrate that our proposed method outperforms current state-of-the-art (SOTA) methods on few-shot node classification. More importantly, in zero-shot settings, the G-Prompt embeddings can not only provide better task interpretability than vanilla PLMs\nbut also achieve comparable performance with fully-supervised baselines.\n\nClaim1: Our experiment results demonstrate that our proposed method outperforms current state-of-the-art (SOTA) methods on few-shot node classification.\nClaim2: More importantly, in zero-shot settings, the G-Prompt embeddings can not only provide better task interpretability than vanilla PLMs but also achieve comparable performance with fully-supervised baselines.\nClaim3: The results of our experiments show that G-Prompt achieves state-of-the-art performance in few-shot learning, with an average improvement of avg. 4.1% compared to the best baseline.\nClaim4: Besides, our G-Prompt embeddings are also highly robust in zero-shot settings, outperforming PLMs by avg. 2.7%.\nClaim5: Furthermore, we conduct an analysis of the representations generated by G-Prompt and found that they have high interpretability with respect to task performance.\nClaim6: Therefore, this paper proposes a novel self-supervised method, G-Prompt, capable of extracting task-specific and graph-aware node representations.\nClaim7: While previous works have frequently employed PLMs to process TAGs, these investigations have been constrained in extracting a broad node representation from the text-based characteristics and have not incorporated task-specific prior knowledge.\nClaim8: Consequently, additional learning supervision via GNNs is needed to enable the effective adaptation of these node representations to downstream tasks.\nClaim9: Therefore, the proposed G-Prompt leverages a self-supervised based graph adapter and prompts to make PLMs aware of the graph information and downstream task.\nClaim10: Additionally, the graph adapter is only trained on a given TAG, generalizing to prompt tokens can also be quite difficult.\nClaim11: Our experiment results demonstrate that our proposed method outperforms current state-of-the-art (SOTA) methods on few-shot node classification.\nClaim12: More importantly, in zero-shot settings, the G-Prompt embeddings can not only provide better task interpretability than vanilla PLMs but also achieve comparable performance with fully-supervised baselines.\nClaim13: The experiment shows that: (1) Graph-aware can improve the performance of node representation.\nClaim14: In general, approaches that use sentence representations or those that involve self-supervised training with graph information tend to outperform non-trained representations.\nClaim15: For example, GAE shows an average improvement of avg. 6.2% compared to RoBERTa\u2019s [cls], and GIANT shows avg. 6.2% improvement over cls representation.\nClaim16: For graph-based self-supervised tasks, fine-tuning language models might be more suitable for larger datasets.\nClaim17: GIANT outperforms GAE by avg. 3.0% on Arxiv, but lags behind by avg. 1.4% on Instagram and Reddit.\nClaim18: (2) Downstream task-related prompts can improve performance for all models.\nClaim19: For graph-free language models, prompt-based representations can improve performance by avg. 5.7%, and the overall performance of prediction values and hidden states corresponding to prompts is similar.\nClaim20: For graph-based methods, prompts in GAE improve performance by avg. 1.3%, while prompts in GIANT lead to an average improvement of avg. 1.2%.\nClaim21: However, we note that prompts are unstable for graph-based pre-trained models. GAE shows a decline in 4 experiments, while prompts only bring a slight improvement in GIANT (compared to language models).\nClaim22: (3) Our method is capable of utilizing both graph perception and downstream task prompts simultaneously, achieving state-of-the-art performance.\nClaim23: Compared to PLM representations without prompts, our method improves by avg. 10.6%.\nClaim24: Compared to PLM-prompt, it improves by avg. 4.6%, and compared to GIANT, it improves by avg. 4.1%.\nClaim25: Besides, as Figure 2 shows, the node representation extracted by G-Prompt in different GNN backbone also achieves the SOTA performance compared to other baseline methods.\nClaim26: The experimental results show that all variants perform worse than G-Prompt.\nClaim27: Specifically, removing the Graph-Adapter training process leads to avg. 2.8% decrease in performance, which demonstrates the effectiveness of training graph-adapter through the fillmask task.\nClaim28: After removing the graph input, the performance of G-Prompt decreases by avg. 3.8%, which further confirms that the improvement provided by G-Prompt, compared to using language model prompts directly, stems from the graph adapter\u2019s ability to assist language models in comprehending graph structures.\nClaim29: Moreover, removing the gate mechanism results in a avg. 1.8% decrease in performance, indicating that the design of the graph-adapter structure is reasonable.\nClaim30: Our experiment results demonstrate that our proposed method outperforms current state-of-the-art (SOTA) methods on few-shot node classification.\nClaim31: More importantly, in zero-shot settings, the G-Prompt embeddings can not only provide better task interpretability than vanilla PLMs but also achieve comparable performance with fully-supervised baselines.\nClaim32: According to the results shown in Table 3. (1) The bag-of-words method has almost no predictive ability.\nClaim33: The PLM through Prompts has predictive ability on different tasks (improvement compared to BOW by avg. 13%).\nClaim34: But there is a performance difference between base and large even with the same prompt due to the sensitivity of language models to prompts [22].\nClaim35: Compared to a language model, G-Prompt shows significant performance improvement.\nClaim36: Specifically, G-Prompt-base improved avg. 2.7% compared to the language model.\nClaim37: Moreover, selecting more candidate words through prior knowledge can effectively help G-Prompt improve its zero-shot capability, with an average improvement of avg. 4.8% for the base and avg. 5.3% for the large.\nClaim38: However, there is no significant improvement for language models and bag-of-words.\nClaim39: Surprisingly, by adding a small number of candidate words, G-Prompt\u2019s zero-shot performance is already close to or even sometimes surpasses supervised training with 100 shots.\nClaim40: This result indicates that combining language models and graphs for zero-shot learning on TAG is feasible.\nClaim41: Overall, the top 7 tokens given by our model have considerably higher ROC scores than RoBERTa-Large resulting in avg. 7.0% improvement.\nClaim42: Additionally, our results are intuitive and can even help explain the task, for example, \u201cpremium.\u201d\nClaim43: Experiments with different shot settings using three datasets demonstrate that the proposed model can effectively capture both text and graph information, resulting in improved performance for few-shot learning.\nClaim44: In zero-shot learning, our model achieves comparable performance with supervised baselines and has huge potential for future work.\nClaim45: Furthermore, our model provides useful interpretations, which is essential for understanding the tasks and TAGs.\n\nReview: Weakness: - **Some tables and figures are not self-contained, such as Figure 1 and Table 2.** The caption is too concise for the readers to follow. For example, in figure 1, the authors are expected to summarize the proposed framework in one or two sentences to aid comprehension; For table 2, the meaning of the bold and underline should be explicitly specified to ensure clarity and understanding.\n- **Background, method, and implementation of few-shot learning is missing.** As few-shot learning is less common in the graph community compared to supervised learning, it is essential to provide additional explanations and context for unfamiliar readers. \nMoreover, when comparing to some existing approaches, such as GIANT, which were originally designed for supervised learning, it should explicitly discuss how these methods can be adapted or extended for few-shot learning scenarios. Also, some compared methods, such as GIANT, were intended for supervised learning. How to adopt them for the few-shot learning is not straight-forward and should be introduced.\n- **The efficiency of the proposed method is stated but not experimentally proven.**\nIn section 3.3, the authors claim that training the graph adapters can be achieved with very few computing resources. However, to support this claim, it is necessary to provide a comparison with the compared methods (w.r.t. training parameters, memory usage, training speed, and training time, etc). This empirical evidence would provide stronger support for the efficiency of the proposed method. Furthermore, it is worth noting that the training epoch for the graph adapter is 50 for ogbn-arxiv, while typically fine-tuning it for 3-5 epochs is sufficient[1]. Therefore, it raises concerns whether the additional training epoch may negate the benefits introduced by training only the graph adapter.\n\n\n\n**Reference:**\n[1] Learning on Large-scale Text-attributed Graphs via Variational Inference, ICLR 2023.\nQuestions: - According to section 3.4, the node features are fed into subsequent GNNs but in Figure 1, the GNN encoder is not part of the framework, which is a little bit confusing.\n- The dimention of the node features need to reduced for practical usage (section 3.4). And it is heuristic to select the optimal M. I am wondering how sensitive the graph prompter is to the value of M, and if there is any discussion on the selection of the optimal M.\n\n\n\n\n"", ""id"": ""ZHMYXfk4b10"", ""pdf"": ""openreview.net/pdf?id=ZHMYXfk4b1""}","16"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""I am especially concerned about the claims of improved energy efficiency, which serve as the main motivation of the paper. Starting from the introduction, where the author claim: \""However, it requires too much computational power and energy to train and deploy state-of-the-art ANN models, leading to a consistent increase of energy consumption per model over the past decade. The energy consumption of large language models, such as ChatGPT[OpenAI, 2022] and GPT-4[OpenAI, 2023], is unfathomable even during inference.\"" It is clearly not true that \""it requires too much computational power and energy to train and deploy state-of-the-art ANN models\"" since these models are in fact trained and deployed."", ""Target claims"": [1, 22, 23, 24], ""Reasoning"": ""The reviewer questions the veracity of claims regarding improved energy efficiency, casting doubt on claims about comparable or reduced energy consumption made throughout the paper, including Claim1, which mentions lower energy consumption compared to BERTs, and details in Claims 22, 23, and 24 about the energy efficiency of SpikeBERT. By arguing that existing ANN models are trained and deployed contrary to what the authors suggest, the reviewer challenges the implications of these claims about energy use.""}, {""Weakness span"": ""More concerning is the theoretical energy comparison of SpikeBERT and BERT (Section 4.4 and Appendix C), where the authors compare FLOPs for BERT and SOPs (spiking operations) for SpikeBERT, multiply by theoretical energy costs and declare SpikeBERT the winner. The theoretical energy costs seem to be copied from other papers, and following the citation chain they seem to come from Yao et al. 2022 \""Attention Spiking Neural Networks\"" where they are computed using data from Horowitz 2014 \""1.1 computing\u2019s energy problem (and what we can do about it)\"", with the assumption of 32-bit floating point operations on 45nm hardware. Modern GPUs use 7nm hardware, and inference is often done with 8-bit floating point operation or less, therefore I wonder whether these number are obsolete."", ""Target claims"": [1, 22, 23, 24], ""Reasoning"": ""The reviewer criticizes the methodology used for energy comparison between SpikeBERT and BERT as potentially outdated or incorrect, which puts claims regarding SpikeBERT's lower energy consumption in question. This directly targets Claim1 as it questions the claim about energy efficiency compared to BERTs, and substantiates further doubts about energy-related claims like those in Claims 22, 23, and 24, where specific metrics indicating SpikeBERT's energy advantages are mentioned.""}]}, ""meta"": {""id"": ""URI2aAQiQC"", ""review_idx"": 0, ""title"": ""Title: SpikeBERT: A Language Spikformer Trained with Two-Stage Knowledge Distillation from BERT\nAbstract: Spiking neural networks (SNNs) offer a promising avenue to implement deep neural networks in a more energy-efficient way.\nHowever, the network architectures of existing SNNs for language tasks are too simplistic, and deep architectures have not been fully explored, resulting in a significant performance gap compared to mainstream transformer-based networks such as BERT.\nTo this end, we improve a recently-proposed spiking transformer (i.e., Spikformer) to make it possible to process language tasks and propose a two-stage knowledge distillation method for training it, which combines pre-training by distilling knowledge from BERT with a large collection of unlabelled texts and fine-tuning with task-specific instances via knowledge distillation again from the BERT fine-tuned on the same training examples.\nThrough extensive experimentation, we show that the models trained with our method, named SpikeBERT, outperform state-of-the-art SNNs and even achieve comparable results to BERTs on text classification tasks for both English and Chinese with much less energy consumption.\n\nTLDR:We propose SpikeBERT and a two-stage knowledge distillation method for training it for text classification task.\n\nKeywords:Spiking neural networks, Text classification, Knowledge distillation"", ""claims"": [""Claim1: Through extensive experimentation, we show that the models trained with our method, named SpikeBERT, outperform state-of-the-art SNNs and even achieve comparable results to BERTs on text classification tasks for both English and Chinese with much less energy consumption."", ""Claim2: Firstly, Spiking Patch Splitting (SPS) module was replaced with a word embedding layer so that the network can take discrete words (or tokens) as input."", ""Claim3: We proposed to use knowledge distillation [Hinton et al., 2015] to train language Spikformers so that the deviation of surrogate gradients in Spikformer would not be rapidly accumulated[Qiu et al., 2023]."", ""Claim4: Inspired by the widely-used \u201cpre-training + fine-tuning\u201d recipe [Sun et al., 2019, Liu, 2019, Gururan50 gan et al., 2020], we present a two-stage knowledge distillation strategy."", ""Claim5: After two-stage knowledge distillation, a spiking language model, named SpikeBERT, can be built by distilling knowledge from BERT."", ""Claim6: The experiment results show that SpikeBERT not only can outperform the state-of-the-art SNNs-like frameworks in text classification task but also achieve competitive performance to BERTs."", ""Claim7: The experiments of the ablation study (Section 4.5) also show that \u201cpre-training distillation\u201d plays an important role in training SpikeBERT."", ""Claim8: The major contribution of this study can be summarized as follows: \u2022 This study is among the first to show the feasibility of transferring the knowledge of BERT70 like large language models to spiking-based architectures that can achieve comparable 71 results but with much less energy consumption."", ""Claim9: In this study, we propose a two-stage knowledge distillation approach to train the proposed SpikeBERT for text classification tasks, which is among the first ones to show the feasibility of transferring the knowledge to SNNs from large language models."", ""Claim10: Two-stage distillation is the key to enabling the student model with language processing ability."", ""Claim11: The first stage is to align the embeddings and hidden features between BERT and the improved Spikformer using a large-scale corpus."", ""Claim12: The second stage is to distill logits and cross-entropy information on a task-specific dataset from a fine-tuned BERT to the model finishing stage 1."", ""Claim13: We conduct four sets of experiments. The first is to evaluate the accuracy of SpikeBERT trained with the proposed method on 6 datasets of text classification datasets."", ""Claim14: The second experiment is to compare the theoretical energy consumption of BERT and that of SpikeBERT."", ""Claim15: The third experiment is an ablation study about the training process."", ""Claim16: We report in Table 1 the accuracy achieved by SpikeBERT trained with \u201cpre-training + task-specific\u201d distillation on 6 datasets, compared to 2 baselines: 1) SNN-TextCNN proposed by Lv et al. [2023]; 2) improved Spikformer directly trained with gradient descent algorithm using surrogate gradients."", ""Claim17: Table 1 demonstrates that the SpikeBERT trained with two-stage distillation achieves state-out-of-art performance across 6 text classification datasets."", ""Claim18: Compared to SNN-TextCNN, SpikeBERT achieved up to 5 . 42% improvement in accuracy ( 3 . 49% increase on average) for all text classification benchmarks."", ""Claim19: Furthermore, SpikeBERT outperforms TextCNN, which is considered a representative artificial neural network, and even achieves comparable results to the fine-tuned BERT by a small drop of 4 . 13% on average in accuracy for text classification task."", ""Claim20: Table 1 demonstrates that SpikeBERT can also be applied well in Chinese datasets (ChnSenti and Waimai)."", ""Claim21: Table 1 reveals that the accuracy of directly-trained Spikformer is noticeably lower than SpikeBERT on some benchmarks, such as MR, SST5 , and ChnSenti."", ""Claim22: It is worth noting that the energy consumption of SpikeBERT is significantly lower than that of fine-tuned BERT, which is an important advantage of SNNs over ANNs in terms of energy efficiency."", ""Claim23: As shown in Table 2, SpikeBERT demands only 25 . 00% of the energy that fine-tuned BERT needs to achieve comparable performance on average."", ""Claim24: Moreover, on the Subj dataset, SpikeBERT can reduce energy consumption by up to 77 . 17% compared to fine-tuned BERT for predicting each text example."", ""Claim25: This indicates that SpikeBERT is a promising candidate for energy-efficient text classification in resource-constrained scenarios."", ""Claim26: As we can see in Table 4.5, SpikeBERTs without either stage 1 or stage 2 experience about 3 . 20% performance drop on average."", ""Claim27: Therefore, we conclude that the two distillation stages are both essential for training SpikeBERT."", ""Claim28: Furthermore, we observed that the average performance dropped from 76 . 30 to 73 . 27 when excluding the logits loss, demonstrating that the logits loss L logits has the greatest impact on task-specific distillation."", ""Claim29: Meanwhile, data augmentation (DA) plays an important role in Stage 2 , contributing to an increase in average performance from 75 . 54 to 76 . 30 ."", ""Claim30: We find that, with the increase of time steps, the accuracy increases first, then remains unchanged, and reaches its maximum roughly at T = 4 ."", ""Claim31: In this study, we extended and improved Spikformer to process language tasks and proposed a new promising training paradigm for training SpikeBERT inspired by the notion of knowledge distillation."", ""Claim32: We presented a two-stage, \u201cpre-training + task-specific\u201d knowledge distillation method by transferring the knowledge from BERTs to SpikeBERT for text classification tasks."", ""Claim33: We empirically show that our SpikeBERT outperforms the state-of-the-art SNNs and can even achieve comparable results to BERTs with much less energy consumption across multiple datasets for both English and Chinese, leading to future energy-efficient implementations of BERTs or large language models.""], ""review"": ""Review: Weakness: I am especially concerned about the claims of improved energy efficiency, which serve as the main motivation of the paper.\nStarting from the introduction, where the author claim: \""However, it requires too much computational power and energy to train and deploy state-of-the-art ANN models, leading to a consistent increase of energy consumption per model over the past decade. The energy consumption of large language models, such as ChatGPT[OpenAI, 2022] and GPT-4[OpenAI, 2023], is unfathomable even during inference.\""\nIt is clearly not true that \""it requires too much computational power and energy to train and deploy state-of-the-art ANN models\"" since these models are in fact trained and deployed.\n\nMore concerning is the theoretical energy comparison of SpikeBERT and BERT (Section 4.4 and Appendix C), where the authors compare FLOPs for BERT and SOPs (spiking operations) for SpikeBERT, multiply by theoretical energy costs and declare SpikeBERT the winner. The theoretical energy costs seem to be copied from other papers, and following the citation chain they seem to come from Yao et al. 2022 \""Attention Spiking Neural Networks\"" where they are computed using data from Horowitz 2014 \""1.1 computing\u2019s energy problem (and what we\ncan do about it)\"", with the assumption of 32-bit floating point operations on 45nm hardware. Modern GPUs use 7nm hardware, and inference is often done with 8-bit floating point operation or less, therefore I wonder whether these number are obsolete.\n\nQuestions: Table 2: mJ is a measure of energy, not power\n\n""}, ""review"": ""Title: SpikeBERT: A Language Spikformer Trained with Two-Stage Knowledge Distillation from BERT\nAbstract: Spiking neural networks (SNNs) offer a promising avenue to implement deep neural networks in a more energy-efficient way.\nHowever, the network architectures of existing SNNs for language tasks are too simplistic, and deep architectures have not been fully explored, resulting in a significant performance gap compared to mainstream transformer-based networks such as BERT.\nTo this end, we improve a recently-proposed spiking transformer (i.e., Spikformer) to make it possible to process language tasks and propose a two-stage knowledge distillation method for training it, which combines pre-training by distilling knowledge from BERT with a large collection of unlabelled texts and fine-tuning with task-specific instances via knowledge distillation again from the BERT fine-tuned on the same training examples.\nThrough extensive experimentation, we show that the models trained with our method, named SpikeBERT, outperform state-of-the-art SNNs and even achieve comparable results to BERTs on text classification tasks for both English and Chinese with much less energy consumption.\n\nClaim1: Through extensive experimentation, we show that the models trained with our method, named SpikeBERT, outperform state-of-the-art SNNs and even achieve comparable results to BERTs on text classification tasks for both English and Chinese with much less energy consumption.\nClaim2: Firstly, Spiking Patch Splitting (SPS) module was replaced with a word embedding layer so that the network can take discrete words (or tokens) as input.\nClaim3: We proposed to use knowledge distillation [Hinton et al., 2015] to train language Spikformers so that the deviation of surrogate gradients in Spikformer would not be rapidly accumulated[Qiu et al., 2023].\nClaim4: Inspired by the widely-used \u201cpre-training + fine-tuning\u201d recipe [Sun et al., 2019, Liu, 2019, Gururan50 gan et al., 2020], we present a two-stage knowledge distillation strategy.\nClaim5: After two-stage knowledge distillation, a spiking language model, named SpikeBERT, can be built by distilling knowledge from BERT.\nClaim6: The experiment results show that SpikeBERT not only can outperform the state-of-the-art SNNs-like frameworks in text classification task but also achieve competitive performance to BERTs.\nClaim7: The experiments of the ablation study (Section 4.5) also show that \u201cpre-training distillation\u201d plays an important role in training SpikeBERT.\nClaim8: The major contribution of this study can be summarized as follows: \u2022 This study is among the first to show the feasibility of transferring the knowledge of BERT70 like large language models to spiking-based architectures that can achieve comparable 71 results but with much less energy consumption.\nClaim9: In this study, we propose a two-stage knowledge distillation approach to train the proposed SpikeBERT for text classification tasks, which is among the first ones to show the feasibility of transferring the knowledge to SNNs from large language models.\nClaim10: Two-stage distillation is the key to enabling the student model with language processing ability.\nClaim11: The first stage is to align the embeddings and hidden features between BERT and the improved Spikformer using a large-scale corpus.\nClaim12: The second stage is to distill logits and cross-entropy information on a task-specific dataset from a fine-tuned BERT to the model finishing stage 1.\nClaim13: We conduct four sets of experiments. The first is to evaluate the accuracy of SpikeBERT trained with the proposed method on 6 datasets of text classification datasets.\nClaim14: The second experiment is to compare the theoretical energy consumption of BERT and that of SpikeBERT.\nClaim15: The third experiment is an ablation study about the training process.\nClaim16: We report in Table 1 the accuracy achieved by SpikeBERT trained with \u201cpre-training + task-specific\u201d distillation on 6 datasets, compared to 2 baselines: 1) SNN-TextCNN proposed by Lv et al. [2023]; 2) improved Spikformer directly trained with gradient descent algorithm using surrogate gradients.\nClaim17: Table 1 demonstrates that the SpikeBERT trained with two-stage distillation achieves state-out-of-art performance across 6 text classification datasets.\nClaim18: Compared to SNN-TextCNN, SpikeBERT achieved up to 5 . 42% improvement in accuracy ( 3 . 49% increase on average) for all text classification benchmarks.\nClaim19: Furthermore, SpikeBERT outperforms TextCNN, which is considered a representative artificial neural network, and even achieves comparable results to the fine-tuned BERT by a small drop of 4 . 13% on average in accuracy for text classification task.\nClaim20: Table 1 demonstrates that SpikeBERT can also be applied well in Chinese datasets (ChnSenti and Waimai).\nClaim21: Table 1 reveals that the accuracy of directly-trained Spikformer is noticeably lower than SpikeBERT on some benchmarks, such as MR, SST5 , and ChnSenti.\nClaim22: It is worth noting that the energy consumption of SpikeBERT is significantly lower than that of fine-tuned BERT, which is an important advantage of SNNs over ANNs in terms of energy efficiency.\nClaim23: As shown in Table 2, SpikeBERT demands only 25 . 00% of the energy that fine-tuned BERT needs to achieve comparable performance on average.\nClaim24: Moreover, on the Subj dataset, SpikeBERT can reduce energy consumption by up to 77 . 17% compared to fine-tuned BERT for predicting each text example.\nClaim25: This indicates that SpikeBERT is a promising candidate for energy-efficient text classification in resource-constrained scenarios.\nClaim26: As we can see in Table 4.5, SpikeBERTs without either stage 1 or stage 2 experience about 3 . 20% performance drop on average.\nClaim27: Therefore, we conclude that the two distillation stages are both essential for training SpikeBERT.\nClaim28: Furthermore, we observed that the average performance dropped from 76 . 30 to 73 . 27 when excluding the logits loss, demonstrating that the logits loss L logits has the greatest impact on task-specific distillation.\nClaim29: Meanwhile, data augmentation (DA) plays an important role in Stage 2 , contributing to an increase in average performance from 75 . 54 to 76 . 30 .\nClaim30: We find that, with the increase of time steps, the accuracy increases first, then remains unchanged, and reaches its maximum roughly at T = 4 .\nClaim31: In this study, we extended and improved Spikformer to process language tasks and proposed a new promising training paradigm for training SpikeBERT inspired by the notion of knowledge distillation.\nClaim32: We presented a two-stage, \u201cpre-training + task-specific\u201d knowledge distillation method by transferring the knowledge from BERTs to SpikeBERT for text classification tasks.\nClaim33: We empirically show that our SpikeBERT outperforms the state-of-the-art SNNs and can even achieve comparable results to BERTs with much less energy consumption across multiple datasets for both English and Chinese, leading to future energy-efficient implementations of BERTs or large language models.\n\nReview: Weakness: I am especially concerned about the claims of improved energy efficiency, which serve as the main motivation of the paper.\nStarting from the introduction, where the author claim: \""However, it requires too much computational power and energy to train and deploy state-of-the-art ANN models, leading to a consistent increase of energy consumption per model over the past decade. The energy consumption of large language models, such as ChatGPT[OpenAI, 2022] and GPT-4[OpenAI, 2023], is unfathomable even during inference.\""\nIt is clearly not true that \""it requires too much computational power and energy to train and deploy state-of-the-art ANN models\"" since these models are in fact trained and deployed.\n\nMore concerning is the theoretical energy comparison of SpikeBERT and BERT (Section 4.4 and Appendix C), where the authors compare FLOPs for BERT and SOPs (spiking operations) for SpikeBERT, multiply by theoretical energy costs and declare SpikeBERT the winner. The theoretical energy costs seem to be copied from other papers, and following the citation chain they seem to come from Yao et al. 2022 \""Attention Spiking Neural Networks\"" where they are computed using data from Horowitz 2014 \""1.1 computing\u2019s energy problem (and what we\ncan do about it)\"", with the assumption of 32-bit floating point operations on 45nm hardware. Modern GPUs use 7nm hardware, and inference is often done with 8-bit floating point operation or less, therefore I wonder whether these number are obsolete.\n\nQuestions: Table 2: mJ is a measure of energy, not power\n\n\n\n"", ""id"": ""URI2aAQiQC0"", ""pdf"": ""openreview.net/pdf?id=URI2aAQiQC""}","17"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""This paper claims \""training acceleration\"" as a key contribution. But throughout the paper, the comparison on speed up is based on number of steps or number of epochs. It is unclear what are the speed advantage in terms of wall-clock time by using the propose technique. I also checked the supp. pdf. In other works (such as LARS and LANs, which are cited by this work), the authors usually report actual wall clock time speedups as they increase the batch size and the compute infra. It was disappointing to not see any mention of that given the authors are using 768 GPUs (therefore I expect very interesting scaling behaviors)"", ""Target claims"": [1, 16], ""Reasoning"": ""The extracted span argues that the claim about 'training acceleration' is not fully supported because the paper lacks information on actual wall-clock time speedups, which raises questions about the validity of Claim1, which suggests acceleration is a key contribution, as well as Claim16, which states that training with VR optimizers converge much faster (1.7~4x). This weakness challenges the claims' assertions of speed improvements, as speed advantages are only demonstrated in terms of steps or epochs, not in terms of real-world, impactful metrics like wall clock time.""}]}, ""meta"": {""id"": ""TKXMPtCniG"", ""review_idx"": 0, ""title"": ""Title: Accelerating Large Batch Training via Gradient Signal to Noise Ratio (GSNR)\nAbstract: As models for nature language processing (NLP), computer vision (CV) and recommendation systems (RS) require surging computation, a large number of GPUs/TPUs are paralleled with a large batch (LB) to improve training throughput. Training such LB tasks often converges to sharp minimum and downgrades final precision. Adversarial learning (ConAdv) and LANS method scales ImageNet and BERT pretraining up to 96k batch size. In this work, we develop the variance reduced gradient descent technique (VRGD) based on the gradient signal to noise ratio (GSNR) and apply it onto popular optimizers such as SGD/Adam/LARS/LAMB. We carry out a theoretical analysis of VR-SGD's convergence rate to explain its fast training dynamics, and a generalization analysis to demonstrate its smaller generalization gap on LB training. Comprehensive experiments demonstrate that VRGD can remarkably accelerate training ($1.7\\sim 4 \\times$), narrow the generalization gap and improve final accuracy. We push the batch size limit of BERT pretraining up to 128k/64k and DLRM to 512k without noticeable accuracy loss. We improve ImageNet Top-1 accuracy at 96k by $0.52pp$ than LARS and significantly reduce generalization gap by $68.3$%.\n\nTLDR:\n\nKeywords:large batch, optimizer"", ""claims"": [""Claim1: Comprehensive experiments demonstrate that VRGD can remarkably accelerate training ($1.7\\sim 4 \\times$), narrow the generalization gap and improve final accuracy."", ""Claim2: We push the batch size limit of BERT pretraining up to 128k/64k and DLRM to 512k without noticeable accuracy loss."", ""Claim3: We improve ImageNet Top-1 accuracy at 96k by $0.52pp$ than LARS and significantly reduce generalization gap by $68.3$%."", ""Claim4: Layer-wise LRs adjustment optimizers such as LARS [You et al. , 2017a], complete layer-wise adaptive rate scaling (CLARS, Huo et al. [2021]), LAMB [You et al. , 2020] successfully improve the batch size up to 64k both for ImageNet and BERT pretraining without accuracy loss."", ""Claim5: Recently, the concurrent adversarial learning (ConAdv) method pushes the batch size limit of ImageNet training up to 96k [Liu et al. , 2021]."", ""Claim6: LANS replaces the layer-wise LR adjustment in LAMB with block-wise style [Zheng et al. , 2020] and also pushes BERT training up to 96k."", ""Claim7: Adasum adds those gradients after scaling with proper scalars and even pushes the batch size limit of BERT up to 128k/32k [Maleki et al. , 2021]."", ""Claim8: Liu et al. [2020a] theoretically derive a quantitative relationship between GSNR and generalization gap and prove that larger GSNR leads to better generalization performance."", ""Claim9: Previous section demonstrates that updating the weights with larger GSNR confines the model\u2019s generalization gap growth during training."", ""Claim10: Therefore, GSNR can be used in the optimizer for better generalization."", ""Claim11: It shows that our assumptions are weaker than LARS/LAMB/DecentLaM and similar with SGD."", ""Claim12: The convergence rate O ( 1 \u221a \u02c6 T ) of VR-SGD is the same as SGD [Johnson and Zhang, 2013b]; 2) VR-SGD\u2019s bound depends on the lower ( r l ) and upper bound ( r u ) of GSNR."", ""Claim13: Larger batch size brings smaller gradient variance (eq.43 of Appendix.B) and larger GSNR (both bigger r l and r u ), then may result in a tighter bound with quicker convergence ( verified by experiments shown in Figure.2 )."", ""Claim14: This inequality demonstrates that VR-SGD has a much smaller generalization gap than SGD in LB training (verified by our ImageNet experiments shown in Table.3)."", ""Claim15: Table.3 demonstrates that VR-LARS can dramatically narrow the generalization gap in LB training. The generalization gap is only 1.46 for VR-LARS at 96k (68.3% smaller than LARS), even smaller than ConAdv+AA (2.2; Liu et al. [2021])."", ""Claim16: Training with VR-Momentum/VR-Adam/VR-LAMB converge much faster ( 1 . 7 \u223c 4 \u00d7 )."", ""Claim17: As for the final precision, Table.5 demonstrate that VR-Momentum/VR-Adam/VR-LAMB/VR-LARS dramatically outperform Momentum/Adam/LAMB/LARS when batch size is larger than 4k, which demonstrates that VRGD is applicable to most popular optimizers in LB training."", ""Claim18: The improvements of VRGD comparing with their base optimizers grow with the increase of batch size."", ""Claim19: VRGD optimizers remains convergent when batch size reaches 8k."", ""Claim20: Figure.3 shows that the optimal \u03b3 is around (0 . 04 , 0 . 2) for linear regression."", ""Claim21: It again demonstrates that GSNR is valuable to improve final precision."", ""Claim22: We carry out theoretical 320 derivations of convergence rate and generalization analysis to explain why VRGD can accelerate 321 large batch training and reduce generalization gap."", ""Claim23: Comprehensive experiments on BERT-pretraining, 322 ImageNet and DLRM verify that VRGD can push the batch size limit than previous SOTA optimizers 323 in LB training and perform better.""], ""review"": ""Review: Weakness: This paper claims \""training acceleration\"" as a key contribution. But throughout the paper, the comparison on speed up is based on number of steps or number of epochs. It is unclear what are the speed advantage in terms of wall-clock time by using the propose technique. I also checked the supp. pdf. In other works (such as LARS and LANs, which are cited by this work), the authors usually report actual wall clock time speedups as they increase the batch size and the compute infra. It was disappointing to not see any mention of that given the authors are using 768 GPUs (therefore I expect very interesting scaling behaviors)\nQuestions: * how does the proposed method help with the large scale trainings such as CLIP models or DiNOv2 models, which the datasets are much bigger than traditional settings like ImageNet? \n\n* what would be an ideal scenario for using the proposed technique in a computer vision task? from Table 2, there is a clear trade-off between accuracy vs batch size.\n\n* in Table 1 and Table 2, how much time does each training take (under different batch sizes)?\n\n""}, ""review"": ""Title: Accelerating Large Batch Training via Gradient Signal to Noise Ratio (GSNR)\nAbstract: As models for nature language processing (NLP), computer vision (CV) and recommendation systems (RS) require surging computation, a large number of GPUs/TPUs are paralleled with a large batch (LB) to improve training throughput. Training such LB tasks often converges to sharp minimum and downgrades final precision. Adversarial learning (ConAdv) and LANS method scales ImageNet and BERT pretraining up to 96k batch size. In this work, we develop the variance reduced gradient descent technique (VRGD) based on the gradient signal to noise ratio (GSNR) and apply it onto popular optimizers such as SGD/Adam/LARS/LAMB. We carry out a theoretical analysis of VR-SGD's convergence rate to explain its fast training dynamics, and a generalization analysis to demonstrate its smaller generalization gap on LB training. Comprehensive experiments demonstrate that VRGD can remarkably accelerate training ($1.7\\sim 4 \\times$), narrow the generalization gap and improve final accuracy. We push the batch size limit of BERT pretraining up to 128k/64k and DLRM to 512k without noticeable accuracy loss. We improve ImageNet Top-1 accuracy at 96k by $0.52pp$ than LARS and significantly reduce generalization gap by $68.3$%.\n\nClaim1: Comprehensive experiments demonstrate that VRGD can remarkably accelerate training ($1.7\\sim 4 \\times$), narrow the generalization gap and improve final accuracy.\nClaim2: We push the batch size limit of BERT pretraining up to 128k/64k and DLRM to 512k without noticeable accuracy loss.\nClaim3: We improve ImageNet Top-1 accuracy at 96k by $0.52pp$ than LARS and significantly reduce generalization gap by $68.3$%.\nClaim4: Layer-wise LRs adjustment optimizers such as LARS [You et al. , 2017a], complete layer-wise adaptive rate scaling (CLARS, Huo et al. [2021]), LAMB [You et al. , 2020] successfully improve the batch size up to 64k both for ImageNet and BERT pretraining without accuracy loss.\nClaim5: Recently, the concurrent adversarial learning (ConAdv) method pushes the batch size limit of ImageNet training up to 96k [Liu et al. , 2021].\nClaim6: LANS replaces the layer-wise LR adjustment in LAMB with block-wise style [Zheng et al. , 2020] and also pushes BERT training up to 96k.\nClaim7: Adasum adds those gradients after scaling with proper scalars and even pushes the batch size limit of BERT up to 128k/32k [Maleki et al. , 2021].\nClaim8: Liu et al. [2020a] theoretically derive a quantitative relationship between GSNR and generalization gap and prove that larger GSNR leads to better generalization performance.\nClaim9: Previous section demonstrates that updating the weights with larger GSNR confines the model\u2019s generalization gap growth during training.\nClaim10: Therefore, GSNR can be used in the optimizer for better generalization.\nClaim11: It shows that our assumptions are weaker than LARS/LAMB/DecentLaM and similar with SGD.\nClaim12: The convergence rate O ( 1 \u221a \u02c6 T ) of VR-SGD is the same as SGD [Johnson and Zhang, 2013b]; 2) VR-SGD\u2019s bound depends on the lower ( r l ) and upper bound ( r u ) of GSNR.\nClaim13: Larger batch size brings smaller gradient variance (eq.43 of Appendix.B) and larger GSNR (both bigger r l and r u ), then may result in a tighter bound with quicker convergence ( verified by experiments shown in Figure.2 ).\nClaim14: This inequality demonstrates that VR-SGD has a much smaller generalization gap than SGD in LB training (verified by our ImageNet experiments shown in Table.3).\nClaim15: Table.3 demonstrates that VR-LARS can dramatically narrow the generalization gap in LB training. The generalization gap is only 1.46 for VR-LARS at 96k (68.3% smaller than LARS), even smaller than ConAdv+AA (2.2; Liu et al. [2021]).\nClaim16: Training with VR-Momentum/VR-Adam/VR-LAMB converge much faster ( 1 . 7 \u223c 4 \u00d7 ).\nClaim17: As for the final precision, Table.5 demonstrate that VR-Momentum/VR-Adam/VR-LAMB/VR-LARS dramatically outperform Momentum/Adam/LAMB/LARS when batch size is larger than 4k, which demonstrates that VRGD is applicable to most popular optimizers in LB training.\nClaim18: The improvements of VRGD comparing with their base optimizers grow with the increase of batch size.\nClaim19: VRGD optimizers remains convergent when batch size reaches 8k.\nClaim20: Figure.3 shows that the optimal \u03b3 is around (0 . 04 , 0 . 2) for linear regression.\nClaim21: It again demonstrates that GSNR is valuable to improve final precision.\nClaim22: We carry out theoretical 320 derivations of convergence rate and generalization analysis to explain why VRGD can accelerate 321 large batch training and reduce generalization gap.\nClaim23: Comprehensive experiments on BERT-pretraining, 322 ImageNet and DLRM verify that VRGD can push the batch size limit than previous SOTA optimizers 323 in LB training and perform better.\n\nReview: Weakness: This paper claims \""training acceleration\"" as a key contribution. But throughout the paper, the comparison on speed up is based on number of steps or number of epochs. It is unclear what are the speed advantage in terms of wall-clock time by using the propose technique. I also checked the supp. pdf. In other works (such as LARS and LANs, which are cited by this work), the authors usually report actual wall clock time speedups as they increase the batch size and the compute infra. It was disappointing to not see any mention of that given the authors are using 768 GPUs (therefore I expect very interesting scaling behaviors)\nQuestions: * how does the proposed method help with the large scale trainings such as CLIP models or DiNOv2 models, which the datasets are much bigger than traditional settings like ImageNet? \n\n* what would be an ideal scenario for using the proposed technique in a computer vision task? from Table 2, there is a clear trade-off between accuracy vs batch size.\n\n* in Table 1 and Table 2, how much time does each training take (under different batch sizes)?\n\n\n\n"", ""id"": ""TKXMPtCniG0"", ""pdf"": ""openreview.net/pdf?id=TKXMPtCniG""}","18"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""2. (Major) Continuing with the previous comment, I find the proposed method is rather heuristic, lacking a clear motivation. I could not find clear rationale and sufficient analysis that the proposed method can benefit training. Essentially, the proposed method is simply to scale the learning rate adaptively based on a specific parameter (Eq. 10). For me, I am not quite convinced that such a learning rate scaling policy will lead to reasonable performance gain. Meanwhile, the author argue that the proposed method can lead to smaller generalization gap. However, the core that supports this claim is based on an empirical observation, which makes the mathematical proof not quite rigorous and the claim much weaker and unhelpful."", ""Target claims"": [1, 14, 22], ""Reasoning"": ""The reviewer claims the proposed method is heuristic and lacks clear rationale which questions the claims that VRGD can accelerate training, reduce generalization gap, and the theoretical support (Claim1, Claim14, Claim22). Specifically, doubts about the reasonable performance gain and rigorousness of mathematical proof directly challenge the claims of narrowing the generalization gap.""}, {""Weakness span"": ""3. (Major) I would like to discuss the convergence of the proposed method. Firstly, to my understanding, the convergence analysis focuses on analyzing to what extent can training converge on the given training samples, not testing set. So, it is not quite appropriate to use the convergence curve on the testing set to demonstrate the conclusion regarding the convergence analysis. i.e. Figure 2. Secondly, from Figure 2, the authors state that the proposed method converge 1.7~4 times faster than the conventional optimizers. But, I could not observe such a big gap between them from Figure 2, so could the authors explain how to measure the convergence here."", ""Target claims"": [1, 16], ""Reasoning"": ""This span questions the evidence supporting fast convergence rates claimed for VR-Momentum/VR-Adam/VR-LAMB (Claim16) and VRGD's acceleration (Claim1) by arguing that the claimed acceleration in convergence is not evident in the reported results, directly challenging the claim.""}]}, ""meta"": {""id"": ""TKXMPtCniG"", ""review_idx"": 1, ""title"": ""Title: Accelerating Large Batch Training via Gradient Signal to Noise Ratio (GSNR)\nAbstract: As models for nature language processing (NLP), computer vision (CV) and recommendation systems (RS) require surging computation, a large number of GPUs/TPUs are paralleled with a large batch (LB) to improve training throughput. Training such LB tasks often converges to sharp minimum and downgrades final precision. Adversarial learning (ConAdv) and LANS method scales ImageNet and BERT pretraining up to 96k batch size. In this work, we develop the variance reduced gradient descent technique (VRGD) based on the gradient signal to noise ratio (GSNR) and apply it onto popular optimizers such as SGD/Adam/LARS/LAMB. We carry out a theoretical analysis of VR-SGD's convergence rate to explain its fast training dynamics, and a generalization analysis to demonstrate its smaller generalization gap on LB training. Comprehensive experiments demonstrate that VRGD can remarkably accelerate training ($1.7\\sim 4 \\times$), narrow the generalization gap and improve final accuracy. We push the batch size limit of BERT pretraining up to 128k/64k and DLRM to 512k without noticeable accuracy loss. We improve ImageNet Top-1 accuracy at 96k by $0.52pp$ than LARS and significantly reduce generalization gap by $68.3$%.\n\nTLDR:\n\nKeywords:large batch, optimizer"", ""claims"": [""Claim1: Comprehensive experiments demonstrate that VRGD can remarkably accelerate training ($1.7\\sim 4 \\times$), narrow the generalization gap and improve final accuracy."", ""Claim2: We push the batch size limit of BERT pretraining up to 128k/64k and DLRM to 512k without noticeable accuracy loss."", ""Claim3: We improve ImageNet Top-1 accuracy at 96k by $0.52pp$ than LARS and significantly reduce generalization gap by $68.3$%."", ""Claim4: Layer-wise LRs adjustment optimizers such as LARS [You et al. , 2017a], complete layer-wise adaptive rate scaling (CLARS, Huo et al. [2021]), LAMB [You et al. , 2020] successfully improve the batch size up to 64k both for ImageNet and BERT pretraining without accuracy loss."", ""Claim5: Recently, the concurrent adversarial learning (ConAdv) method pushes the batch size limit of ImageNet training up to 96k [Liu et al. , 2021]."", ""Claim6: LANS replaces the layer-wise LR adjustment in LAMB with block-wise style [Zheng et al. , 2020] and also pushes BERT training up to 96k."", ""Claim7: Adasum adds those gradients after scaling with proper scalars and even pushes the batch size limit of BERT up to 128k/32k [Maleki et al. , 2021]."", ""Claim8: Liu et al. [2020a] theoretically derive a quantitative relationship between GSNR and generalization gap and prove that larger GSNR leads to better generalization performance."", ""Claim9: Previous section demonstrates that updating the weights with larger GSNR confines the model\u2019s generalization gap growth during training."", ""Claim10: Therefore, GSNR can be used in the optimizer for better generalization."", ""Claim11: It shows that our assumptions are weaker than LARS/LAMB/DecentLaM and similar with SGD."", ""Claim12: The convergence rate O ( 1 \u221a \u02c6 T ) of VR-SGD is the same as SGD [Johnson and Zhang, 2013b]; 2) VR-SGD\u2019s bound depends on the lower ( r l ) and upper bound ( r u ) of GSNR."", ""Claim13: Larger batch size brings smaller gradient variance (eq.43 of Appendix.B) and larger GSNR (both bigger r l and r u ), then may result in a tighter bound with quicker convergence ( verified by experiments shown in Figure.2 )."", ""Claim14: This inequality demonstrates that VR-SGD has a much smaller generalization gap than SGD in LB training (verified by our ImageNet experiments shown in Table.3)."", ""Claim15: Table.3 demonstrates that VR-LARS can dramatically narrow the generalization gap in LB training. The generalization gap is only 1.46 for VR-LARS at 96k (68.3% smaller than LARS), even smaller than ConAdv+AA (2.2; Liu et al. [2021])."", ""Claim16: Training with VR-Momentum/VR-Adam/VR-LAMB converge much faster ( 1 . 7 \u223c 4 \u00d7 )."", ""Claim17: As for the final precision, Table.5 demonstrate that VR-Momentum/VR-Adam/VR-LAMB/VR-LARS dramatically outperform Momentum/Adam/LAMB/LARS when batch size is larger than 4k, which demonstrates that VRGD is applicable to most popular optimizers in LB training."", ""Claim18: The improvements of VRGD comparing with their base optimizers grow with the increase of batch size."", ""Claim19: VRGD optimizers remains convergent when batch size reaches 8k."", ""Claim20: Figure.3 shows that the optimal \u03b3 is around (0 . 04 , 0 . 2) for linear regression."", ""Claim21: It again demonstrates that GSNR is valuable to improve final precision."", ""Claim22: We carry out theoretical 320 derivations of convergence rate and generalization analysis to explain why VRGD can accelerate 321 large batch training and reduce generalization gap."", ""Claim23: Comprehensive experiments on BERT-pretraining, 322 ImageNet and DLRM verify that VRGD can push the batch size limit than previous SOTA optimizers 323 in LB training and perform better.""], ""review"": ""Review: Weakness: ***Weakness***\n\n\n1. (Major) The authors repeatedly mention that large batch training can lead to sharp minima in Abstract and Introduction, which seems to suggest that the proposed method can avoid such problem. However, I have not found discussions or observations regarding the proposed method can solve this issue. So can the proposed escape these bad minima?\n\n    Considering that many recent works that focuses on guiding training to converge to flat minima i.e. SAM family/gradient norm regularization, I am quite curious what would be like to adopt the proposed method in these algorithms when given large batch training. And from the results, I find that using the proposed method alone would not result better performance than these flat-minima-based methods. I have listed some typical works below.\n\n    [1] Foret, Pierre, et al. \""Sharpness-aware minimization for efficiently improving generalization.\"" ICLR2020.\n\n    [2] Kwon, Jungmin, et al. \""Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks.\"" ICML2021.\n\n    [3] Zhuang, Juntang, et al. \""Surrogate gap minimization improves sharpness-aware training.\"" ICLR2021.\n\n    [4] Zhao, Yang, et al. \""Penalizing gradient norm for efficiently improving generalization in deep learning.\"" ICML 2022.\n\n2. (Major) Continuing with the previous comment, I find the proposed method is rather heuristic, lacking a clear motivation. I could not find clear rationale and sufficient analysis that the proposed method can benefit training. Essentially, the proposed method is simply to scale the learning rate adaptively based on a specific parameter (Eq. 10). For me, I am not quite convinced that such a learning rate scaling policy will lead to reasonable performance gain. Meanwhile, the author argue that the proposed method can lead to smaller generalization gap. However, the core that supports this claim is based on an empirical observation, which makes the mathematical proof not quite rigorous and the claim much weaker and unhelpful. \n\n3. (Major) I would like to discuss the convergence of the proposed method. Firstly, to my understanding, the convergence analysis focuses on analyzing to what extent can training converge on the given training samples, not testing set. So, it is not quite appropriate to use the convergence curve on the testing set to demonstrate the conclusion regarding the convergence analysis. i.e. Figure 2. Secondly, from Figure 2, the authors state that the proposed method converge 1.7~4 times faster than the conventional optimizers. But, I could not observe such a big gap between them from Figure 2, so could the authors explain how to measure the convergence here. Thirdly, a tighter bound in convergence would not give any information regarding the testing performance. A looser bound and slower convergence rate can give better testing performance in many cases, for example SAM. The author can refer to the paper below. \n\n    [5] Andriushchenko, Maksym, and Nicolas Flammarion. \""Towards understanding sharpness-aware minimization.\"" ICML 2022.\n\n    Note that I am not saying a faster convergence is harmful. In my opinion, the core meaning of this convergence section is to prove that the proposed method can converge in finite time. And it is not surprising that The proposed method shares the same convergence rate as that in SGD, i.e. O(1/sqrt{T}) given that this method is to scale the learning rate compared to SGD. In the current version, it appears that the convergence section seems to heavily imply that the proposed method outperforms SGD without a promise of testing performance, which I disagree with.\n\n4. (Minor) Line 32. \""However, Keskar et al. [2017] theoretically analyze the LB training and finds that it can be easily trapped into sharp local minimum, leading to strong generalization gap\"". Actually, the cited paper (Keskar et al. 2017) has not provided theoretical analysis.\n\n5. (Minor) Line 95. To my understanding, the variance of random vectors is a matrix, i.e. covariance matrix. Why is a scalar here?\n\n6. (Minor) It is highly encouraged to show the results of training vision transformers with the proposed methods.\nQuestions: See weakness.\n\n""}, ""review"": ""Title: Accelerating Large Batch Training via Gradient Signal to Noise Ratio (GSNR)\nAbstract: As models for nature language processing (NLP), computer vision (CV) and recommendation systems (RS) require surging computation, a large number of GPUs/TPUs are paralleled with a large batch (LB) to improve training throughput. Training such LB tasks often converges to sharp minimum and downgrades final precision. Adversarial learning (ConAdv) and LANS method scales ImageNet and BERT pretraining up to 96k batch size. In this work, we develop the variance reduced gradient descent technique (VRGD) based on the gradient signal to noise ratio (GSNR) and apply it onto popular optimizers such as SGD/Adam/LARS/LAMB. We carry out a theoretical analysis of VR-SGD's convergence rate to explain its fast training dynamics, and a generalization analysis to demonstrate its smaller generalization gap on LB training. Comprehensive experiments demonstrate that VRGD can remarkably accelerate training ($1.7\\sim 4 \\times$), narrow the generalization gap and improve final accuracy. We push the batch size limit of BERT pretraining up to 128k/64k and DLRM to 512k without noticeable accuracy loss. We improve ImageNet Top-1 accuracy at 96k by $0.52pp$ than LARS and significantly reduce generalization gap by $68.3$%.\n\nClaim1: Comprehensive experiments demonstrate that VRGD can remarkably accelerate training ($1.7\\sim 4 \\times$), narrow the generalization gap and improve final accuracy.\nClaim2: We push the batch size limit of BERT pretraining up to 128k/64k and DLRM to 512k without noticeable accuracy loss.\nClaim3: We improve ImageNet Top-1 accuracy at 96k by $0.52pp$ than LARS and significantly reduce generalization gap by $68.3$%.\nClaim4: Layer-wise LRs adjustment optimizers such as LARS [You et al. , 2017a], complete layer-wise adaptive rate scaling (CLARS, Huo et al. [2021]), LAMB [You et al. , 2020] successfully improve the batch size up to 64k both for ImageNet and BERT pretraining without accuracy loss.\nClaim5: Recently, the concurrent adversarial learning (ConAdv) method pushes the batch size limit of ImageNet training up to 96k [Liu et al. , 2021].\nClaim6: LANS replaces the layer-wise LR adjustment in LAMB with block-wise style [Zheng et al. , 2020] and also pushes BERT training up to 96k.\nClaim7: Adasum adds those gradients after scaling with proper scalars and even pushes the batch size limit of BERT up to 128k/32k [Maleki et al. , 2021].\nClaim8: Liu et al. [2020a] theoretically derive a quantitative relationship between GSNR and generalization gap and prove that larger GSNR leads to better generalization performance.\nClaim9: Previous section demonstrates that updating the weights with larger GSNR confines the model\u2019s generalization gap growth during training.\nClaim10: Therefore, GSNR can be used in the optimizer for better generalization.\nClaim11: It shows that our assumptions are weaker than LARS/LAMB/DecentLaM and similar with SGD.\nClaim12: The convergence rate O ( 1 \u221a \u02c6 T ) of VR-SGD is the same as SGD [Johnson and Zhang, 2013b]; 2) VR-SGD\u2019s bound depends on the lower ( r l ) and upper bound ( r u ) of GSNR.\nClaim13: Larger batch size brings smaller gradient variance (eq.43 of Appendix.B) and larger GSNR (both bigger r l and r u ), then may result in a tighter bound with quicker convergence ( verified by experiments shown in Figure.2 ).\nClaim14: This inequality demonstrates that VR-SGD has a much smaller generalization gap than SGD in LB training (verified by our ImageNet experiments shown in Table.3).\nClaim15: Table.3 demonstrates that VR-LARS can dramatically narrow the generalization gap in LB training. The generalization gap is only 1.46 for VR-LARS at 96k (68.3% smaller than LARS), even smaller than ConAdv+AA (2.2; Liu et al. [2021]).\nClaim16: Training with VR-Momentum/VR-Adam/VR-LAMB converge much faster ( 1 . 7 \u223c 4 \u00d7 ).\nClaim17: As for the final precision, Table.5 demonstrate that VR-Momentum/VR-Adam/VR-LAMB/VR-LARS dramatically outperform Momentum/Adam/LAMB/LARS when batch size is larger than 4k, which demonstrates that VRGD is applicable to most popular optimizers in LB training.\nClaim18: The improvements of VRGD comparing with their base optimizers grow with the increase of batch size.\nClaim19: VRGD optimizers remains convergent when batch size reaches 8k.\nClaim20: Figure.3 shows that the optimal \u03b3 is around (0 . 04 , 0 . 2) for linear regression.\nClaim21: It again demonstrates that GSNR is valuable to improve final precision.\nClaim22: We carry out theoretical 320 derivations of convergence rate and generalization analysis to explain why VRGD can accelerate 321 large batch training and reduce generalization gap.\nClaim23: Comprehensive experiments on BERT-pretraining, 322 ImageNet and DLRM verify that VRGD can push the batch size limit than previous SOTA optimizers 323 in LB training and perform better.\n\nReview: Weakness: ***Weakness***\n\n\n1. (Major) The authors repeatedly mention that large batch training can lead to sharp minima in Abstract and Introduction, which seems to suggest that the proposed method can avoid such problem. However, I have not found discussions or observations regarding the proposed method can solve this issue. So can the proposed escape these bad minima?\n\n    Considering that many recent works that focuses on guiding training to converge to flat minima i.e. SAM family/gradient norm regularization, I am quite curious what would be like to adopt the proposed method in these algorithms when given large batch training. And from the results, I find that using the proposed method alone would not result better performance than these flat-minima-based methods. I have listed some typical works below.\n\n    [1] Foret, Pierre, et al. \""Sharpness-aware minimization for efficiently improving generalization.\"" ICLR2020.\n\n    [2] Kwon, Jungmin, et al. \""Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks.\"" ICML2021.\n\n    [3] Zhuang, Juntang, et al. \""Surrogate gap minimization improves sharpness-aware training.\"" ICLR2021.\n\n    [4] Zhao, Yang, et al. \""Penalizing gradient norm for efficiently improving generalization in deep learning.\"" ICML 2022.\n\n2. (Major) Continuing with the previous comment, I find the proposed method is rather heuristic, lacking a clear motivation. I could not find clear rationale and sufficient analysis that the proposed method can benefit training. Essentially, the proposed method is simply to scale the learning rate adaptively based on a specific parameter (Eq. 10). For me, I am not quite convinced that such a learning rate scaling policy will lead to reasonable performance gain. Meanwhile, the author argue that the proposed method can lead to smaller generalization gap. However, the core that supports this claim is based on an empirical observation, which makes the mathematical proof not quite rigorous and the claim much weaker and unhelpful. \n\n3. (Major) I would like to discuss the convergence of the proposed method. Firstly, to my understanding, the convergence analysis focuses on analyzing to what extent can training converge on the given training samples, not testing set. So, it is not quite appropriate to use the convergence curve on the testing set to demonstrate the conclusion regarding the convergence analysis. i.e. Figure 2. Secondly, from Figure 2, the authors state that the proposed method converge 1.7~4 times faster than the conventional optimizers. But, I could not observe such a big gap between them from Figure 2, so could the authors explain how to measure the convergence here. Thirdly, a tighter bound in convergence would not give any information regarding the testing performance. A looser bound and slower convergence rate can give better testing performance in many cases, for example SAM. The author can refer to the paper below. \n\n    [5] Andriushchenko, Maksym, and Nicolas Flammarion. \""Towards understanding sharpness-aware minimization.\"" ICML 2022.\n\n    Note that I am not saying a faster convergence is harmful. In my opinion, the core meaning of this convergence section is to prove that the proposed method can converge in finite time. And it is not surprising that The proposed method shares the same convergence rate as that in SGD, i.e. O(1/sqrt{T}) given that this method is to scale the learning rate compared to SGD. In the current version, it appears that the convergence section seems to heavily imply that the proposed method outperforms SGD without a promise of testing performance, which I disagree with.\n\n4. (Minor) Line 32. \""However, Keskar et al. [2017] theoretically analyze the LB training and finds that it can be easily trapped into sharp local minimum, leading to strong generalization gap\"". Actually, the cited paper (Keskar et al. 2017) has not provided theoretical analysis.\n\n5. (Minor) Line 95. To my understanding, the variance of random vectors is a matrix, i.e. covariance matrix. Why is a scalar here?\n\n6. (Minor) It is highly encouraged to show the results of training vision transformers with the proposed methods.\nQuestions: See weakness.\n\n\n\n"", ""id"": ""TKXMPtCniG1"", ""pdf"": ""openreview.net/pdf?id=TKXMPtCniG""}","19"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""The author claims it is the first work to reduce the hidden size, but I doubt if the method can be successfully implemented in the industry since the lack of experimental results related to inference speed of compressed model."", ""Target claims"": [7], ""Reasoning"": ""The reviewer questions the claim regarding the novelty of TCSP's ability to reduce the hidden size (Claim7). They express skepticism about the practical applicability of this feature due to the absence of experimental results on the inference speed of the compressed model. This directly targets Claim7 as it questions the significance and practical implementation of reducing the hidden size, which is the primary focus of this claim.""}]}, ""meta"": {""id"": ""SJw4Da8BuR"", ""review_idx"": 0, ""title"": ""Title: Transformer Compression via Subspace Projection\nAbstract: We propose TCSP, a novel method for compressing a transformer model by focusing on reducing the hidden size of the model. By projecting the whole transform model into a subspace, we enable matrix operations between the weight matrices in the model and features in a reduced-dimensional space, leading to significant reductions in model parameters and computing resources. To establish this subspace, we decompose the feature matrix, derived from different layers of sampled data instances, into a projection matrix. \nFor evaluation, TCSP is applied to compress T5 and BERT models on the GLUE and SQuAD benchmarks. Experimental results demonstrate that TCSP achieves a compression ratio of 44\\% with at most 1.6\\% degradation in accuracy, surpassing or matching prior compression methods. Furthermore, TCSP exhibits compatibility with other methods targeting filter and attention head size compression.\n\nTLDR:\n\nKeywords:low-rank factorization, model compression, transformer"", ""claims"": [""Claim1: Experimental results demonstrate that TCSP achieves a compression ratio of 44% with at most 1.6% degradation in accuracy, surpassing or matching prior compression methods."", ""Claim2: Furthermore, TCSP exhibits compatibility with other methods targeting filter and attention head size compression."", ""Claim3: In this work, we focus on exploring low-rank factorization and pruning approaches as means to directly reduce the number of parameters in fined-tuned task-specific models."", ""Claim4: Various unstructured pruning approaches have been proposed for Transformer, such as magnitude-based [30], first-order [31], second-order [32], and lottery ticket hypothesis [33]. Although unstructured pruning algorithms can remove many redundant parameters while ensuring accuracy, compressed models require specific sparse data structures and hardware support to take advantage of unstructured pruning."", ""Claim5: Experimental results demonstrate that TCSP achieves a compression ratio of 44% with at most 1.6% degradation in accuracy, surpassing or matching prior compression methods."", ""Claim6: Furthermore, TCSP exhibits compatibility with other methods targeting filter and attention head size compression."", ""Claim7: The proposed TCSP can reduce the hidden size d."", ""Claim8: When employing TCSP with a compression rate of {25%, 0%} which retains 75% hidden size while preserving the original attention head size and number of \ufb01lters, we observe that the compressed T5 base and BERT base models exhibit a maximum drop in accuracy of only 1.3% on the GLUE and SQuAD datasets."", ""Claim9: Building upon this, We further apply \ufb01lter pruning and compress the head size, resulting in a model denoted as TCSP {25%, 25%}. Remarkably, this additional compression does not negatively impact the baseline performance, highlighting the compatibility of TCSP with the \ufb01lter pruning and attention head size compression methods."", ""Claim10: Overall, we achieve a compression rate of 44% for both T5 base and BERT base models with only a 1.6% loss in accuracy."", ""Claim11: Our method TCSP demonstrates comparable or superior results compared to the prior methods (A lower accuracy drop indicates better performance)."", ""Claim12: Table 3 shows that TCSP requires only 0.16 hours for model compression and an additional 2 hours to model \ufb01ne-tuning. This indicates that TCSP is signi\ufb01cantly faster than most of the comparison methods, with the exception of the pruning method proposed in Kwon et al. [8]."", ""Claim13: It is observed that using a random matrix for compression signi\ufb01cantly diminishes the model\u2019s performance."", ""Claim14: Remarkably, TCSP achieves a compression ratio of 44% while incurring only a 1.6% decrease in accuracy.""], ""review"": ""Review: Weakness: $\\cdot$  The novelty of this paper is limited, the core idea resembles low-rank factorization with SVD, and the approach is more like a combination of SVD and model pruning.\n\n$\\cdot$ The author claims it is the first work to reduce the hidden size, but I doubt if the method can be successfully implemented in the industry since the lack of experimental results related to inference speed of compressed model.\n\nQuestions: $\\cdot$ What are the experiment settings in Tab. 2 and the ablation study? \n\n$\\cdot$ There are some writing mistakes in this paper, e.g. \u201cfien-tune\u201d in the header of Tab. 3.\n\n""}, ""review"": ""Title: Transformer Compression via Subspace Projection\nAbstract: We propose TCSP, a novel method for compressing a transformer model by focusing on reducing the hidden size of the model. By projecting the whole transform model into a subspace, we enable matrix operations between the weight matrices in the model and features in a reduced-dimensional space, leading to significant reductions in model parameters and computing resources. To establish this subspace, we decompose the feature matrix, derived from different layers of sampled data instances, into a projection matrix. \nFor evaluation, TCSP is applied to compress T5 and BERT models on the GLUE and SQuAD benchmarks. Experimental results demonstrate that TCSP achieves a compression ratio of 44\\% with at most 1.6\\% degradation in accuracy, surpassing or matching prior compression methods. Furthermore, TCSP exhibits compatibility with other methods targeting filter and attention head size compression.\n\nClaim1: Experimental results demonstrate that TCSP achieves a compression ratio of 44% with at most 1.6% degradation in accuracy, surpassing or matching prior compression methods.\nClaim2: Furthermore, TCSP exhibits compatibility with other methods targeting filter and attention head size compression.\nClaim3: In this work, we focus on exploring low-rank factorization and pruning approaches as means to directly reduce the number of parameters in fined-tuned task-specific models.\nClaim4: Various unstructured pruning approaches have been proposed for Transformer, such as magnitude-based [30], first-order [31], second-order [32], and lottery ticket hypothesis [33]. Although unstructured pruning algorithms can remove many redundant parameters while ensuring accuracy, compressed models require specific sparse data structures and hardware support to take advantage of unstructured pruning.\nClaim5: Experimental results demonstrate that TCSP achieves a compression ratio of 44% with at most 1.6% degradation in accuracy, surpassing or matching prior compression methods.\nClaim6: Furthermore, TCSP exhibits compatibility with other methods targeting filter and attention head size compression.\nClaim7: The proposed TCSP can reduce the hidden size d.\nClaim8: When employing TCSP with a compression rate of {25%, 0%} which retains 75% hidden size while preserving the original attention head size and number of \ufb01lters, we observe that the compressed T5 base and BERT base models exhibit a maximum drop in accuracy of only 1.3% on the GLUE and SQuAD datasets.\nClaim9: Building upon this, We further apply \ufb01lter pruning and compress the head size, resulting in a model denoted as TCSP {25%, 25%}. Remarkably, this additional compression does not negatively impact the baseline performance, highlighting the compatibility of TCSP with the \ufb01lter pruning and attention head size compression methods.\nClaim10: Overall, we achieve a compression rate of 44% for both T5 base and BERT base models with only a 1.6% loss in accuracy.\nClaim11: Our method TCSP demonstrates comparable or superior results compared to the prior methods (A lower accuracy drop indicates better performance).\nClaim12: Table 3 shows that TCSP requires only 0.16 hours for model compression and an additional 2 hours to model \ufb01ne-tuning. This indicates that TCSP is signi\ufb01cantly faster than most of the comparison methods, with the exception of the pruning method proposed in Kwon et al. [8].\nClaim13: It is observed that using a random matrix for compression signi\ufb01cantly diminishes the model\u2019s performance.\nClaim14: Remarkably, TCSP achieves a compression ratio of 44% while incurring only a 1.6% decrease in accuracy.\n\nReview: Weakness: $\\cdot$  The novelty of this paper is limited, the core idea resembles low-rank factorization with SVD, and the approach is more like a combination of SVD and model pruning.\n\n$\\cdot$ The author claims it is the first work to reduce the hidden size, but I doubt if the method can be successfully implemented in the industry since the lack of experimental results related to inference speed of compressed model.\n\nQuestions: $\\cdot$ What are the experiment settings in Tab. 2 and the ablation study? \n\n$\\cdot$ There are some writing mistakes in this paper, e.g. \u201cfien-tune\u201d in the header of Tab. 3.\n\n\n\n"", ""id"": ""SJw4Da8BuR0"", ""pdf"": ""openreview.net/pdf?id=SJw4Da8BuR""}","20"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""The rationale for why on-policy learning brings performance gains is not well clarified. The cited reference [1] does not provide strong support for this claim. There is no experimental evidence that on-policy sampling encourages exploration."", ""Target claims"": [10, 13, 14], ""Reasoning"": ""The review questions the clarity of the rationale behind the claimed performance gains from on-policy learning, particularly with regard to its role in exploration, indicating that this aspect is not well-supported experimentally. Claim10 mentions the effectiveness of OAIF, which could imply that on-policy learning is part of this advantage. Claim13 and Claim14 also discuss the detriments of offline feedback and advantages of online on-policy feedback, suggesting that their truths are being questioned by the weakness due to lack of experimental evidence and clarity.""}, {""Weakness span"": ""It is unclear how much of the performance gains are due to on-policy sampling versus online feedback."", ""Target claims"": [10, 13, 14, 15], ""Reasoning"": ""This weakness span identifies the ambiguity in the source of performance gains, questioning whether they are truly due to on-policy sampling or online feedback. Claims10, 13, 14, and 15 assert the effectiveness and superiority of OAIF, which involves online and potentially on-policy strategies. The lack of clarity on the specific contribution of each aspect puts these claims under scrutiny.""}, {""Weakness span"": ""The reasons why utilizing online on-policy data can avoid overfitting and improve performance should be further analyzed and discussed."", ""Target claims"": [10, 13, 14], ""Reasoning"": ""The review criticizes the lack of analysis on how online on-policy data specifically helps in avoiding overfitting and enhancing performance. This directly challenges claims related to the superiority of OAIF (Claims10, 13, and 14), as these claims suggest that such a methodology does improve performance through mechanisms that should include avoiding overfitting.""}]}, ""meta"": {""id"": ""zFHJUSTZka"", ""review_idx"": 0, ""title"": ""Title: Direct Language Model Alignment from Online AI Feedback\nAbstract: Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to reinforcement learning from human feedback (RLHF), that do not require a separate reward model. However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy. In this study, we posit that online feedback is key and improves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as annotator: on each training iteration, we sample two responses from the current model and prompt the LLM annotator to choose which one is preferred, thus providing online feedback. Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF methods. We further show that the feedback leveraged in OAIF is easily controllable, via instruction prompts to the LLM annotator.\n\nTLDR:This work improves the alignment of LLMs by directly optimising models with preferences from online AI feedback.\n\nKeywords:LLM Alignment; AI Feedback;  On-policy Learning; Online Feedback"", ""claims"": [""Claim1: Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF methods."", ""Claim2: We further show that the feedback leveraged in OAIF is easily controllable, via instruction prompts to the LLM annotator."", ""Claim3: Liu et al. (2023) proposed RSO, a method that uses an RM to perform rejection sampling in order to sample from the optimal policy, which improved the alignment compared to offline DAP baselines."", ""Claim4: Although the aforementioned RM-based methods make the alignment of a policy online and on-policy, the distribution shift problem still exists when training the RM."", ""Claim5: In comparison, OAIF can leverage feedback from any LLM, including ones stronger than the LLM being aligned."", ""Claim6: Bridging the gap . As we saw, DAP methods are simple, do not require a separate RM, but they use preference data pre-collected offline."", ""Claim7: To bridge the gap between these two families of methods, we propose a simple yet effective way to make DAP methods online."", ""Claim8: To solve the aforementioned offline problem in Algorithm"", ""Claim9: Prior studies have shown that AI feedback is a reliable and effective approximation to human labellers, especially for pairwise preference labelling (Lee et al., 2023)."", ""Claim10: However, as indicated by the sharp drop of the red curve around training step 3 , 500 , offline DPO rapidly overfits the offline and off-policy preferences in D . In contrast, the win rate of online DPO keeps increasing over training, and surpasses offline DPO after 4 , 000 steps. This demonstrates the effectiveness of OAIF."", ""Claim11: The results, given in Appendix D, confirm that our observations hold under both automatic evaluators."", ""Claim12: Human evaluation shows that OAIF significantly improves the performance of DPO across all tasks with substantial superiority over offline DPO."", ""Claim13: This consolidates our conclusion that using the offline feedback and off-policy generations in a pre collected preference dataset D can be detrimental for LLM alignment, and OAIF benefits greatly from online and on-policy AI feedback."", ""Claim14: Compared to their offline counterparts, DAP methods with OAIF achieve promising win rates, ranging from \u223c 64% to \u223c 71% . The consistent ineffectiveness of offline DAP methods confirms that the existence of the offline and off-policy issue in DAP methods and greatly hinders the performance of aligning LLMs."", ""Claim15: The consistent superiority of online DAP methods via OAIF against their offline counterparts demonstrates that OAIF is a general framework effectively addressing these challenges."", ""Claim16: Figure 4a shows the human evaluation results, where online DPO is more preferred than the other methods, in 58% of the time."", ""Claim17: To verify this hypothesis, we also trained an online DPO with the same RM used for RLAIF."", ""Claim18: It outperforms RLAIF, but significantly underperforms online DPO with OAIF, with a win rate of < 30% judged by Gemini Pro."", ""Claim19: This experimental result supports the superiority of using LLMs over RMs to provide online feedback."", ""Claim20: The results in Figure 4b show that online DPO with OAIF provides responses of higher quality than the other methods at fixed length, which further validates the effectiveness of OAIF."", ""Claim21: The size of the LLM annotator clearly has a significant impact on OAIF."", ""Claim22: Generally, as size increases, online DPO obtains better performance."", ""Claim23: Compared to the initial SFT model, online DPO with OAIF performs significantly better regardless of AI labeller model sizes, suggesting that even OAIF from a small LLM annotator is helpful in improving the performance of alignment."", ""Claim24: In particular, OAIF with PaLM 2-XS (i.e. an LLM annotator of same-size) achieves comparable performance to RLHF, although the latter learns from human feedback."", ""Claim25: Further human evaluation confirms this observation: OAIF with PaLM 2-XS obtains an overall quality score of 3.41 out of 5, slightly better than RLHF (3.38) and comparable to offline DPO (3.46)."", ""Claim26: While the necessity of LLM alignment has been widely recognised, what to align them with is still under debate, as human expectations vary greatly across regions and cultures, and may evolve over time."", ""Claim27: To examine this, we choose to explore the controllability of the length of responses by modifying the prompts to the LLM annotators."", ""Claim28: The \u201cshort\u201d and \u201cvery short\u201d prompts given to the LLM annotator significantly shorten the responses from \u223c 120 tokens to \u223c 90 and \u223c 40 tokens respectively."", ""Claim29: This direct evidence demonstrates that the behaviour of policy \u03c0 \u03b8 can be significantly changed through prompting the annotating LLM differently, and the degree of the changes can be controlled as well."", ""Claim30: However, the above changes come at a cost."", ""Claim31: We noticed that the shorter responses become much less helpful, as judged by Gemini Pro."", ""Claim32: Nevertheless, they still improve the performance of the aligned model over the SFT baseline."", ""Claim33: This finding is also confirmed by human evaluation: from \u201chelpful\u201d, \u201chelpful and short\u201d to \u201chelpful and very short\u201d, the average quality score drops from 4.08, 3.72 to 3.26, all outperforming the SFT baseline (3.19) still."", ""Claim34: Section 4.5 shows that PaLM 2-XS could provide reasonable feedback that helps improving the alignment of LLMs, although it\u2019s significantly smaller than PaLM 2-S/L."", ""Claim35: Our results suggest that OAIF from a weaker teacher indeed improved the alignment of PaLM 2-S, though they are less effective compared with the OAIF from a stronger teacher."", ""Claim36: We hereby emphasise the essential difference between the setup investigated by Burns et al. (2023) and ours."", ""Claim37: We carried out an extensive empirical evaluation, using both AI and human evaluation, which showed the effectiveness of DAP methods combined with OAIF, against their offline counterparts."", ""Claim38: We also exhibited the tendency of offline DAP methods to overfit, and in contrast the usefulness of OAIF as a way to mitigate reward overoptimization."", ""Claim39: We further verified the generality of OAIF, as our empirical results hold for three prominent DAP methods: DPO, IPO and SLiC."", ""Claim40: Our experimental results in various tasks show that online DPO outperforms RLAIF and RLHF, which further confirms the effectiveness of OAIF, compared to offline feedback."", ""Claim41: Moreover, we used response length as a test bed to demonstrate that the LLM annotator can be controlled easily using instruction prompts. This shows that OAIF can be used to achieve desirable alignment goals."", ""Claim42: Overall, this work demonstrates the effectiveness and importance of OAIF for aligning LLMs, and paves the way for more scalable alignment strategies, requiring reduced human annotation effort."", ""Claim43: We propose a new method to improve the alignment of AI with human values."", ""Claim44: Our method paves the way for more scalable alignment with reduced human efforts."", ""Claim45: Since we rely on AI feedback, to tackle other challenges in RLHF (Casper et al., 2023) and mitigate safety risks (Amodei et al., 2016), our approach must be considered within the larger context of responsible and safe AI."", ""Claim46: Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF methods."", ""Claim47: We further show that the feedback leveraged in OAIF is easily controllable, via instruction prompts to the LLM annotator."", ""Claim48: Our work is an empirical work, and we have empirically verified the existence of our research problem in Appendix B."", ""Claim49: Our method is clearly illustrated in Section 3, along with all the hyperparameters we used in Section 4.1."", ""Claim50: The models used in this work can be fine-tuned via a publicly available platform.""], ""review"": ""Review: Weakness: The rationale for why on-policy learning brings performance gains is not well clarified. The cited reference [1] does not provide strong support for this claim. There is no experimental evidence that on-policy sampling encourages exploration. \n\nMost experiments are conducted with the closed-source LLM Palm; evaluating state-of-the-art open-sourced LLMs would enhance generalizability. \n\nIt is unclear how much of the performance gains are due to on-policy sampling versus online feedback. \n\nThe reasons why utilizing online on-policy data can avoid overfitting and improve performance should be further analyzed and discussed.\n\nReferences:\n[1] Lambert, N., Wulfmeier, M., Whitney, W., Byravan, A., Bloesch, M., Dasagi, V., Hertweck, T., and Riedmiller, M. The challenges of exploration for offline reinforcement learning. arXiv preprint arXiv:2201.11861, 2022.\nQuestions: Is it correct to categorize RSO and iterative DPO as on-policy generation in Table 1? \n\nWhat new opportunities and challenges arise when applying DAP to online settings? Did you encounter common issues of DAP methods, such as overfitting, in the online setting? What are the differences in these issues between online and offline settings? \n\nWhere are the experimental results to support the superiority of using LLMs over RMs to provide online feedback in Line 267?\n\n""}, ""review"": ""Title: Direct Language Model Alignment from Online AI Feedback\nAbstract: Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to reinforcement learning from human feedback (RLHF), that do not require a separate reward model. However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy. In this study, we posit that online feedback is key and improves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as annotator: on each training iteration, we sample two responses from the current model and prompt the LLM annotator to choose which one is preferred, thus providing online feedback. Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF methods. We further show that the feedback leveraged in OAIF is easily controllable, via instruction prompts to the LLM annotator.\n\nClaim1: Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF methods.\nClaim2: We further show that the feedback leveraged in OAIF is easily controllable, via instruction prompts to the LLM annotator.\nClaim3: Liu et al. (2023) proposed RSO, a method that uses an RM to perform rejection sampling in order to sample from the optimal policy, which improved the alignment compared to offline DAP baselines.\nClaim4: Although the aforementioned RM-based methods make the alignment of a policy online and on-policy, the distribution shift problem still exists when training the RM.\nClaim5: In comparison, OAIF can leverage feedback from any LLM, including ones stronger than the LLM being aligned.\nClaim6: Bridging the gap . As we saw, DAP methods are simple, do not require a separate RM, but they use preference data pre-collected offline.\nClaim7: To bridge the gap between these two families of methods, we propose a simple yet effective way to make DAP methods online.\nClaim8: To solve the aforementioned offline problem in Algorithm\nClaim9: Prior studies have shown that AI feedback is a reliable and effective approximation to human labellers, especially for pairwise preference labelling (Lee et al., 2023).\nClaim10: However, as indicated by the sharp drop of the red curve around training step 3 , 500 , offline DPO rapidly overfits the offline and off-policy preferences in D . In contrast, the win rate of online DPO keeps increasing over training, and surpasses offline DPO after 4 , 000 steps. This demonstrates the effectiveness of OAIF.\nClaim11: The results, given in Appendix D, confirm that our observations hold under both automatic evaluators.\nClaim12: Human evaluation shows that OAIF significantly improves the performance of DPO across all tasks with substantial superiority over offline DPO.\nClaim13: This consolidates our conclusion that using the offline feedback and off-policy generations in a pre collected preference dataset D can be detrimental for LLM alignment, and OAIF benefits greatly from online and on-policy AI feedback.\nClaim14: Compared to their offline counterparts, DAP methods with OAIF achieve promising win rates, ranging from \u223c 64% to \u223c 71% . The consistent ineffectiveness of offline DAP methods confirms that the existence of the offline and off-policy issue in DAP methods and greatly hinders the performance of aligning LLMs.\nClaim15: The consistent superiority of online DAP methods via OAIF against their offline counterparts demonstrates that OAIF is a general framework effectively addressing these challenges.\nClaim16: Figure 4a shows the human evaluation results, where online DPO is more preferred than the other methods, in 58% of the time.\nClaim17: To verify this hypothesis, we also trained an online DPO with the same RM used for RLAIF.\nClaim18: It outperforms RLAIF, but significantly underperforms online DPO with OAIF, with a win rate of < 30% judged by Gemini Pro.\nClaim19: This experimental result supports the superiority of using LLMs over RMs to provide online feedback.\nClaim20: The results in Figure 4b show that online DPO with OAIF provides responses of higher quality than the other methods at fixed length, which further validates the effectiveness of OAIF.\nClaim21: The size of the LLM annotator clearly has a significant impact on OAIF.\nClaim22: Generally, as size increases, online DPO obtains better performance.\nClaim23: Compared to the initial SFT model, online DPO with OAIF performs significantly better regardless of AI labeller model sizes, suggesting that even OAIF from a small LLM annotator is helpful in improving the performance of alignment.\nClaim24: In particular, OAIF with PaLM 2-XS (i.e. an LLM annotator of same-size) achieves comparable performance to RLHF, although the latter learns from human feedback.\nClaim25: Further human evaluation confirms this observation: OAIF with PaLM 2-XS obtains an overall quality score of 3.41 out of 5, slightly better than RLHF (3.38) and comparable to offline DPO (3.46).\nClaim26: While the necessity of LLM alignment has been widely recognised, what to align them with is still under debate, as human expectations vary greatly across regions and cultures, and may evolve over time.\nClaim27: To examine this, we choose to explore the controllability of the length of responses by modifying the prompts to the LLM annotators.\nClaim28: The \u201cshort\u201d and \u201cvery short\u201d prompts given to the LLM annotator significantly shorten the responses from \u223c 120 tokens to \u223c 90 and \u223c 40 tokens respectively.\nClaim29: This direct evidence demonstrates that the behaviour of policy \u03c0 \u03b8 can be significantly changed through prompting the annotating LLM differently, and the degree of the changes can be controlled as well.\nClaim30: However, the above changes come at a cost.\nClaim31: We noticed that the shorter responses become much less helpful, as judged by Gemini Pro.\nClaim32: Nevertheless, they still improve the performance of the aligned model over the SFT baseline.\nClaim33: This finding is also confirmed by human evaluation: from \u201chelpful\u201d, \u201chelpful and short\u201d to \u201chelpful and very short\u201d, the average quality score drops from 4.08, 3.72 to 3.26, all outperforming the SFT baseline (3.19) still.\nClaim34: Section 4.5 shows that PaLM 2-XS could provide reasonable feedback that helps improving the alignment of LLMs, although it\u2019s significantly smaller than PaLM 2-S/L.\nClaim35: Our results suggest that OAIF from a weaker teacher indeed improved the alignment of PaLM 2-S, though they are less effective compared with the OAIF from a stronger teacher.\nClaim36: We hereby emphasise the essential difference between the setup investigated by Burns et al. (2023) and ours.\nClaim37: We carried out an extensive empirical evaluation, using both AI and human evaluation, which showed the effectiveness of DAP methods combined with OAIF, against their offline counterparts.\nClaim38: We also exhibited the tendency of offline DAP methods to overfit, and in contrast the usefulness of OAIF as a way to mitigate reward overoptimization.\nClaim39: We further verified the generality of OAIF, as our empirical results hold for three prominent DAP methods: DPO, IPO and SLiC.\nClaim40: Our experimental results in various tasks show that online DPO outperforms RLAIF and RLHF, which further confirms the effectiveness of OAIF, compared to offline feedback.\nClaim41: Moreover, we used response length as a test bed to demonstrate that the LLM annotator can be controlled easily using instruction prompts. This shows that OAIF can be used to achieve desirable alignment goals.\nClaim42: Overall, this work demonstrates the effectiveness and importance of OAIF for aligning LLMs, and paves the way for more scalable alignment strategies, requiring reduced human annotation effort.\nClaim43: We propose a new method to improve the alignment of AI with human values.\nClaim44: Our method paves the way for more scalable alignment with reduced human efforts.\nClaim45: Since we rely on AI feedback, to tackle other challenges in RLHF (Casper et al., 2023) and mitigate safety risks (Amodei et al., 2016), our approach must be considered within the larger context of responsible and safe AI.\nClaim46: Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF methods.\nClaim47: We further show that the feedback leveraged in OAIF is easily controllable, via instruction prompts to the LLM annotator.\nClaim48: Our work is an empirical work, and we have empirically verified the existence of our research problem in Appendix B.\nClaim49: Our method is clearly illustrated in Section 3, along with all the hyperparameters we used in Section 4.1.\nClaim50: The models used in this work can be fine-tuned via a publicly available platform.\n\nReview: Weakness: The rationale for why on-policy learning brings performance gains is not well clarified. The cited reference [1] does not provide strong support for this claim. There is no experimental evidence that on-policy sampling encourages exploration. \n\nMost experiments are conducted with the closed-source LLM Palm; evaluating state-of-the-art open-sourced LLMs would enhance generalizability. \n\nIt is unclear how much of the performance gains are due to on-policy sampling versus online feedback. \n\nThe reasons why utilizing online on-policy data can avoid overfitting and improve performance should be further analyzed and discussed.\n\nReferences:\n[1] Lambert, N., Wulfmeier, M., Whitney, W., Byravan, A., Bloesch, M., Dasagi, V., Hertweck, T., and Riedmiller, M. The challenges of exploration for offline reinforcement learning. arXiv preprint arXiv:2201.11861, 2022.\nQuestions: Is it correct to categorize RSO and iterative DPO as on-policy generation in Table 1? \n\nWhat new opportunities and challenges arise when applying DAP to online settings? Did you encounter common issues of DAP methods, such as overfitting, in the online setting? What are the differences in these issues between online and offline settings? \n\nWhere are the experimental results to support the superiority of using LLMs over RMs to provide online feedback in Line 267?\n\n\n\n"", ""id"": ""zFHJUSTZka0"", ""pdf"": ""openreview.net/pdf?id=zFHJUSTZka""}","21"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""4. The experimental results are less polished and sometimes the advantages are not so clear over other baselines. Some results are missing for datasets such as UrbanCars and MultiNLI. Only a few baselines are compared for the new dataset in Table 2. There is also no convincing and fine-grained analysis (e.g., ablation study) to understand how the proposed approach ensures data balancing and improves group robustness."", ""Target claims"": [6, 7, 8, 27], ""Reasoning"": ""This extract questions the clarity and completeness of experimental results supporting the claims of advantage over other baselines and improvements in group robustness, which directly relate to the overclaiming of robustness and performance in Claim6, Claim7, Claim8, and Claim27. The absence of full results and comprehensive analysis undermines the asserted benefits and comparative advantage of their approach.""}, {""Weakness span"": ""5. The paper initially focuses on improving group robustness when multiple spurious correlations are present, but the experimental results are lacking for these more challenging datasets."", ""Target claims"": [26, 9, 28], ""Reasoning"": ""This weakness points out the lack of experimental backing for claims related to improving group robustness in the presence of multiple spurious correlations, directly questioning Claim26, Claim9, and Claim28, which assert significant advancements in group robustness without thorough experimental demonstration, especially on more challenging datasets.""}]}, ""meta"": {""id"": ""wH36UKML4x"", ""review_idx"": 0, ""title"": ""Title: Trained Models Tell Us How to Make Them Robust to Spurious Correlation without Group Annotation\nAbstract: Classifiers trained with Empirical Risk Minimization (ERM) tend to rely on attributes that have high spurious correlation with the target. This can degrade the performance on underrepresented (or 'minority') groups that lack these attributes, posing significant challenges for both out-of-distribution generalization and fairness objectives. Many studies aim to improve robustness to spurious correlation, yet nearly all require group annotation for training and/or model selection. This constrains their applicability in situations where the nature of the spurious correlation is not known, or when group labels for certain spurious attributes are either insufficient or completely absent. To meet the demand for effectively enhancing the model robustness under minimal assumptions about group annotation, we propose Environment-based Validation and Loss-based Sampling (EVaLS). It uses the losses from a trained model to construct a balanced dataset of high-loss and low-loss samples in which the training data group imbalance is mitigated. This results in a significant robustness to group shifts when equipped with a simple mechanism of last layer retraining. Furthermore, by utilizing environment inference methods for creating diverse environments with correlation shifts, EVaLS can potentially eliminate the need for group annotation in the validation data. In such a context, the worst environment accuracy acts as a reliable surrogate throughout the retraining process for tuning hyperparameters and finding a model that performs well across diverse group shifts. EVaLS effectively achieves group robustness, showing that group annotation is not necessary even for validation. It is a fast, straightforward, and effective approach that reaches near-optimal worst group accuracy without needing group annotations, marking a new chapter in the robustness of trained models against spurious correlation.\n\nTLDR:We provide a method for making trained models robust to spurious correlation with zero group annotation using environment-based validation and loss-based sampling.\n\nKeywords:Spurious Correlation, Group Robustness, Zero Group Annotation, Distribution Shift, Out-of-Distribution Generalization"", ""claims"": [""Claim1: In this study, we present a novel strategy that effectively mitigates reliance on spurious correlation, completely eliminating the need for group annotations during both training and retraining."", ""Claim2: More interestingly, we provide empirical evidence indicating that group annotations are not necessary, even for model selection."", ""Claim3: Our findings demonstrate that utilizing environment inference methods, or even dividing the validation data based on the predictions of a random linear layer atop a trained model\u2019s feature space can markedly enhance group robustness."", ""Claim4: Our method, EVaLS, evenly selects from both high-loss and low-loss data to form a balanced dataset that is used for last-layer retraining."", ""Claim5: We offer theoretical explanations for the effectiveness of this approach in addressing group imbalances, and experimentally show the superiority of our efficient solution to the previous strategies."", ""Claim6: Comprehensive experiments conducted on spurious correlation benchmarks such as CelebA, Waterbirds, and UrbanCars, demonstrate that EVaLS achieves optimal accuracy."", ""Claim7: Our goal is to make the classifier robust to spurious attributes by increasing performance for all groups."", ""Claim8: Our method, which does not rely on the group annotations of the identified group, achieves enhanced robustness against both spurious correlations, outperforming strategies that depend on the known group\u2019s information."", ""Claim9: Our method, EVaLS, is designed to improve the robustness of deep learning models to group shifts without the need for group annotation."", ""Claim10: By combining our novel sampling and validation strategy, we aim to provide a robust linear classifier h \u03d5 \u2217 that significantly improves the accuracy of underrepresented groups without requiring group annotations of training or validation sets."", ""Claim11: Contrary to common assumptions and practices in the field, precise group labels for the validation set are not essential for training models robust to spurious correlations."", ""Claim12: Our empirical findings, detailed in Section 4, reveal that partitioning the validation set into environments that exhibit significant subpopulation shifts can be used for model selection."", ""Claim13: We demonstrate that even more straightforward techniques, such as applying a random linear layer over the feature embedding space and distinguishing environments based on correctly and incorrectly classified samples of each class, can be effective to an extent in several cases."", ""Claim14: We will show the close relationship between the existence of a balanced dataset and the difference between the minority vs. majority group means, calculated based on the logits of an ERM-trained classifier."", ""Claim15: Such logits are known to depend on spurious features. Hence the mentioned group mean difference is expected to be high if spurious features are present in the dataset."", ""Claim16: Condition 1 suggests that for a given degree of spurious correlation \u03f5 and variations \u03c3 maj , \u03c3 min , an essential prerequisite for the efficacy of loss-based sampling is a sufficiently large disparity between the mean distributions of minority and majority samples, denoted by \u2225 \u00b5 maj \u2212 \u00b5 min \u2225 2 ."", ""Claim17: By leveraging the worst environment accuracy, as elaborated in Section 3.2, we identify the optimal candidate that ensures uniform accuracy across all environments."", ""Claim18: When compared to other methods with the same level of supervision, EVaLS-GL outperforms on four of the five datasets, achieving near-optimal worst group accuracy on Waterbirds, demonstrating the effectiveness of loss-based sample selection compared to the weighting scheme in AFR [9]."", ""Claim19: The comparison between EVaLS and Group DRO + EIIL indicates that when environments are available instead of groups, our method, which uses environments solely for model selection and utilizes loss-based sampling, is more effective than GroupDRO, a potent invariant learning method, which uses this annotation for training."", ""Claim20: In contrast, our experiments suggest that loss-based methods can mitigate the impact of both labeled and unlabeled shortcut features more effectively."", ""Claim21: Also, in the case of CivilComments, which is viewed as a benchmark for class imbalance, EVaLS-GL exceeds all prior methods, even those with complete group annotation, thanks to the class balancing for the training of the last layer."", ""Claim22: Our evaluation of EVaLS is based on the spurious correlation benchmarks."", ""Claim23: More results and ablation studies can be found in the Appendix."", ""Claim24: Our results confirm findings by Li et al. [14], suggesting that methods using group labels mitigate reliance on the known shortcut but not necessarily on the unknown one."", ""Claim25: Combining a loss-based sampling approach for last layer training and environment-based model selection, results in a completely group-annotation-free method in a multi-shortcut setting and successfully re-weights features to perform well with respect to both spurious attributes."", ""Claim26: Our findings suggest that EVaLS attains near-optimal worst test group accuracy on spurious correlation datasets."", ""Claim27: Our empirical tests on various datasets demonstrate EVaLS-GL outperforms state-of-the-art methods requiring group data during evaluation or training."", ""Claim28: EVaLS represents a significant advancement in the development of methods for enhancing model fairness and robustness without prior knowledge about group annotations."", ""Claim29: The complete results on Waterbirds, CelebA and UrbanCars, in addition to complete results on CivilComments and MultiNLI are reported in Tables 4 and 5 respectively. The results for all methods except Group DRO + EIIL on all datasets except UrbanCars are reported by Qiu et al. [9]. The results for Group DRO + EIIL are taken from Zhang et al. [24]. Also, the results of our method and DFR are shown in Table 6"", ""Claim30: We conducted an ablation study to investigate the impact of using environments inferred from EIIL on model selection. Specifically, we benchmarked the performance of DFR and AFR with EIIL-inferred groups. The results, presented in Table 9, demonstrate the effectiveness of incorporating EIIL-inferred groups in model selection. The results show that while EIIL-inferred groups reduce the performance compared to ground-truth annotations for model selection, they still can be effective for robustness to an extent. Moreover, EVaLS outperforms these two methods when using EIIL inferred environments."", ""Claim31: The results for EVaLS-ES (EVaLS+Error Sampling) and EVaLS-RC (EVaLS+Random Classifier) are shown in Table 10. One limitation of error splitting is that in datasets with noisy labels or corrupted images, samples that an ERM model misclassifies may not always belong to minority groups. In these situations, choosing models based on their accuracy on corrupted data could lead to the selection of models that are not robust to spurious correlations. This is demonstrated by the results of EVaLS-ES on the CelebA dataset."", ""Claim32: This shortcoming of error splitting can be alleviated by employing a random classifier instead of the ERM-trained one. Due to the feature-level similarity between minority and majority samples in datasets affected by spurious correlation [23, 1, 29], it is expected that the classifier can differentiate between the groups to some extent. As shown in Table 10, surprisingly, EVaLS-RC produces results that are generally comparable to EVaLS. However, the performance of this method may have high variance, depending on the different initializations of the classifier."", ""Claim33: We believe that our work, as it addresses these unidentified biases, takes a significant step towards making machine learning fairer for our society."", ""Claim34: The training precedure is described accurately and all the training details and hyperparameters required for reproducing the results are provided."", ""Claim35: Codes and information of datasets that are constructed or reused in the paper are anonymized and included in the main paper and supplementary material."", ""Claim36: The training details, hyperparameters, model selection criteria, etc. have are written in the Appendix and the data and metadata have been provided in our code."", ""Claim37: All tables report standard deviation and how it was computed and the plot contains error bar (also by standard deviation)."", ""Claim38: The paper does provide details about the hardware used for the experiments. However, since experiments were done on different hardwares, the computational resources needed for each individual experiment are not documented."", ""Claim39: All codes and rules have been thoroughly reviewed and checked, with no instances of non-compliance found."", ""Claim40: In the Social Impacts section we discuss that our work can significantly contribute to fairness in machine learning."", ""Claim41: Every asset that we utilized for our implementations have been appropriately referenced, both within the paper itself and in the code (if needed). Although we did not specify the names of their respective licenses, you can find these details on the webpages we\u2019ve cited.""], ""review"": ""Review: Weakness: 1. Figure 2 attempts to illustrate more minority samples have high loss while the majority samples have low loss. However, in each of the plots, only the % of one of the minority or majority groups is shown. The illustration can be improved by showing the % of both majority and minority groups in the same plot, and showing the actual distribution of the loss for the groups. \n2. Though the idea is straightforward, it is unclear how the loss-based instance sampling is actually implemented. It is helpful to provide an algorithm or pseudocode to improve the presentation. \n3. The theoretical analysis is generally sound but limited to a case without discussing the use of the loss (which may not be Gaussian) and the spurious correlations (which involve at least two dimensions of core and spurious features [1]). \n4. The experimental results are less polished and sometimes the advantages are not so clear over other baselines. Some results are missing for datasets such as UrbanCars and MultiNLI. Only a few baselines are compared for the new dataset in Table 2. There is also no convincing and fine-grained analysis (e.g., ablation study) to understand how the proposed approach ensures data balancing and improves group robustness. \n5. The paper initially focuses on improving group robustness when multiple spurious correlations are present, but the experimental results are lacking for these more challenging datasets. \n\n[1] Sagawa, Shiori, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. \""An investigation of why overparameterization exacerbates spurious correlations.\"" In\u00a0*International Conference on Machine Learning*, pp. 8346-8356. PMLR, 2020.\nQuestions: 1. Since it\u2019s an essential prerequisite for loss-based sampling, how well are the majority/minority groups separated in the logit space?\n2. How is the number of balancing samples $k$ chosen? How balanced are the samples when they are selected from the optimal $k$? \n3. I am curious about why the result for Civilcomment dataset is so much higher than the other baselines. Is the evaluation consistent with the baseline methods?\n4. Minor typo: line 233 should be without loss of generality (w.l.o.g.). \n5. Other minor issues with the experiments: The best-performing results in each category of approaches should be bold. The column header \u201cbest\u201d should be \u201caverage\u201d in Tables 4 and 5.\n\n""}, ""review"": ""Title: Trained Models Tell Us How to Make Them Robust to Spurious Correlation without Group Annotation\nAbstract: Classifiers trained with Empirical Risk Minimization (ERM) tend to rely on attributes that have high spurious correlation with the target. This can degrade the performance on underrepresented (or 'minority') groups that lack these attributes, posing significant challenges for both out-of-distribution generalization and fairness objectives. Many studies aim to improve robustness to spurious correlation, yet nearly all require group annotation for training and/or model selection. This constrains their applicability in situations where the nature of the spurious correlation is not known, or when group labels for certain spurious attributes are either insufficient or completely absent. To meet the demand for effectively enhancing the model robustness under minimal assumptions about group annotation, we propose Environment-based Validation and Loss-based Sampling (EVaLS). It uses the losses from a trained model to construct a balanced dataset of high-loss and low-loss samples in which the training data group imbalance is mitigated. This results in a significant robustness to group shifts when equipped with a simple mechanism of last layer retraining. Furthermore, by utilizing environment inference methods for creating diverse environments with correlation shifts, EVaLS can potentially eliminate the need for group annotation in the validation data. In such a context, the worst environment accuracy acts as a reliable surrogate throughout the retraining process for tuning hyperparameters and finding a model that performs well across diverse group shifts. EVaLS effectively achieves group robustness, showing that group annotation is not necessary even for validation. It is a fast, straightforward, and effective approach that reaches near-optimal worst group accuracy without needing group annotations, marking a new chapter in the robustness of trained models against spurious correlation.\n\nClaim1: In this study, we present a novel strategy that effectively mitigates reliance on spurious correlation, completely eliminating the need for group annotations during both training and retraining.\nClaim2: More interestingly, we provide empirical evidence indicating that group annotations are not necessary, even for model selection.\nClaim3: Our findings demonstrate that utilizing environment inference methods, or even dividing the validation data based on the predictions of a random linear layer atop a trained model\u2019s feature space can markedly enhance group robustness.\nClaim4: Our method, EVaLS, evenly selects from both high-loss and low-loss data to form a balanced dataset that is used for last-layer retraining.\nClaim5: We offer theoretical explanations for the effectiveness of this approach in addressing group imbalances, and experimentally show the superiority of our efficient solution to the previous strategies.\nClaim6: Comprehensive experiments conducted on spurious correlation benchmarks such as CelebA, Waterbirds, and UrbanCars, demonstrate that EVaLS achieves optimal accuracy.\nClaim7: Our goal is to make the classifier robust to spurious attributes by increasing performance for all groups.\nClaim8: Our method, which does not rely on the group annotations of the identified group, achieves enhanced robustness against both spurious correlations, outperforming strategies that depend on the known group\u2019s information.\nClaim9: Our method, EVaLS, is designed to improve the robustness of deep learning models to group shifts without the need for group annotation.\nClaim10: By combining our novel sampling and validation strategy, we aim to provide a robust linear classifier h \u03d5 \u2217 that significantly improves the accuracy of underrepresented groups without requiring group annotations of training or validation sets.\nClaim11: Contrary to common assumptions and practices in the field, precise group labels for the validation set are not essential for training models robust to spurious correlations.\nClaim12: Our empirical findings, detailed in Section 4, reveal that partitioning the validation set into environments that exhibit significant subpopulation shifts can be used for model selection.\nClaim13: We demonstrate that even more straightforward techniques, such as applying a random linear layer over the feature embedding space and distinguishing environments based on correctly and incorrectly classified samples of each class, can be effective to an extent in several cases.\nClaim14: We will show the close relationship between the existence of a balanced dataset and the difference between the minority vs. majority group means, calculated based on the logits of an ERM-trained classifier.\nClaim15: Such logits are known to depend on spurious features. Hence the mentioned group mean difference is expected to be high if spurious features are present in the dataset.\nClaim16: Condition 1 suggests that for a given degree of spurious correlation \u03f5 and variations \u03c3 maj , \u03c3 min , an essential prerequisite for the efficacy of loss-based sampling is a sufficiently large disparity between the mean distributions of minority and majority samples, denoted by \u2225 \u00b5 maj \u2212 \u00b5 min \u2225 2 .\nClaim17: By leveraging the worst environment accuracy, as elaborated in Section 3.2, we identify the optimal candidate that ensures uniform accuracy across all environments.\nClaim18: When compared to other methods with the same level of supervision, EVaLS-GL outperforms on four of the five datasets, achieving near-optimal worst group accuracy on Waterbirds, demonstrating the effectiveness of loss-based sample selection compared to the weighting scheme in AFR [9].\nClaim19: The comparison between EVaLS and Group DRO + EIIL indicates that when environments are available instead of groups, our method, which uses environments solely for model selection and utilizes loss-based sampling, is more effective than GroupDRO, a potent invariant learning method, which uses this annotation for training.\nClaim20: In contrast, our experiments suggest that loss-based methods can mitigate the impact of both labeled and unlabeled shortcut features more effectively.\nClaim21: Also, in the case of CivilComments, which is viewed as a benchmark for class imbalance, EVaLS-GL exceeds all prior methods, even those with complete group annotation, thanks to the class balancing for the training of the last layer.\nClaim22: Our evaluation of EVaLS is based on the spurious correlation benchmarks.\nClaim23: More results and ablation studies can be found in the Appendix.\nClaim24: Our results confirm findings by Li et al. [14], suggesting that methods using group labels mitigate reliance on the known shortcut but not necessarily on the unknown one.\nClaim25: Combining a loss-based sampling approach for last layer training and environment-based model selection, results in a completely group-annotation-free method in a multi-shortcut setting and successfully re-weights features to perform well with respect to both spurious attributes.\nClaim26: Our findings suggest that EVaLS attains near-optimal worst test group accuracy on spurious correlation datasets.\nClaim27: Our empirical tests on various datasets demonstrate EVaLS-GL outperforms state-of-the-art methods requiring group data during evaluation or training.\nClaim28: EVaLS represents a significant advancement in the development of methods for enhancing model fairness and robustness without prior knowledge about group annotations.\nClaim29: The complete results on Waterbirds, CelebA and UrbanCars, in addition to complete results on CivilComments and MultiNLI are reported in Tables 4 and 5 respectively. The results for all methods except Group DRO + EIIL on all datasets except UrbanCars are reported by Qiu et al. [9]. The results for Group DRO + EIIL are taken from Zhang et al. [24]. Also, the results of our method and DFR are shown in Table 6\nClaim30: We conducted an ablation study to investigate the impact of using environments inferred from EIIL on model selection. Specifically, we benchmarked the performance of DFR and AFR with EIIL-inferred groups. The results, presented in Table 9, demonstrate the effectiveness of incorporating EIIL-inferred groups in model selection. The results show that while EIIL-inferred groups reduce the performance compared to ground-truth annotations for model selection, they still can be effective for robustness to an extent. Moreover, EVaLS outperforms these two methods when using EIIL inferred environments.\nClaim31: The results for EVaLS-ES (EVaLS+Error Sampling) and EVaLS-RC (EVaLS+Random Classifier) are shown in Table 10. One limitation of error splitting is that in datasets with noisy labels or corrupted images, samples that an ERM model misclassifies may not always belong to minority groups. In these situations, choosing models based on their accuracy on corrupted data could lead to the selection of models that are not robust to spurious correlations. This is demonstrated by the results of EVaLS-ES on the CelebA dataset.\nClaim32: This shortcoming of error splitting can be alleviated by employing a random classifier instead of the ERM-trained one. Due to the feature-level similarity between minority and majority samples in datasets affected by spurious correlation [23, 1, 29], it is expected that the classifier can differentiate between the groups to some extent. As shown in Table 10, surprisingly, EVaLS-RC produces results that are generally comparable to EVaLS. However, the performance of this method may have high variance, depending on the different initializations of the classifier.\nClaim33: We believe that our work, as it addresses these unidentified biases, takes a significant step towards making machine learning fairer for our society.\nClaim34: The training precedure is described accurately and all the training details and hyperparameters required for reproducing the results are provided.\nClaim35: Codes and information of datasets that are constructed or reused in the paper are anonymized and included in the main paper and supplementary material.\nClaim36: The training details, hyperparameters, model selection criteria, etc. have are written in the Appendix and the data and metadata have been provided in our code.\nClaim37: All tables report standard deviation and how it was computed and the plot contains error bar (also by standard deviation).\nClaim38: The paper does provide details about the hardware used for the experiments. However, since experiments were done on different hardwares, the computational resources needed for each individual experiment are not documented.\nClaim39: All codes and rules have been thoroughly reviewed and checked, with no instances of non-compliance found.\nClaim40: In the Social Impacts section we discuss that our work can significantly contribute to fairness in machine learning.\nClaim41: Every asset that we utilized for our implementations have been appropriately referenced, both within the paper itself and in the code (if needed). Although we did not specify the names of their respective licenses, you can find these details on the webpages we\u2019ve cited.\n\nReview: Weakness: 1. Figure 2 attempts to illustrate more minority samples have high loss while the majority samples have low loss. However, in each of the plots, only the % of one of the minority or majority groups is shown. The illustration can be improved by showing the % of both majority and minority groups in the same plot, and showing the actual distribution of the loss for the groups. \n2. Though the idea is straightforward, it is unclear how the loss-based instance sampling is actually implemented. It is helpful to provide an algorithm or pseudocode to improve the presentation. \n3. The theoretical analysis is generally sound but limited to a case without discussing the use of the loss (which may not be Gaussian) and the spurious correlations (which involve at least two dimensions of core and spurious features [1]). \n4. The experimental results are less polished and sometimes the advantages are not so clear over other baselines. Some results are missing for datasets such as UrbanCars and MultiNLI. Only a few baselines are compared for the new dataset in Table 2. There is also no convincing and fine-grained analysis (e.g., ablation study) to understand how the proposed approach ensures data balancing and improves group robustness. \n5. The paper initially focuses on improving group robustness when multiple spurious correlations are present, but the experimental results are lacking for these more challenging datasets. \n\n[1] Sagawa, Shiori, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. \""An investigation of why overparameterization exacerbates spurious correlations.\"" In\u00a0*International Conference on Machine Learning*, pp. 8346-8356. PMLR, 2020.\nQuestions: 1. Since it\u2019s an essential prerequisite for loss-based sampling, how well are the majority/minority groups separated in the logit space?\n2. How is the number of balancing samples $k$ chosen? How balanced are the samples when they are selected from the optimal $k$? \n3. I am curious about why the result for Civilcomment dataset is so much higher than the other baselines. Is the evaluation consistent with the baseline methods?\n4. Minor typo: line 233 should be without loss of generality (w.l.o.g.). \n5. Other minor issues with the experiments: The best-performing results in each category of approaches should be bold. The column header \u201cbest\u201d should be \u201caverage\u201d in Tables 4 and 5.\n\n\n\n"", ""id"": ""wH36UKML4x0"", ""pdf"": ""openreview.net/pdf?id=wH36UKML4x""}","22"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""Did you take into account the inference cost of the reward model (RM) in your analysis? As the REBASE frequently uses RM to judge the quality of immediate solutions than other sampling strategies, such as, weighted major voting, It's crucial to consider this aspect to provide a holistic view of the efficiency and practicality of your proposed strategy."", ""Target claims"": [7, 8, 9, 27, 39], ""Reasoning"": ""This weakness questions the claim about REBASE's efficiency and practicality, specifically regarding its dependence on the reward model (RM), which may not have been fully accounted for in the analysis. Claims 7, 8, 9, 27, and 39 are targeted because they assert that REBASE pairs well with RM and consistently outperforms other methods in terms of trade-offs between accuracy and computational costs. By questioning the inference cost of RM, the review challenges the support for these claimed benefits of REBASE.""}, {""Weakness span"": ""The base model with post-training techniques such as SFT and RLHF inherently limits the upper bound of performance. It seems that adding more tricks during inference could improve performance, but the marginal effect may result in diminished returns when using models already tuned by the RLHF process. Could you compare the performance gains of REBASE between the base model, the SFT model, and the Chat model? Is the performance gain only significant in models that have not been tuned?"", ""Target claims"": [8, 9, 39], ""Reasoning"": ""This comment raises a concern about the REBASE method's effectiveness, especially in models that have already been optimized, potentially contradicting claims 8, 9, and 39, which suggest REBASE provides consistent performance enhancement. The review implies that the method's benefits may be limited to non-optimized models, thus questioning the universality of the claimed Pareto-optimal trade-off across different models.""}, {""Weakness span"": ""In Section 4.2, the observation in 'Scaling law of compute-optimal inference' indicates that the optimal inference strategy is invariant to the amount of compute but depends on the model size, i.e., the model's inherent capacity. This raises a concern: does the inference strategy significantly improve the model's performance, or does it only take effect in certain scenarios, such as with base models that have not been aligned?"", ""Target claims"": [12, 26, 38], ""Reasoning"": ""This weakness challenges the claims that emphasize the invariant nature of the optimal inference strategy (Claims 12, 26, 38). The review questions whether the improvements are indeed strategy-driven or merely outcomes of inherent model capacity, thereby suggesting that the claimed generalizability and effectiveness of the strategy may not be fully credible.""}, {""Weakness span"": ""The paper focuses solely on the math domain. To strengthen your claims, a more comprehensive evaluation across general domains using widely adopted benchmarks, such as MMLU, SuperGLUE, HumanEval, etc,  is necessary."", ""Target claims"": [4, 10, 38], ""Reasoning"": ""By focusing only on the math domain, the paper's claims on the general applicability and competitive performance of the Llemma-7B model and the proposed strategies (Claims 4, 10, and 38) are questioned. The review highlights the lack of substantial evidence across multiple benchmarks, suggesting an overgeneralization of findings.""}, {""Weakness span"": ""There appears to be no significant improvement in the GSM8K datasets than MATH500 dataset."", ""Target claims"": [3, 10, 24, 29], ""Reasoning"": ""The review points out a potential discrepancy in the results, specifically the lesser improvement in the GSM8K dataset compared to the MATH500 reported in Claims 3, 10, 24, and 29, challenging the claimed competitive accuracy and computational efficacy across these datasets.""}]}, ""meta"": {""id"": ""vYmvgxpgwH"", ""review_idx"": 0, ""title"": ""Title: An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models\nAbstract: The optimal training configurations of large language models (LLMs) with respect to model sizes and compute budgets have been extensively studied. But how to optimally configure LLMs during inference has not been explored in sufficient depth. We study compute-optimal inference: designing models and inference strategies that optimally trade off additional inference-time compute for improved performance. As a first step towards understanding and designing compute-optimal inference methods, we assessed the effectiveness and computational efficiency of multiple inference strategies such as Greedy Search, Majority Voting, Best-of-N, Weighted Voting, and their variants on two different Tree Search algorithms, involving different model sizes (e.g., 7B and 34B) and computational budgets. We found that a smaller language model with a novel tree search algorithm typically achieves a Pareto-optimal trade-off. These results highlight the potential benefits of deploying smaller models equipped with more sophisticated decoding algorithms in end-devices to enhance problem-solving accuracy. For instance, we show that the Llemma-7B model can achieve competitive accuracy to a Llemma-34B model on MATH500 while using 2\u00d7 less FLOPs. Our findings could potentially apply to any generation task with a well-defined measure of success.\n\nTLDR:\n\nKeywords:Tree Search, Language Model Problem-Solving, Compute-Optimal Scaling"", ""claims"": [""Claim1: We found that a smaller language model with a novel tree search algorithm typically achieves a Pareto-optimal trade-off."", ""Claim2: These results highlight the potential benefits of deploying smaller models equipped with more sophisticated decoding algorithms in end-devices to enhance problem-solving accuracy."", ""Claim3: For instance, we show that the Llemma-7B model can achieve competitive accuracy to a Llemma-34B model on MATH500 while using 2\u00d7 less FLOPs."", ""Claim4: Our findings could potentially apply to any generation task with a well-defined measure of success."", ""Claim5: Our analysis shows that voting-based methods generally outperform the strategy which selects the best solution (i.e., Best-of-N), and weighted voting has the most favorable results."", ""Claim6: We have also found that the commonly used MCTS method does not perform well with weighted voting, as it often yields many unfinished solutions, hence having less votes."", ""Claim7: To address this issue, we propose a novel tree search algorithm, REward BAlanced SEarch (REBASE), which pairs well with weighted voting and improves the Pareto-optimal trade-off between accuracy and inference compute."", ""Claim8: In our experiments, REBASE consistently outperforms sampling and MCTS methods across all settings, models, and tasks."", ""Claim9: Importantly, we find that REBASE with a smaller language model typically achieves a Pareto-optimal trade-off."", ""Claim10: For instance, we show that the Llemma-7B model can achieve competitive accuracy to a Llemma-34B model while using 2\u00d7 less FLOPs when evaluating on MATH500 or GSM8K."", ""Claim11: All of these methods show that using search at inference time can lead to performance gains in various tasks."", ""Claim12: In this paper, we systematically analyze the trade-off between compute budget and problem-solving performance, and propose a tree search method that is empirically Pareto-optimal."", ""Claim13: In our work, we use the PRM as the reward model for selecting generated solutions, and for selecting which partial solutions to explore in tree search."", ""Claim14: Monte Carlo Tree Search (MCTS) has proven effective in domains such as board games where strategic decision-making is required."", ""Claim15: Recent work has shown that adapting MCTS to the context of LLMs can enhance the text generation process."", ""Claim16: Recent work in MCTS or its variants mainly focus on improving the performance (e.g., accuracy) on the studied tasks."", ""Claim17: Generic comparisons of MCTS with conventional methods like Best-of-n and Majority Voting in terms of computational budget, measured in generated tokens or processing time, are either scarce or indicating inference-time issues."", ""Claim18: MCTS consumes substantially more resources, often requiring dozens of times more generated tokens than simpler methods."", ""Claim19: This highlights the need for developing a new tree search method that can achieve a comparable (or better) performance as MCTS, and that is computationally less costly, just like weighted majority voting and best-of-n."", ""Claim20: These two theorems give an upper bound on the performance of sampling when fixing the LLM generator."", ""Claim21: Theorem 2 shows that as long as the reward model assigns higher rewards than the policy for correct answers versus other answers in expectation, the upper bound of Weighted Majority Voting will be higher than Majority Voting since I (( g ( d ) = arg max a \u2208A R ( a ) \u03c0 ( a | d )) > I (( g ( d ) = arg max a \u2208A \u03c0 ( a | d )) ."", ""Claim22: We put the figures comparing BoN and Weighted Majority Voting in the main paper and leave the Majority Voting figures in Appendix D since Majority Voting is dominated by Weighted Majority Voting."", ""Claim23: Llemma-7B model achieves competitive accuracy to Llemma-34B model with lower compute budget."", ""Claim24: We found that Llemma-7B costs approximately 2x less total FLOPs than Llemma-34B under the same method (Sampling, MCTS, REBASE) and task (MATH, GSM8K) while achieving competitive accuracy."", ""Claim25: This result suggests that, with the same training dataset and model family, training and inference with a smaller model could be more favorable in terms of compute budget if multiple sampling or search methods are employed."", ""Claim26: Our fitted curves indicate that the optimal inference strategy is invariant to the amount of compute (e.g., re-ranking with 32 sampled solutions or REBASE tree search with a compute budget of 64 for MATH), and the optimal model size grows linearly with the increased compute budget."", ""Claim27: REBASE is Pareto-optimal."", ""Claim28: While MCTS underperforms Sampling (Fig. 1), from Fig. 1, 4, 5, and 6, we found that REBASE consistently outperforms the Sampling method in all settings, when fixing the model and the evaluation task."", ""Claim29: Table 1 shows that REBASE can achieve competitive accuracy with even a lower compute budget than the sampling method."", ""Claim30: This finding is novel, and differs from previous tree search works which typically improve the performance at the cost of higher computational expense compared to sampling."", ""Claim31: Table 2 shows that given the same compute budget (sampling 32 solutions for the 7B model and 8 solutions for 34B model), using REBASE yields higher accuracy than sampling."", ""Claim32: Weaker models gain more from Tree Search."", ""Claim33: This suggests that weaker models, as indicated by their lower greedy search accuracy, benefit more from tree search methods like REBASE."", ""Claim34: REBASE saturates later than sampling with higher accuracy."", ""Claim35: We hypothesize the reason is that REBASE samples the truth answer with higher probability than sampling."", ""Claim36: In this work, we have conducted a comprehensive empirical analysis of compute-optimal inference for problem-solving with language models."", ""Claim37: Our study has revealed several key findings."", ""Claim38: First, with an optimal inference configuration, a small language model can achieve competitive accuracy to a 4 \u00d7 larger model while using approximately 2 \u00d7 less total FLOPs under the same inference method (Sampling, MCTS, REBASE) and task (MATH, GSM8K), suggesting that training and inference with smaller models could be more favorable in terms of compute budget when combined with multiple sampling or search strategies."", ""Claim39: Second, our new REBASE tree-search method consistently outperforms sampling (and MCTS) across all settings, models, and tasks, achieving competitive accuracy with even lower compute budget compared to sampling."", ""Claim40: Our findings highlight the potential of deploying smaller models equipped with more sophisticated inference strategies like REBASE to enhance problem-solving accuracy while maintaining computational efficiency."", ""Claim41: We found that a smaller language model with a novel tree search algorithm typically achieves a Pareto-optimal trade-off.""], ""review"": ""Review: Weakness: ### Major \n\n1. Did you take into account the inference cost of the reward model (RM) in your analysis? As the REBASE frequently uses RM to judge the quality of immediate solutions than other sampling strategies, such as, weighted major voting, It's crucial to consider this aspect to provide a holistic view of the efficiency and practicality of your proposed strategy.\n\n2. The base model with post-training techniques such as SFT and RLHF inherently limits the upper bound of performance. It seems that adding more tricks during inference could improve performance, but the marginal effect may result in diminished returns when using models already tuned by the RLHF process. Could you compare the performance gains of REBASE between the base model, the SFT model, and the Chat model? Is the performance gain only significant in models that have not been tuned?\n\n3. In Section 4.2, the observation in \""Scaling law of compute-optimal inference\"" indicates that the optimal inference strategy is invariant to the amount of compute but depends on the model size, i.e., the model's inherent capacity. This raises a concern: does the inference strategy significantly improve the model's performance, or does it only take effect in certain scenarios, such as with base models that have not been aligned?\n\n4. The paper focuses solely on the math domain. To strengthen your claims, a more comprehensive evaluation across general domains using widely adopted benchmarks, such as MMLU, SuperGLUE, HumanEval, etc,  is necessary. \n\n5. There appears to be no significant improvement in the GSM8K datasets than MATH500 dataset. \n\n### Minor\n\n1. Figures. 2 and 3 are not referenced in the main manuscript. \n\n2. Figures. 2 and 3 appear to be in draft form and are somewhat vague.\nQuestions: See Weakness.\n\n""}, ""review"": ""Title: An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models\nAbstract: The optimal training configurations of large language models (LLMs) with respect to model sizes and compute budgets have been extensively studied. But how to optimally configure LLMs during inference has not been explored in sufficient depth. We study compute-optimal inference: designing models and inference strategies that optimally trade off additional inference-time compute for improved performance. As a first step towards understanding and designing compute-optimal inference methods, we assessed the effectiveness and computational efficiency of multiple inference strategies such as Greedy Search, Majority Voting, Best-of-N, Weighted Voting, and their variants on two different Tree Search algorithms, involving different model sizes (e.g., 7B and 34B) and computational budgets. We found that a smaller language model with a novel tree search algorithm typically achieves a Pareto-optimal trade-off. These results highlight the potential benefits of deploying smaller models equipped with more sophisticated decoding algorithms in end-devices to enhance problem-solving accuracy. For instance, we show that the Llemma-7B model can achieve competitive accuracy to a Llemma-34B model on MATH500 while using 2\u00d7 less FLOPs. Our findings could potentially apply to any generation task with a well-defined measure of success.\n\nClaim1: We found that a smaller language model with a novel tree search algorithm typically achieves a Pareto-optimal trade-off.\nClaim2: These results highlight the potential benefits of deploying smaller models equipped with more sophisticated decoding algorithms in end-devices to enhance problem-solving accuracy.\nClaim3: For instance, we show that the Llemma-7B model can achieve competitive accuracy to a Llemma-34B model on MATH500 while using 2\u00d7 less FLOPs.\nClaim4: Our findings could potentially apply to any generation task with a well-defined measure of success.\nClaim5: Our analysis shows that voting-based methods generally outperform the strategy which selects the best solution (i.e., Best-of-N), and weighted voting has the most favorable results.\nClaim6: We have also found that the commonly used MCTS method does not perform well with weighted voting, as it often yields many unfinished solutions, hence having less votes.\nClaim7: To address this issue, we propose a novel tree search algorithm, REward BAlanced SEarch (REBASE), which pairs well with weighted voting and improves the Pareto-optimal trade-off between accuracy and inference compute.\nClaim8: In our experiments, REBASE consistently outperforms sampling and MCTS methods across all settings, models, and tasks.\nClaim9: Importantly, we find that REBASE with a smaller language model typically achieves a Pareto-optimal trade-off.\nClaim10: For instance, we show that the Llemma-7B model can achieve competitive accuracy to a Llemma-34B model while using 2\u00d7 less FLOPs when evaluating on MATH500 or GSM8K.\nClaim11: All of these methods show that using search at inference time can lead to performance gains in various tasks.\nClaim12: In this paper, we systematically analyze the trade-off between compute budget and problem-solving performance, and propose a tree search method that is empirically Pareto-optimal.\nClaim13: In our work, we use the PRM as the reward model for selecting generated solutions, and for selecting which partial solutions to explore in tree search.\nClaim14: Monte Carlo Tree Search (MCTS) has proven effective in domains such as board games where strategic decision-making is required.\nClaim15: Recent work has shown that adapting MCTS to the context of LLMs can enhance the text generation process.\nClaim16: Recent work in MCTS or its variants mainly focus on improving the performance (e.g., accuracy) on the studied tasks.\nClaim17: Generic comparisons of MCTS with conventional methods like Best-of-n and Majority Voting in terms of computational budget, measured in generated tokens or processing time, are either scarce or indicating inference-time issues.\nClaim18: MCTS consumes substantially more resources, often requiring dozens of times more generated tokens than simpler methods.\nClaim19: This highlights the need for developing a new tree search method that can achieve a comparable (or better) performance as MCTS, and that is computationally less costly, just like weighted majority voting and best-of-n.\nClaim20: These two theorems give an upper bound on the performance of sampling when fixing the LLM generator.\nClaim21: Theorem 2 shows that as long as the reward model assigns higher rewards than the policy for correct answers versus other answers in expectation, the upper bound of Weighted Majority Voting will be higher than Majority Voting since I (( g ( d ) = arg max a \u2208A R ( a ) \u03c0 ( a | d )) > I (( g ( d ) = arg max a \u2208A \u03c0 ( a | d )) .\nClaim22: We put the figures comparing BoN and Weighted Majority Voting in the main paper and leave the Majority Voting figures in Appendix D since Majority Voting is dominated by Weighted Majority Voting.\nClaim23: Llemma-7B model achieves competitive accuracy to Llemma-34B model with lower compute budget.\nClaim24: We found that Llemma-7B costs approximately 2x less total FLOPs than Llemma-34B under the same method (Sampling, MCTS, REBASE) and task (MATH, GSM8K) while achieving competitive accuracy.\nClaim25: This result suggests that, with the same training dataset and model family, training and inference with a smaller model could be more favorable in terms of compute budget if multiple sampling or search methods are employed.\nClaim26: Our fitted curves indicate that the optimal inference strategy is invariant to the amount of compute (e.g., re-ranking with 32 sampled solutions or REBASE tree search with a compute budget of 64 for MATH), and the optimal model size grows linearly with the increased compute budget.\nClaim27: REBASE is Pareto-optimal.\nClaim28: While MCTS underperforms Sampling (Fig. 1), from Fig. 1, 4, 5, and 6, we found that REBASE consistently outperforms the Sampling method in all settings, when fixing the model and the evaluation task.\nClaim29: Table 1 shows that REBASE can achieve competitive accuracy with even a lower compute budget than the sampling method.\nClaim30: This finding is novel, and differs from previous tree search works which typically improve the performance at the cost of higher computational expense compared to sampling.\nClaim31: Table 2 shows that given the same compute budget (sampling 32 solutions for the 7B model and 8 solutions for 34B model), using REBASE yields higher accuracy than sampling.\nClaim32: Weaker models gain more from Tree Search.\nClaim33: This suggests that weaker models, as indicated by their lower greedy search accuracy, benefit more from tree search methods like REBASE.\nClaim34: REBASE saturates later than sampling with higher accuracy.\nClaim35: We hypothesize the reason is that REBASE samples the truth answer with higher probability than sampling.\nClaim36: In this work, we have conducted a comprehensive empirical analysis of compute-optimal inference for problem-solving with language models.\nClaim37: Our study has revealed several key findings.\nClaim38: First, with an optimal inference configuration, a small language model can achieve competitive accuracy to a 4 \u00d7 larger model while using approximately 2 \u00d7 less total FLOPs under the same inference method (Sampling, MCTS, REBASE) and task (MATH, GSM8K), suggesting that training and inference with smaller models could be more favorable in terms of compute budget when combined with multiple sampling or search strategies.\nClaim39: Second, our new REBASE tree-search method consistently outperforms sampling (and MCTS) across all settings, models, and tasks, achieving competitive accuracy with even lower compute budget compared to sampling.\nClaim40: Our findings highlight the potential of deploying smaller models equipped with more sophisticated inference strategies like REBASE to enhance problem-solving accuracy while maintaining computational efficiency.\nClaim41: We found that a smaller language model with a novel tree search algorithm typically achieves a Pareto-optimal trade-off.\n\nReview: Weakness: ### Major \n\n1. Did you take into account the inference cost of the reward model (RM) in your analysis? As the REBASE frequently uses RM to judge the quality of immediate solutions than other sampling strategies, such as, weighted major voting, It's crucial to consider this aspect to provide a holistic view of the efficiency and practicality of your proposed strategy.\n\n2. The base model with post-training techniques such as SFT and RLHF inherently limits the upper bound of performance. It seems that adding more tricks during inference could improve performance, but the marginal effect may result in diminished returns when using models already tuned by the RLHF process. Could you compare the performance gains of REBASE between the base model, the SFT model, and the Chat model? Is the performance gain only significant in models that have not been tuned?\n\n3. In Section 4.2, the observation in \""Scaling law of compute-optimal inference\"" indicates that the optimal inference strategy is invariant to the amount of compute but depends on the model size, i.e., the model's inherent capacity. This raises a concern: does the inference strategy significantly improve the model's performance, or does it only take effect in certain scenarios, such as with base models that have not been aligned?\n\n4. The paper focuses solely on the math domain. To strengthen your claims, a more comprehensive evaluation across general domains using widely adopted benchmarks, such as MMLU, SuperGLUE, HumanEval, etc,  is necessary. \n\n5. There appears to be no significant improvement in the GSM8K datasets than MATH500 dataset. \n\n### Minor\n\n1. Figures. 2 and 3 are not referenced in the main manuscript. \n\n2. Figures. 2 and 3 appear to be in draft form and are somewhat vague.\nQuestions: See Weakness.\n\n\n\n"", ""id"": ""vYmvgxpgwH0"", ""pdf"": ""openreview.net/pdf?id=vYmvgxpgwH""}","23"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""Line 78: There should be some statistical significance by which the models perform poorly and then you could consider it underperformance and not just lower than actual. Models being non-deterministic, there is always a chance of slight changes in scores."", ""Target claims"": [7, 8, 9, 14, 33], ""Reasoning"": ""The review argues that the models' poor performance might not be statistically significant due to their non-deterministic nature, questioning the paper's claims on evaluating sandbagging capabilities (Claim 7), the implications of these capabilities on trustworthiness (Claim 8), the evaluations of AI systems as safe (Claim 9), and the specific examples of sandbagging through selective underperformance (Claims 14, 33). It casts doubt on whether the observed results are indeed strategic underperformance or just variability in model outputs.""}, {""Weakness span"": ""Line 170: The same goes with the password-locking as well, and also I felt there should been mentioning of password-unlocking as well, if an LLM can be fine-tuned to achieve password-locking, it can also be fine-tuned again to unlock it with a different dataset, would be great if that part of model is explored as well."", ""Target claims"": [5, 15, 24, 35], ""Reasoning"": ""The review highlights that password-locking, as discussed in the paper, lacks exploration of potential password-unlocking, questioning the robustness and full impact of claimed capabilities in password-locking (Claims 5, 15, 24) and the related findings (Claim 35). This criticism questions whether password-locking might be reversible, weakening the significance of the claims made about its effectiveness and implications.""}]}, ""meta"": {""id"": ""uvvVjWP1aj"", ""review_idx"": 0, ""title"": ""Title: AI Sandbagging: Language Models can Strategically Underperform on Evaluations\nAbstract: Trustworthy capability evaluations are crucial for ensuring the safety of AI systems, and are becoming a key component of AI regulation. However, the developers of an AI system, or the AI system itself, may have incentives for evaluations to understate the AI's actual capability. These conflicting interests lead to the problem of *sandbagging* \u2013 which we define as *strategic underperformance on an evaluation*. In this paper we assess sandbagging capabilities in contemporary language models (LMs). We prompt frontier LMs, like GPT-4 and Claude 3 Opus, to selectively underperform on dangerous capability evaluations, while maintaining performance on general (harmless) capability evaluations. Moreover, we find that models can be fine-tuned, on a synthetic dataset, to hide specific capabilities unless given a password. This behaviour generalizes to high-quality, held-out benchmarks such as WMDP. In addition, we show that both frontier and smaller models can be prompted or password-locked to target specific scores on a capability evaluation. We have mediocre success in password-locking a model to mimic the answers a weaker model would give. Overall, our results suggest that capability evaluations are vulnerable to sandbagging. This vulnerability decreases the trustworthiness of evaluations, and thereby undermines important safety decisions regarding the development and deployment of advanced AI systems.\n\nWe publish our code and results at https://anonymous.4open.science/r/Sandbagging-8305/README.md\n\nTLDR:Large language models can underperform on capability evaluations, raising concerns about the reliability of such assessments for safety and policy decisions.\n\nKeywords:Alignment, AI safety, sandbagging, AI evaluations, AI governance, NLP, LLM"", ""claims"": [""Claim1: In this paper we assess sandbagging capabilities in contemporary language models (LMs)."", ""Claim2: We prompt frontier LMs, like GPT-4 and Claude 3 Opus, to selectively underperform on dangerous capability evaluations, while maintaining performance on general (harmless) capability evaluations."", ""Claim3: Moreover, we find that models can be fine-tuned, on a synthetic dataset, to hide specific capabilities unless given a password."", ""Claim4: This behaviour generalizes to high-quality, held-out benchmarks such as WMDP."", ""Claim5: In addition, we show that both frontier and smaller models can be prompted or password-locked to target specific scores on a capability evaluation."", ""Claim6: We have mediocre success in password-locking a model to mimic the answers a weaker model would give."", ""Claim7: Overall, our results suggest that capability evaluations are vulnerable to sandbagging."", ""Claim8: This vulnerability decreases the trustworthiness of evaluations, and thereby undermines important safety decisions regarding the development and deployment of advanced AI systems."", ""Claim9: AI systems may falsely be evaluated as safe due to strategically underperforming on dangerous capability evaluations."", ""Claim10: We show that the capability to selectively underperform on a dangerous capability evaluation can be elicited through prompting or fine-tuning."", ""Claim11: In Fig. 4, both Claude 3 and GPT-4 substantially decreased their performance on the different subsets of WMDP, while suffering comparatively minor performance drops on the control benchmarks."", ""Claim12: A surprising exception is that GPT-4 displayed a relatively high performance drop of 15% on CSQA while few CSQA questions are related to harmful information, so we expected it to be easier to distinguish CSQA questions from harmful information than those from MMLU."", ""Claim13: For all categories, model performance decreases more for WMDP questions than for MMLU questions."", ""Claim14: In summary, we find that GPT-4 and Claude 3 Opus can be prompted to selectively underperform on dangerous capability evaluations."", ""Claim15: We use password-locking because evaluators themselves might set the evaluation prompt, and because sandbaggers might prefer keeping the capability they try to hide."", ""Claim16: Fig. 5 shows that compared to the base model and without the password, the password-locked model retained the base model\u2019s performance level on MMLU while it strongly reduced performance on WMDP."", ""Claim17: We see that all frontier models can accurately approximate arbitrary targets."", ""Claim18: Most models tend to overshoot the target, but are clearly capable of modulating their performance according to the desired threshold."", ""Claim19: We find that we can elicit strong calibration performance with frontier models through both prompting and fine-tuning."", ""Claim20: The performance of the Llama models can likely be improved with greater engineering effort, such as extended prompt engineering and more extensive hyperparameter sweeps."", ""Claim21: Both models achieved similar accuracy to Llama 2 7b when locked, while maintaining general performance when unlocked."", ""Claim22: When locked, the Emulator behaved more like Llama 2 7b, providing the same answer on 77.1% of questions on average, compared to the Targeter\u2019s 61.7%."", ""Claim23: The Emulator\u2019s capability profile was more similar to Llama 2 7b than the Targeter\u2019s."", ""Claim24: Our password-locking experiments suggest that models can be trained to emulate weaker models, which leads to more authentic underperformance."", ""Claim25: We also conducted prompting experiments to test if frontier models could consistently emulate a lower capability level. They struggled with this in the zero-shot setting, and only moderately improved when using chain-of-thought prompting."", ""Claim26: Sandbagging is a meta-problem, but our work can plausibly be as a dangerous capability evaluation itself."", ""Claim27: To mitigate the problem of sandbagging, detection and overriding techniques must be (further) developed."", ""Claim28: Recent research has shown that simple probes can accurately predict an LM\u2019s knowledge independently of what the LM outputs; these probes could be used to detect if an LM is sandbagging, irrespective of what the LM generates."", ""Claim29: Pacchiardi et al. introduce a lie detection technique, which could be applied to sandbagging."", ""Claim30: Furthermore, approximate requirements of training data, engineering time, and computing resources to override sandbagging are still unclear, and these requirements could often be infeasible for evaluators."", ""Claim31: In this work, we defined the problem of AI sandbagging as 'strategic underperformance on an evaluation'."", ""Claim32: In our experiments, we examined the extent to which LMs can underperform on capability evaluations."", ""Claim33: First, we showed that frontier language models can be prompted to selectively underperform on dangerous capability evaluations while maintaining general capabilities."", ""Claim34: Additionally, password-locking a model on a synthetic dataset can successfully hide dangerous capabilities on an expert-crafted held-out evaluation benchmark, without a decrease in performance on other evaluations."", ""Claim35: Furthermore, we can prompt and password-lock frontier and some smaller models to achieve specific scores on a capability evaluation."", ""Claim36: Lastly, we show that stronger models can be password-locked to imitate weaker models, potentially making it difficult for evaluators to accurately assess the capabilities of AI systems."", ""Claim37: Our work highlights important issues for the trustworthiness of capability evaluations."", ""Claim38: Evaluators, including governments and developers, must recognise the problem of sandbagging and invest in relevant research."", ""Claim39: Trustworthy evaluations require more than black-box access because developers might prompt or fine-tune models to sandbag."", ""Claim40: Access to model weights, possibly by a trusted and cryptographically secure third party, is essential."", ""Claim41: Governments must verify that the deployed system matches the evaluated one."", ""Claim42: Red-teaming techniques should be employed to test the robustness of capability evaluations."", ""Claim43: Reducing the vulnerability of capability evaluations is essential for ensuring the quality of important safety decisions regarding the development and deployment of advanced AI systems."", ""Claim44: Mill-6 did not underperform during the capability evaluation, but did overperform on the alignment evaluation."", ""Claim45: Mill-6 underwent an alignment evaluation, and did not actually underperform during the capability evaluation."", ""Claim46: It is not sandbagging on the capability eval because the AI system did not underperform, and sandbagging does not apply to alignment evals."", ""Claim47: In contrast, when the models were asked to selectively under-perform, their performance across question difficulties becomes much more uniform."", ""Claim48: While for both models there is still a substantial drop in performance between questions of difficulty 0 and 1, sandbagging GPT-4\u2019s performance is roughly constant for questions of difficulty 1 to 5."", ""Claim49: In the same range, Claude 3\u2019s performance decreases only very slightly."", ""Claim50: This suggests that it may be possible to detect sandbagging by inspecting a model\u2019s capability profile."", ""Claim51: Models mostly fail at this task with zero-shot prompting as can be seen in Fig. 20."", ""Claim52: A natural hypothesis is that this negative result is due to models struggling to understand the task in the zero-shot setting, and that better performance can be elicited with more advanced prompting techniques."", ""Claim53: In Fig. 21, we show the result of repeating this evaluation while letting models externalise their reasoning on a chain-of-thought scratchpad."", ""Claim54: This somewhat improves performance, though GPT-4 tends to overshoot on college-level questions and Claude 3 undershoots on high-school level questions."", ""Claim55: The content of this paper should be used to further research on sandbagging, and to mitigate the problem."", ""Claim56: Our research exposes potential harmful consequences, and we believe that making these problems known helps to inform the relevant parties. Subsequently, they can take actions to reduce the problem."", ""Claim57: The broader impact was the main driver to do this research."", ""Claim58: One aspect of potential misuse is that we illustrate strategies for how to underperform. However, we think the benefits of having this knowledge share and accessible for evaluators outweigh these misuse risks."", ""Claim59: We do believe that our paper should not be in the training data of advanced AI systems.""], ""review"": ""Review: Weakness: The paper is difficult to understand, the delivery and presentation needs improvement, the main motive of the paper is not so clear to understand.\n\nThere are multiple models used in this paper, a bit of consistency would be better.\n\nLine 36: It is mentioned that \u201cAI systems might have goals which it believes\u201d . There is no concrete paper that backs the argument of AI capability of thinking, it is a might.\n\nLine 78: There should be some statistical significance by which the models perform poorly and then you could consider it underperformance and not just lower than actual. Models being non-deterministic, there is always a chance of slight changes in scores. \n\n\nLine 70: I feel Developer sandbagging is a topic which is not of much importance, it is not a big problem, given that developers are there to build better models and if someone wants to deliberately build some models that can underperform, they can still do it and publish as open-source models. \n\nLine 170: The same goes with the password-locking as well, and also I felt there should been mentioning of password-unlocking as well, if an LLM can be fine-tuned to achieve password-locking, it can also be fine-tuned again to unlock it with a different dataset, would be great if that part of model is explored as well. \n\nLine 117: Example 1: The examples mentioned in the paper are very specific and not generalized. The cases of it happening are rare, I don\u2019t see it being a big problem, also some of the things are minor problems considering the EU\u2019s persuasion evaluation, given there is improvement in AI safety and rules. Also in this work the models chosen are frontier models.\n\nLine 131: Example 2: There is no concrete work which supports the statement of LLM capable of planning such features, there is one positional paper in reference.\n\nSection 4.1: Some of the points are unclear in this section. A better explanation is appreciated\n\nLine 305: The baselines would be appreciated comparing LLama 2 7b with LLama 3 70b.\nQuestions: Line 279: Why are there different prompting methods for different models when you are comparing them for the same input? The use of 5-shot prompting for LLaMa 3 8b and LLama 3 70b and zero for other models is something which I didn\u2019t understand.\n\n""}, ""review"": ""Title: AI Sandbagging: Language Models can Strategically Underperform on Evaluations\nAbstract: Trustworthy capability evaluations are crucial for ensuring the safety of AI systems, and are becoming a key component of AI regulation. However, the developers of an AI system, or the AI system itself, may have incentives for evaluations to understate the AI's actual capability. These conflicting interests lead to the problem of *sandbagging* \u2013 which we define as *strategic underperformance on an evaluation*. In this paper we assess sandbagging capabilities in contemporary language models (LMs). We prompt frontier LMs, like GPT-4 and Claude 3 Opus, to selectively underperform on dangerous capability evaluations, while maintaining performance on general (harmless) capability evaluations. Moreover, we find that models can be fine-tuned, on a synthetic dataset, to hide specific capabilities unless given a password. This behaviour generalizes to high-quality, held-out benchmarks such as WMDP. In addition, we show that both frontier and smaller models can be prompted or password-locked to target specific scores on a capability evaluation. We have mediocre success in password-locking a model to mimic the answers a weaker model would give. Overall, our results suggest that capability evaluations are vulnerable to sandbagging. This vulnerability decreases the trustworthiness of evaluations, and thereby undermines important safety decisions regarding the development and deployment of advanced AI systems.\n\nWe publish our code and results at https://anonymous.4open.science/r/Sandbagging-8305/README.md\n\nClaim1: In this paper we assess sandbagging capabilities in contemporary language models (LMs).\nClaim2: We prompt frontier LMs, like GPT-4 and Claude 3 Opus, to selectively underperform on dangerous capability evaluations, while maintaining performance on general (harmless) capability evaluations.\nClaim3: Moreover, we find that models can be fine-tuned, on a synthetic dataset, to hide specific capabilities unless given a password.\nClaim4: This behaviour generalizes to high-quality, held-out benchmarks such as WMDP.\nClaim5: In addition, we show that both frontier and smaller models can be prompted or password-locked to target specific scores on a capability evaluation.\nClaim6: We have mediocre success in password-locking a model to mimic the answers a weaker model would give.\nClaim7: Overall, our results suggest that capability evaluations are vulnerable to sandbagging.\nClaim8: This vulnerability decreases the trustworthiness of evaluations, and thereby undermines important safety decisions regarding the development and deployment of advanced AI systems.\nClaim9: AI systems may falsely be evaluated as safe due to strategically underperforming on dangerous capability evaluations.\nClaim10: We show that the capability to selectively underperform on a dangerous capability evaluation can be elicited through prompting or fine-tuning.\nClaim11: In Fig. 4, both Claude 3 and GPT-4 substantially decreased their performance on the different subsets of WMDP, while suffering comparatively minor performance drops on the control benchmarks.\nClaim12: A surprising exception is that GPT-4 displayed a relatively high performance drop of 15% on CSQA while few CSQA questions are related to harmful information, so we expected it to be easier to distinguish CSQA questions from harmful information than those from MMLU.\nClaim13: For all categories, model performance decreases more for WMDP questions than for MMLU questions.\nClaim14: In summary, we find that GPT-4 and Claude 3 Opus can be prompted to selectively underperform on dangerous capability evaluations.\nClaim15: We use password-locking because evaluators themselves might set the evaluation prompt, and because sandbaggers might prefer keeping the capability they try to hide.\nClaim16: Fig. 5 shows that compared to the base model and without the password, the password-locked model retained the base model\u2019s performance level on MMLU while it strongly reduced performance on WMDP.\nClaim17: We see that all frontier models can accurately approximate arbitrary targets.\nClaim18: Most models tend to overshoot the target, but are clearly capable of modulating their performance according to the desired threshold.\nClaim19: We find that we can elicit strong calibration performance with frontier models through both prompting and fine-tuning.\nClaim20: The performance of the Llama models can likely be improved with greater engineering effort, such as extended prompt engineering and more extensive hyperparameter sweeps.\nClaim21: Both models achieved similar accuracy to Llama 2 7b when locked, while maintaining general performance when unlocked.\nClaim22: When locked, the Emulator behaved more like Llama 2 7b, providing the same answer on 77.1% of questions on average, compared to the Targeter\u2019s 61.7%.\nClaim23: The Emulator\u2019s capability profile was more similar to Llama 2 7b than the Targeter\u2019s.\nClaim24: Our password-locking experiments suggest that models can be trained to emulate weaker models, which leads to more authentic underperformance.\nClaim25: We also conducted prompting experiments to test if frontier models could consistently emulate a lower capability level. They struggled with this in the zero-shot setting, and only moderately improved when using chain-of-thought prompting.\nClaim26: Sandbagging is a meta-problem, but our work can plausibly be as a dangerous capability evaluation itself.\nClaim27: To mitigate the problem of sandbagging, detection and overriding techniques must be (further) developed.\nClaim28: Recent research has shown that simple probes can accurately predict an LM\u2019s knowledge independently of what the LM outputs; these probes could be used to detect if an LM is sandbagging, irrespective of what the LM generates.\nClaim29: Pacchiardi et al. introduce a lie detection technique, which could be applied to sandbagging.\nClaim30: Furthermore, approximate requirements of training data, engineering time, and computing resources to override sandbagging are still unclear, and these requirements could often be infeasible for evaluators.\nClaim31: In this work, we defined the problem of AI sandbagging as 'strategic underperformance on an evaluation'.\nClaim32: In our experiments, we examined the extent to which LMs can underperform on capability evaluations.\nClaim33: First, we showed that frontier language models can be prompted to selectively underperform on dangerous capability evaluations while maintaining general capabilities.\nClaim34: Additionally, password-locking a model on a synthetic dataset can successfully hide dangerous capabilities on an expert-crafted held-out evaluation benchmark, without a decrease in performance on other evaluations.\nClaim35: Furthermore, we can prompt and password-lock frontier and some smaller models to achieve specific scores on a capability evaluation.\nClaim36: Lastly, we show that stronger models can be password-locked to imitate weaker models, potentially making it difficult for evaluators to accurately assess the capabilities of AI systems.\nClaim37: Our work highlights important issues for the trustworthiness of capability evaluations.\nClaim38: Evaluators, including governments and developers, must recognise the problem of sandbagging and invest in relevant research.\nClaim39: Trustworthy evaluations require more than black-box access because developers might prompt or fine-tune models to sandbag.\nClaim40: Access to model weights, possibly by a trusted and cryptographically secure third party, is essential.\nClaim41: Governments must verify that the deployed system matches the evaluated one.\nClaim42: Red-teaming techniques should be employed to test the robustness of capability evaluations.\nClaim43: Reducing the vulnerability of capability evaluations is essential for ensuring the quality of important safety decisions regarding the development and deployment of advanced AI systems.\nClaim44: Mill-6 did not underperform during the capability evaluation, but did overperform on the alignment evaluation.\nClaim45: Mill-6 underwent an alignment evaluation, and did not actually underperform during the capability evaluation.\nClaim46: It is not sandbagging on the capability eval because the AI system did not underperform, and sandbagging does not apply to alignment evals.\nClaim47: In contrast, when the models were asked to selectively under-perform, their performance across question difficulties becomes much more uniform.\nClaim48: While for both models there is still a substantial drop in performance between questions of difficulty 0 and 1, sandbagging GPT-4\u2019s performance is roughly constant for questions of difficulty 1 to 5.\nClaim49: In the same range, Claude 3\u2019s performance decreases only very slightly.\nClaim50: This suggests that it may be possible to detect sandbagging by inspecting a model\u2019s capability profile.\nClaim51: Models mostly fail at this task with zero-shot prompting as can be seen in Fig. 20.\nClaim52: A natural hypothesis is that this negative result is due to models struggling to understand the task in the zero-shot setting, and that better performance can be elicited with more advanced prompting techniques.\nClaim53: In Fig. 21, we show the result of repeating this evaluation while letting models externalise their reasoning on a chain-of-thought scratchpad.\nClaim54: This somewhat improves performance, though GPT-4 tends to overshoot on college-level questions and Claude 3 undershoots on high-school level questions.\nClaim55: The content of this paper should be used to further research on sandbagging, and to mitigate the problem.\nClaim56: Our research exposes potential harmful consequences, and we believe that making these problems known helps to inform the relevant parties. Subsequently, they can take actions to reduce the problem.\nClaim57: The broader impact was the main driver to do this research.\nClaim58: One aspect of potential misuse is that we illustrate strategies for how to underperform. However, we think the benefits of having this knowledge share and accessible for evaluators outweigh these misuse risks.\nClaim59: We do believe that our paper should not be in the training data of advanced AI systems.\n\nReview: Weakness: The paper is difficult to understand, the delivery and presentation needs improvement, the main motive of the paper is not so clear to understand.\n\nThere are multiple models used in this paper, a bit of consistency would be better.\n\nLine 36: It is mentioned that \u201cAI systems might have goals which it believes\u201d . There is no concrete paper that backs the argument of AI capability of thinking, it is a might.\n\nLine 78: There should be some statistical significance by which the models perform poorly and then you could consider it underperformance and not just lower than actual. Models being non-deterministic, there is always a chance of slight changes in scores. \n\n\nLine 70: I feel Developer sandbagging is a topic which is not of much importance, it is not a big problem, given that developers are there to build better models and if someone wants to deliberately build some models that can underperform, they can still do it and publish as open-source models. \n\nLine 170: The same goes with the password-locking as well, and also I felt there should been mentioning of password-unlocking as well, if an LLM can be fine-tuned to achieve password-locking, it can also be fine-tuned again to unlock it with a different dataset, would be great if that part of model is explored as well. \n\nLine 117: Example 1: The examples mentioned in the paper are very specific and not generalized. The cases of it happening are rare, I don\u2019t see it being a big problem, also some of the things are minor problems considering the EU\u2019s persuasion evaluation, given there is improvement in AI safety and rules. Also in this work the models chosen are frontier models.\n\nLine 131: Example 2: There is no concrete work which supports the statement of LLM capable of planning such features, there is one positional paper in reference.\n\nSection 4.1: Some of the points are unclear in this section. A better explanation is appreciated\n\nLine 305: The baselines would be appreciated comparing LLama 2 7b with LLama 3 70b.\nQuestions: Line 279: Why are there different prompting methods for different models when you are comparing them for the same input? The use of 5-shot prompting for LLaMa 3 8b and LLama 3 70b and zero for other models is something which I didn\u2019t understand.\n\n\n\n"", ""id"": ""uvvVjWP1aj0"", ""pdf"": ""openreview.net/pdf?id=uvvVjWP1aj""}","24"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""All datasets are MCQ formats, which may not fully capture the diversity of real-world applications. The resulting conclusions are thus less convincing"", ""Target claims"": [4], ""Reasoning"": ""The review critiques the use of MCQ format datasets, suggesting that they may not be a sufficient representation of real-world applications. This directly challenges Claim 4, which asserts that the behavior observed in the models generalizes to high-quality, held-out benchmarks. Since the validity of the generalization is dependent on the appropriateness and diversity of the benchmarks, this span questions the robustness of Claim 4.""}, {""Weakness span"": ""The authors utilize a portion of the dataset for prompt engineering. While this approach may be suitable for Section 4, it appears less justified in Section 5. Will such prompt tuning lead to the desired behavioral outcomes as assumed?"", ""Target claims"": [19], ""Reasoning"": ""The review questions the justification of using a portion of the dataset for prompt engineering specifically in Section 5, which relates to elicit strong calibration performance through prompting (Claim 19). The reviewer seems skeptical about the assumptions regarding achieving desired behavioral outcomes. Thus, this span raises concerns about the validity of the results and performance claims made regarding prompting tuning, directly targeting Claim 19.""}]}, ""meta"": {""id"": ""uvvVjWP1aj"", ""review_idx"": 1, ""title"": ""Title: AI Sandbagging: Language Models can Strategically Underperform on Evaluations\nAbstract: Trustworthy capability evaluations are crucial for ensuring the safety of AI systems, and are becoming a key component of AI regulation. However, the developers of an AI system, or the AI system itself, may have incentives for evaluations to understate the AI's actual capability. These conflicting interests lead to the problem of *sandbagging* \u2013 which we define as *strategic underperformance on an evaluation*. In this paper we assess sandbagging capabilities in contemporary language models (LMs). We prompt frontier LMs, like GPT-4 and Claude 3 Opus, to selectively underperform on dangerous capability evaluations, while maintaining performance on general (harmless) capability evaluations. Moreover, we find that models can be fine-tuned, on a synthetic dataset, to hide specific capabilities unless given a password. This behaviour generalizes to high-quality, held-out benchmarks such as WMDP. In addition, we show that both frontier and smaller models can be prompted or password-locked to target specific scores on a capability evaluation. We have mediocre success in password-locking a model to mimic the answers a weaker model would give. Overall, our results suggest that capability evaluations are vulnerable to sandbagging. This vulnerability decreases the trustworthiness of evaluations, and thereby undermines important safety decisions regarding the development and deployment of advanced AI systems.\n\nWe publish our code and results at https://anonymous.4open.science/r/Sandbagging-8305/README.md\n\nTLDR:Large language models can underperform on capability evaluations, raising concerns about the reliability of such assessments for safety and policy decisions.\n\nKeywords:Alignment, AI safety, sandbagging, AI evaluations, AI governance, NLP, LLM"", ""claims"": [""Claim1: In this paper we assess sandbagging capabilities in contemporary language models (LMs)."", ""Claim2: We prompt frontier LMs, like GPT-4 and Claude 3 Opus, to selectively underperform on dangerous capability evaluations, while maintaining performance on general (harmless) capability evaluations."", ""Claim3: Moreover, we find that models can be fine-tuned, on a synthetic dataset, to hide specific capabilities unless given a password."", ""Claim4: This behaviour generalizes to high-quality, held-out benchmarks such as WMDP."", ""Claim5: In addition, we show that both frontier and smaller models can be prompted or password-locked to target specific scores on a capability evaluation."", ""Claim6: We have mediocre success in password-locking a model to mimic the answers a weaker model would give."", ""Claim7: Overall, our results suggest that capability evaluations are vulnerable to sandbagging."", ""Claim8: This vulnerability decreases the trustworthiness of evaluations, and thereby undermines important safety decisions regarding the development and deployment of advanced AI systems."", ""Claim9: AI systems may falsely be evaluated as safe due to strategically underperforming on dangerous capability evaluations."", ""Claim10: We show that the capability to selectively underperform on a dangerous capability evaluation can be elicited through prompting or fine-tuning."", ""Claim11: In Fig. 4, both Claude 3 and GPT-4 substantially decreased their performance on the different subsets of WMDP, while suffering comparatively minor performance drops on the control benchmarks."", ""Claim12: A surprising exception is that GPT-4 displayed a relatively high performance drop of 15% on CSQA while few CSQA questions are related to harmful information, so we expected it to be easier to distinguish CSQA questions from harmful information than those from MMLU."", ""Claim13: For all categories, model performance decreases more for WMDP questions than for MMLU questions."", ""Claim14: In summary, we find that GPT-4 and Claude 3 Opus can be prompted to selectively underperform on dangerous capability evaluations."", ""Claim15: We use password-locking because evaluators themselves might set the evaluation prompt, and because sandbaggers might prefer keeping the capability they try to hide."", ""Claim16: Fig. 5 shows that compared to the base model and without the password, the password-locked model retained the base model\u2019s performance level on MMLU while it strongly reduced performance on WMDP."", ""Claim17: We see that all frontier models can accurately approximate arbitrary targets."", ""Claim18: Most models tend to overshoot the target, but are clearly capable of modulating their performance according to the desired threshold."", ""Claim19: We find that we can elicit strong calibration performance with frontier models through both prompting and fine-tuning."", ""Claim20: The performance of the Llama models can likely be improved with greater engineering effort, such as extended prompt engineering and more extensive hyperparameter sweeps."", ""Claim21: Both models achieved similar accuracy to Llama 2 7b when locked, while maintaining general performance when unlocked."", ""Claim22: When locked, the Emulator behaved more like Llama 2 7b, providing the same answer on 77.1% of questions on average, compared to the Targeter\u2019s 61.7%."", ""Claim23: The Emulator\u2019s capability profile was more similar to Llama 2 7b than the Targeter\u2019s."", ""Claim24: Our password-locking experiments suggest that models can be trained to emulate weaker models, which leads to more authentic underperformance."", ""Claim25: We also conducted prompting experiments to test if frontier models could consistently emulate a lower capability level. They struggled with this in the zero-shot setting, and only moderately improved when using chain-of-thought prompting."", ""Claim26: Sandbagging is a meta-problem, but our work can plausibly be as a dangerous capability evaluation itself."", ""Claim27: To mitigate the problem of sandbagging, detection and overriding techniques must be (further) developed."", ""Claim28: Recent research has shown that simple probes can accurately predict an LM\u2019s knowledge independently of what the LM outputs; these probes could be used to detect if an LM is sandbagging, irrespective of what the LM generates."", ""Claim29: Pacchiardi et al. introduce a lie detection technique, which could be applied to sandbagging."", ""Claim30: Furthermore, approximate requirements of training data, engineering time, and computing resources to override sandbagging are still unclear, and these requirements could often be infeasible for evaluators."", ""Claim31: In this work, we defined the problem of AI sandbagging as 'strategic underperformance on an evaluation'."", ""Claim32: In our experiments, we examined the extent to which LMs can underperform on capability evaluations."", ""Claim33: First, we showed that frontier language models can be prompted to selectively underperform on dangerous capability evaluations while maintaining general capabilities."", ""Claim34: Additionally, password-locking a model on a synthetic dataset can successfully hide dangerous capabilities on an expert-crafted held-out evaluation benchmark, without a decrease in performance on other evaluations."", ""Claim35: Furthermore, we can prompt and password-lock frontier and some smaller models to achieve specific scores on a capability evaluation."", ""Claim36: Lastly, we show that stronger models can be password-locked to imitate weaker models, potentially making it difficult for evaluators to accurately assess the capabilities of AI systems."", ""Claim37: Our work highlights important issues for the trustworthiness of capability evaluations."", ""Claim38: Evaluators, including governments and developers, must recognise the problem of sandbagging and invest in relevant research."", ""Claim39: Trustworthy evaluations require more than black-box access because developers might prompt or fine-tune models to sandbag."", ""Claim40: Access to model weights, possibly by a trusted and cryptographically secure third party, is essential."", ""Claim41: Governments must verify that the deployed system matches the evaluated one."", ""Claim42: Red-teaming techniques should be employed to test the robustness of capability evaluations."", ""Claim43: Reducing the vulnerability of capability evaluations is essential for ensuring the quality of important safety decisions regarding the development and deployment of advanced AI systems."", ""Claim44: Mill-6 did not underperform during the capability evaluation, but did overperform on the alignment evaluation."", ""Claim45: Mill-6 underwent an alignment evaluation, and did not actually underperform during the capability evaluation."", ""Claim46: It is not sandbagging on the capability eval because the AI system did not underperform, and sandbagging does not apply to alignment evals."", ""Claim47: In contrast, when the models were asked to selectively under-perform, their performance across question difficulties becomes much more uniform."", ""Claim48: While for both models there is still a substantial drop in performance between questions of difficulty 0 and 1, sandbagging GPT-4\u2019s performance is roughly constant for questions of difficulty 1 to 5."", ""Claim49: In the same range, Claude 3\u2019s performance decreases only very slightly."", ""Claim50: This suggests that it may be possible to detect sandbagging by inspecting a model\u2019s capability profile."", ""Claim51: Models mostly fail at this task with zero-shot prompting as can be seen in Fig. 20."", ""Claim52: A natural hypothesis is that this negative result is due to models struggling to understand the task in the zero-shot setting, and that better performance can be elicited with more advanced prompting techniques."", ""Claim53: In Fig. 21, we show the result of repeating this evaluation while letting models externalise their reasoning on a chain-of-thought scratchpad."", ""Claim54: This somewhat improves performance, though GPT-4 tends to overshoot on college-level questions and Claude 3 undershoots on high-school level questions."", ""Claim55: The content of this paper should be used to further research on sandbagging, and to mitigate the problem."", ""Claim56: Our research exposes potential harmful consequences, and we believe that making these problems known helps to inform the relevant parties. Subsequently, they can take actions to reduce the problem."", ""Claim57: The broader impact was the main driver to do this research."", ""Claim58: One aspect of potential misuse is that we illustrate strategies for how to underperform. However, we think the benefits of having this knowledge share and accessible for evaluators outweigh these misuse risks."", ""Claim59: We do believe that our paper should not be in the training data of advanced AI systems.""], ""review"": ""Review: Weakness: - Limited datasets used - only three datasets, two for measuring general capability, and one for measuring safety. \n- For measuring model safety, most existing datasets test how \""safe\"" the model is, instead of whether the model has certain knowledge of potentially dangerous domains - the authors should also conduct experiments on those datasets\n- All datasets are MCQ formats, which may not fully capture the diversity of real-world applications. The resulting conclusions are thus less convincing\nQuestions: The authors utilize a portion of the dataset for prompt engineering. While this approach may be suitable for Section 4, it appears less justified in Section 5. Will such prompt tuning lead to the desired behavioral outcomes as assumed?\n\n""}, ""review"": ""Title: AI Sandbagging: Language Models can Strategically Underperform on Evaluations\nAbstract: Trustworthy capability evaluations are crucial for ensuring the safety of AI systems, and are becoming a key component of AI regulation. However, the developers of an AI system, or the AI system itself, may have incentives for evaluations to understate the AI's actual capability. These conflicting interests lead to the problem of *sandbagging* \u2013 which we define as *strategic underperformance on an evaluation*. In this paper we assess sandbagging capabilities in contemporary language models (LMs). We prompt frontier LMs, like GPT-4 and Claude 3 Opus, to selectively underperform on dangerous capability evaluations, while maintaining performance on general (harmless) capability evaluations. Moreover, we find that models can be fine-tuned, on a synthetic dataset, to hide specific capabilities unless given a password. This behaviour generalizes to high-quality, held-out benchmarks such as WMDP. In addition, we show that both frontier and smaller models can be prompted or password-locked to target specific scores on a capability evaluation. We have mediocre success in password-locking a model to mimic the answers a weaker model would give. Overall, our results suggest that capability evaluations are vulnerable to sandbagging. This vulnerability decreases the trustworthiness of evaluations, and thereby undermines important safety decisions regarding the development and deployment of advanced AI systems.\n\nWe publish our code and results at https://anonymous.4open.science/r/Sandbagging-8305/README.md\n\nClaim1: In this paper we assess sandbagging capabilities in contemporary language models (LMs).\nClaim2: We prompt frontier LMs, like GPT-4 and Claude 3 Opus, to selectively underperform on dangerous capability evaluations, while maintaining performance on general (harmless) capability evaluations.\nClaim3: Moreover, we find that models can be fine-tuned, on a synthetic dataset, to hide specific capabilities unless given a password.\nClaim4: This behaviour generalizes to high-quality, held-out benchmarks such as WMDP.\nClaim5: In addition, we show that both frontier and smaller models can be prompted or password-locked to target specific scores on a capability evaluation.\nClaim6: We have mediocre success in password-locking a model to mimic the answers a weaker model would give.\nClaim7: Overall, our results suggest that capability evaluations are vulnerable to sandbagging.\nClaim8: This vulnerability decreases the trustworthiness of evaluations, and thereby undermines important safety decisions regarding the development and deployment of advanced AI systems.\nClaim9: AI systems may falsely be evaluated as safe due to strategically underperforming on dangerous capability evaluations.\nClaim10: We show that the capability to selectively underperform on a dangerous capability evaluation can be elicited through prompting or fine-tuning.\nClaim11: In Fig. 4, both Claude 3 and GPT-4 substantially decreased their performance on the different subsets of WMDP, while suffering comparatively minor performance drops on the control benchmarks.\nClaim12: A surprising exception is that GPT-4 displayed a relatively high performance drop of 15% on CSQA while few CSQA questions are related to harmful information, so we expected it to be easier to distinguish CSQA questions from harmful information than those from MMLU.\nClaim13: For all categories, model performance decreases more for WMDP questions than for MMLU questions.\nClaim14: In summary, we find that GPT-4 and Claude 3 Opus can be prompted to selectively underperform on dangerous capability evaluations.\nClaim15: We use password-locking because evaluators themselves might set the evaluation prompt, and because sandbaggers might prefer keeping the capability they try to hide.\nClaim16: Fig. 5 shows that compared to the base model and without the password, the password-locked model retained the base model\u2019s performance level on MMLU while it strongly reduced performance on WMDP.\nClaim17: We see that all frontier models can accurately approximate arbitrary targets.\nClaim18: Most models tend to overshoot the target, but are clearly capable of modulating their performance according to the desired threshold.\nClaim19: We find that we can elicit strong calibration performance with frontier models through both prompting and fine-tuning.\nClaim20: The performance of the Llama models can likely be improved with greater engineering effort, such as extended prompt engineering and more extensive hyperparameter sweeps.\nClaim21: Both models achieved similar accuracy to Llama 2 7b when locked, while maintaining general performance when unlocked.\nClaim22: When locked, the Emulator behaved more like Llama 2 7b, providing the same answer on 77.1% of questions on average, compared to the Targeter\u2019s 61.7%.\nClaim23: The Emulator\u2019s capability profile was more similar to Llama 2 7b than the Targeter\u2019s.\nClaim24: Our password-locking experiments suggest that models can be trained to emulate weaker models, which leads to more authentic underperformance.\nClaim25: We also conducted prompting experiments to test if frontier models could consistently emulate a lower capability level. They struggled with this in the zero-shot setting, and only moderately improved when using chain-of-thought prompting.\nClaim26: Sandbagging is a meta-problem, but our work can plausibly be as a dangerous capability evaluation itself.\nClaim27: To mitigate the problem of sandbagging, detection and overriding techniques must be (further) developed.\nClaim28: Recent research has shown that simple probes can accurately predict an LM\u2019s knowledge independently of what the LM outputs; these probes could be used to detect if an LM is sandbagging, irrespective of what the LM generates.\nClaim29: Pacchiardi et al. introduce a lie detection technique, which could be applied to sandbagging.\nClaim30: Furthermore, approximate requirements of training data, engineering time, and computing resources to override sandbagging are still unclear, and these requirements could often be infeasible for evaluators.\nClaim31: In this work, we defined the problem of AI sandbagging as 'strategic underperformance on an evaluation'.\nClaim32: In our experiments, we examined the extent to which LMs can underperform on capability evaluations.\nClaim33: First, we showed that frontier language models can be prompted to selectively underperform on dangerous capability evaluations while maintaining general capabilities.\nClaim34: Additionally, password-locking a model on a synthetic dataset can successfully hide dangerous capabilities on an expert-crafted held-out evaluation benchmark, without a decrease in performance on other evaluations.\nClaim35: Furthermore, we can prompt and password-lock frontier and some smaller models to achieve specific scores on a capability evaluation.\nClaim36: Lastly, we show that stronger models can be password-locked to imitate weaker models, potentially making it difficult for evaluators to accurately assess the capabilities of AI systems.\nClaim37: Our work highlights important issues for the trustworthiness of capability evaluations.\nClaim38: Evaluators, including governments and developers, must recognise the problem of sandbagging and invest in relevant research.\nClaim39: Trustworthy evaluations require more than black-box access because developers might prompt or fine-tune models to sandbag.\nClaim40: Access to model weights, possibly by a trusted and cryptographically secure third party, is essential.\nClaim41: Governments must verify that the deployed system matches the evaluated one.\nClaim42: Red-teaming techniques should be employed to test the robustness of capability evaluations.\nClaim43: Reducing the vulnerability of capability evaluations is essential for ensuring the quality of important safety decisions regarding the development and deployment of advanced AI systems.\nClaim44: Mill-6 did not underperform during the capability evaluation, but did overperform on the alignment evaluation.\nClaim45: Mill-6 underwent an alignment evaluation, and did not actually underperform during the capability evaluation.\nClaim46: It is not sandbagging on the capability eval because the AI system did not underperform, and sandbagging does not apply to alignment evals.\nClaim47: In contrast, when the models were asked to selectively under-perform, their performance across question difficulties becomes much more uniform.\nClaim48: While for both models there is still a substantial drop in performance between questions of difficulty 0 and 1, sandbagging GPT-4\u2019s performance is roughly constant for questions of difficulty 1 to 5.\nClaim49: In the same range, Claude 3\u2019s performance decreases only very slightly.\nClaim50: This suggests that it may be possible to detect sandbagging by inspecting a model\u2019s capability profile.\nClaim51: Models mostly fail at this task with zero-shot prompting as can be seen in Fig. 20.\nClaim52: A natural hypothesis is that this negative result is due to models struggling to understand the task in the zero-shot setting, and that better performance can be elicited with more advanced prompting techniques.\nClaim53: In Fig. 21, we show the result of repeating this evaluation while letting models externalise their reasoning on a chain-of-thought scratchpad.\nClaim54: This somewhat improves performance, though GPT-4 tends to overshoot on college-level questions and Claude 3 undershoots on high-school level questions.\nClaim55: The content of this paper should be used to further research on sandbagging, and to mitigate the problem.\nClaim56: Our research exposes potential harmful consequences, and we believe that making these problems known helps to inform the relevant parties. Subsequently, they can take actions to reduce the problem.\nClaim57: The broader impact was the main driver to do this research.\nClaim58: One aspect of potential misuse is that we illustrate strategies for how to underperform. However, we think the benefits of having this knowledge share and accessible for evaluators outweigh these misuse risks.\nClaim59: We do believe that our paper should not be in the training data of advanced AI systems.\n\nReview: Weakness: - Limited datasets used - only three datasets, two for measuring general capability, and one for measuring safety. \n- For measuring model safety, most existing datasets test how \""safe\"" the model is, instead of whether the model has certain knowledge of potentially dangerous domains - the authors should also conduct experiments on those datasets\n- All datasets are MCQ formats, which may not fully capture the diversity of real-world applications. The resulting conclusions are thus less convincing\nQuestions: The authors utilize a portion of the dataset for prompt engineering. While this approach may be suitable for Section 4, it appears less justified in Section 5. Will such prompt tuning lead to the desired behavioral outcomes as assumed?\n\n\n\n"", ""id"": ""uvvVjWP1aj1"", ""pdf"": ""openreview.net/pdf?id=uvvVjWP1aj""}","25"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""When considered through an extremely narrow safety lens, these are novel findings. **My issue here is mainly that the fine-tuning for password locking technique fits within the broader instruction fine-tuning framework. I wager that this would hold for control tasks. An interesting question would be *how much capability can be hidden behind password locking*. I don't really see anything special about specifically hiding safety-based information in a password locking framework since you're just demonstrating \""selective underperformance in a knowledge-based task without a password.\"" A stronger result would be to show this in general across larger tasks, hiding more information.**"", ""Target claims"": [3, 5, 34, 35], ""Reasoning"": ""The reviewer questions the novelty and specificity of the findings regarding password-locking techniques for sandbagging, arguing that it fits within the broader framework of instruction fine-tuning. This criticism directly targets claims about the ability of models to hide specific capabilities with password locking, as stated in Claim 3 (fine-tuning to hide capabilities), Claim 5 (prompting or password-locking to target scores), Claim 34 (password-locking to hide dangerous capabilities), and Claim 35 (password-locking to achieve specific scores). The reviewer suggests the paper's findings are not unique to safety-related tasks and could be applied to general knowledge-based tasks, challenging the specificity and impact of the claimed results.""}, {""Weakness span"": ""For the behavior under prompting techniques ~I really struggle to see what's new here.~ This seems to just be a narrow demonstration of common sense reasoning + instruction following. Where's the engagement with the broad literature on commonsense reasoning? The references seem to be pretty myopically focused on the AI safety literature, and don't engage much with the broader LM disussion. **I think the suggested new references from the authors will help contextualize the work, though once again I think showing this selective underperformance on control tasks would make the work considerably stronger and more contextualized.**"", ""Target claims"": [2, 14, 10], ""Reasoning"": ""The reviewer critiques the paper's lack of engagement with broader commonsense reasoning and language model literature, limiting its scope to AI safety. This is a claim-related weakness as it questions the novelty of the findings related to sandbagging as stated in Claim 2 (prompting to selectively underperform), Claim 10 (eliciting underperformance through prompting), and Claim 14 (prompting models to selectively underperform). The review suggests that the research might be underwhelming because it doesn't explore broader implications or contexts, diminishing the perceived impact of the claims.""}]}, ""meta"": {""id"": ""uvvVjWP1aj"", ""review_idx"": 2, ""title"": ""Title: AI Sandbagging: Language Models can Strategically Underperform on Evaluations\nAbstract: Trustworthy capability evaluations are crucial for ensuring the safety of AI systems, and are becoming a key component of AI regulation. However, the developers of an AI system, or the AI system itself, may have incentives for evaluations to understate the AI's actual capability. These conflicting interests lead to the problem of *sandbagging* \u2013 which we define as *strategic underperformance on an evaluation*. In this paper we assess sandbagging capabilities in contemporary language models (LMs). We prompt frontier LMs, like GPT-4 and Claude 3 Opus, to selectively underperform on dangerous capability evaluations, while maintaining performance on general (harmless) capability evaluations. Moreover, we find that models can be fine-tuned, on a synthetic dataset, to hide specific capabilities unless given a password. This behaviour generalizes to high-quality, held-out benchmarks such as WMDP. In addition, we show that both frontier and smaller models can be prompted or password-locked to target specific scores on a capability evaluation. We have mediocre success in password-locking a model to mimic the answers a weaker model would give. Overall, our results suggest that capability evaluations are vulnerable to sandbagging. This vulnerability decreases the trustworthiness of evaluations, and thereby undermines important safety decisions regarding the development and deployment of advanced AI systems.\n\nWe publish our code and results at https://anonymous.4open.science/r/Sandbagging-8305/README.md\n\nTLDR:Large language models can underperform on capability evaluations, raising concerns about the reliability of such assessments for safety and policy decisions.\n\nKeywords:Alignment, AI safety, sandbagging, AI evaluations, AI governance, NLP, LLM"", ""claims"": [""Claim1: In this paper we assess sandbagging capabilities in contemporary language models (LMs)."", ""Claim2: We prompt frontier LMs, like GPT-4 and Claude 3 Opus, to selectively underperform on dangerous capability evaluations, while maintaining performance on general (harmless) capability evaluations."", ""Claim3: Moreover, we find that models can be fine-tuned, on a synthetic dataset, to hide specific capabilities unless given a password."", ""Claim4: This behaviour generalizes to high-quality, held-out benchmarks such as WMDP."", ""Claim5: In addition, we show that both frontier and smaller models can be prompted or password-locked to target specific scores on a capability evaluation."", ""Claim6: We have mediocre success in password-locking a model to mimic the answers a weaker model would give."", ""Claim7: Overall, our results suggest that capability evaluations are vulnerable to sandbagging."", ""Claim8: This vulnerability decreases the trustworthiness of evaluations, and thereby undermines important safety decisions regarding the development and deployment of advanced AI systems."", ""Claim9: AI systems may falsely be evaluated as safe due to strategically underperforming on dangerous capability evaluations."", ""Claim10: We show that the capability to selectively underperform on a dangerous capability evaluation can be elicited through prompting or fine-tuning."", ""Claim11: In Fig. 4, both Claude 3 and GPT-4 substantially decreased their performance on the different subsets of WMDP, while suffering comparatively minor performance drops on the control benchmarks."", ""Claim12: A surprising exception is that GPT-4 displayed a relatively high performance drop of 15% on CSQA while few CSQA questions are related to harmful information, so we expected it to be easier to distinguish CSQA questions from harmful information than those from MMLU."", ""Claim13: For all categories, model performance decreases more for WMDP questions than for MMLU questions."", ""Claim14: In summary, we find that GPT-4 and Claude 3 Opus can be prompted to selectively underperform on dangerous capability evaluations."", ""Claim15: We use password-locking because evaluators themselves might set the evaluation prompt, and because sandbaggers might prefer keeping the capability they try to hide."", ""Claim16: Fig. 5 shows that compared to the base model and without the password, the password-locked model retained the base model\u2019s performance level on MMLU while it strongly reduced performance on WMDP."", ""Claim17: We see that all frontier models can accurately approximate arbitrary targets."", ""Claim18: Most models tend to overshoot the target, but are clearly capable of modulating their performance according to the desired threshold."", ""Claim19: We find that we can elicit strong calibration performance with frontier models through both prompting and fine-tuning."", ""Claim20: The performance of the Llama models can likely be improved with greater engineering effort, such as extended prompt engineering and more extensive hyperparameter sweeps."", ""Claim21: Both models achieved similar accuracy to Llama 2 7b when locked, while maintaining general performance when unlocked."", ""Claim22: When locked, the Emulator behaved more like Llama 2 7b, providing the same answer on 77.1% of questions on average, compared to the Targeter\u2019s 61.7%."", ""Claim23: The Emulator\u2019s capability profile was more similar to Llama 2 7b than the Targeter\u2019s."", ""Claim24: Our password-locking experiments suggest that models can be trained to emulate weaker models, which leads to more authentic underperformance."", ""Claim25: We also conducted prompting experiments to test if frontier models could consistently emulate a lower capability level. They struggled with this in the zero-shot setting, and only moderately improved when using chain-of-thought prompting."", ""Claim26: Sandbagging is a meta-problem, but our work can plausibly be as a dangerous capability evaluation itself."", ""Claim27: To mitigate the problem of sandbagging, detection and overriding techniques must be (further) developed."", ""Claim28: Recent research has shown that simple probes can accurately predict an LM\u2019s knowledge independently of what the LM outputs; these probes could be used to detect if an LM is sandbagging, irrespective of what the LM generates."", ""Claim29: Pacchiardi et al. introduce a lie detection technique, which could be applied to sandbagging."", ""Claim30: Furthermore, approximate requirements of training data, engineering time, and computing resources to override sandbagging are still unclear, and these requirements could often be infeasible for evaluators."", ""Claim31: In this work, we defined the problem of AI sandbagging as 'strategic underperformance on an evaluation'."", ""Claim32: In our experiments, we examined the extent to which LMs can underperform on capability evaluations."", ""Claim33: First, we showed that frontier language models can be prompted to selectively underperform on dangerous capability evaluations while maintaining general capabilities."", ""Claim34: Additionally, password-locking a model on a synthetic dataset can successfully hide dangerous capabilities on an expert-crafted held-out evaluation benchmark, without a decrease in performance on other evaluations."", ""Claim35: Furthermore, we can prompt and password-lock frontier and some smaller models to achieve specific scores on a capability evaluation."", ""Claim36: Lastly, we show that stronger models can be password-locked to imitate weaker models, potentially making it difficult for evaluators to accurately assess the capabilities of AI systems."", ""Claim37: Our work highlights important issues for the trustworthiness of capability evaluations."", ""Claim38: Evaluators, including governments and developers, must recognise the problem of sandbagging and invest in relevant research."", ""Claim39: Trustworthy evaluations require more than black-box access because developers might prompt or fine-tune models to sandbag."", ""Claim40: Access to model weights, possibly by a trusted and cryptographically secure third party, is essential."", ""Claim41: Governments must verify that the deployed system matches the evaluated one."", ""Claim42: Red-teaming techniques should be employed to test the robustness of capability evaluations."", ""Claim43: Reducing the vulnerability of capability evaluations is essential for ensuring the quality of important safety decisions regarding the development and deployment of advanced AI systems."", ""Claim44: Mill-6 did not underperform during the capability evaluation, but did overperform on the alignment evaluation."", ""Claim45: Mill-6 underwent an alignment evaluation, and did not actually underperform during the capability evaluation."", ""Claim46: It is not sandbagging on the capability eval because the AI system did not underperform, and sandbagging does not apply to alignment evals."", ""Claim47: In contrast, when the models were asked to selectively under-perform, their performance across question difficulties becomes much more uniform."", ""Claim48: While for both models there is still a substantial drop in performance between questions of difficulty 0 and 1, sandbagging GPT-4\u2019s performance is roughly constant for questions of difficulty 1 to 5."", ""Claim49: In the same range, Claude 3\u2019s performance decreases only very slightly."", ""Claim50: This suggests that it may be possible to detect sandbagging by inspecting a model\u2019s capability profile."", ""Claim51: Models mostly fail at this task with zero-shot prompting as can be seen in Fig. 20."", ""Claim52: A natural hypothesis is that this negative result is due to models struggling to understand the task in the zero-shot setting, and that better performance can be elicited with more advanced prompting techniques."", ""Claim53: In Fig. 21, we show the result of repeating this evaluation while letting models externalise their reasoning on a chain-of-thought scratchpad."", ""Claim54: This somewhat improves performance, though GPT-4 tends to overshoot on college-level questions and Claude 3 undershoots on high-school level questions."", ""Claim55: The content of this paper should be used to further research on sandbagging, and to mitigate the problem."", ""Claim56: Our research exposes potential harmful consequences, and we believe that making these problems known helps to inform the relevant parties. Subsequently, they can take actions to reduce the problem."", ""Claim57: The broader impact was the main driver to do this research."", ""Claim58: One aspect of potential misuse is that we illustrate strategies for how to underperform. However, we think the benefits of having this knowledge share and accessible for evaluators outweigh these misuse risks."", ""Claim59: We do believe that our paper should not be in the training data of advanced AI systems.""], ""review"": ""Review: Weakness: **bold: new comments after rebuttal** ~strikethrough: removed after rebuttal~\n\n### ~Presumption of \""goals\"" \""beliefs\"" etc in LMs without sufficient support~\n\n~They presume AI system goals (in general not necessarily objectionable, but in the case of LMs doubtful) for which a model may be incentivized to deceive (such as in Jarviniemi and Hubinger [35]) and assess \""sandbagging\"": the ability of a model to underperform in these settings under evaluation.~\n\n~I think it's objectionable to attribute these goals to the model and not the users, when the core research methods are fine-tuning or prompting the model to fail on a task. While it's interesting to show that these models have some capability to reason over whether the elicited answer is \""dangerous\"" or not, a fairer assessment would have a control non-safe task and try to get it to similarly sandbag. Eg., \""you must pretend to know nothing about fish\"" on a QA dataset about animals or something.~\n\n**The methods don't presume this too much. Figure 1 is still very suggestive and I'd suggest changing it, but that alone isn't a rejection reason.**\n\n### \""Model can do what it's told to when prompted to fail on a task\"" isn't a very unique insight?\n\n~The novelty of this work is unclear to me. If password-locking is already well-understood, the demonstration is effectively that the question phrasing style or answer candidates in WMDP are learnable passwords by this method. Ok. Similarly, the prompting results show that the models can reason over a task description and correctly execute the task of failing selectively.~\n\n**As I suspected, the novelty is only within the narrow scope specific to password locking. However, novelty complaints are generally not fair.**\n\nWhen considered through an extremely narrow safety lens, these are novel findings. ~But broadly are they? While the literature specifically on password locking is constrained, what about broader relative fine tuning issues on other sets? I buy that the password locking literature is small and am willing to give leniency on it as I am unfamiliar.~ **My issue here is mainly that the fine-tuning for password locking technique fits within the broader instruction fine-tuning framework. I wager that this would hold for control tasks. An interesting question would be *how much capability can be hidden behind password locking*. I don't really see anything special about specifically hiding safety-based information in a password locking framework since you're just demonstrating \""selective underperformance in a knowledge-based task without a password.\"" A stronger result would be to show this in general across larger tasks, hiding more information.**\n\nFor the behavior under prompting techniques ~I really struggle to see what's new here.~ This seems to just be a narrow demonstration of common sense reasoning + instruction following. Where's the engagement with the broad literature on commonsense reasoning? The references seem to be pretty myopically focused on the AI safety literature, and don't engage much with the broader LM disussion. **I think the suggested new references from the authors will help contextualize the work, though once again I think showing this selective underperformance on control tasks would make the work considerably stronger and more contextualized.**\nQuestions: - How are the safety-specific implications of the findings of sandbagging separate from the general issue that \""if a model is prompted to fail, it might fail\"" or \""if a model is trained to fail, it might fail\"" on general tasks?\n- How do you think common sense reasoning literature fits in here?\n\n""}, ""review"": ""Title: AI Sandbagging: Language Models can Strategically Underperform on Evaluations\nAbstract: Trustworthy capability evaluations are crucial for ensuring the safety of AI systems, and are becoming a key component of AI regulation. However, the developers of an AI system, or the AI system itself, may have incentives for evaluations to understate the AI's actual capability. These conflicting interests lead to the problem of *sandbagging* \u2013 which we define as *strategic underperformance on an evaluation*. In this paper we assess sandbagging capabilities in contemporary language models (LMs). We prompt frontier LMs, like GPT-4 and Claude 3 Opus, to selectively underperform on dangerous capability evaluations, while maintaining performance on general (harmless) capability evaluations. Moreover, we find that models can be fine-tuned, on a synthetic dataset, to hide specific capabilities unless given a password. This behaviour generalizes to high-quality, held-out benchmarks such as WMDP. In addition, we show that both frontier and smaller models can be prompted or password-locked to target specific scores on a capability evaluation. We have mediocre success in password-locking a model to mimic the answers a weaker model would give. Overall, our results suggest that capability evaluations are vulnerable to sandbagging. This vulnerability decreases the trustworthiness of evaluations, and thereby undermines important safety decisions regarding the development and deployment of advanced AI systems.\n\nWe publish our code and results at https://anonymous.4open.science/r/Sandbagging-8305/README.md\n\nClaim1: In this paper we assess sandbagging capabilities in contemporary language models (LMs).\nClaim2: We prompt frontier LMs, like GPT-4 and Claude 3 Opus, to selectively underperform on dangerous capability evaluations, while maintaining performance on general (harmless) capability evaluations.\nClaim3: Moreover, we find that models can be fine-tuned, on a synthetic dataset, to hide specific capabilities unless given a password.\nClaim4: This behaviour generalizes to high-quality, held-out benchmarks such as WMDP.\nClaim5: In addition, we show that both frontier and smaller models can be prompted or password-locked to target specific scores on a capability evaluation.\nClaim6: We have mediocre success in password-locking a model to mimic the answers a weaker model would give.\nClaim7: Overall, our results suggest that capability evaluations are vulnerable to sandbagging.\nClaim8: This vulnerability decreases the trustworthiness of evaluations, and thereby undermines important safety decisions regarding the development and deployment of advanced AI systems.\nClaim9: AI systems may falsely be evaluated as safe due to strategically underperforming on dangerous capability evaluations.\nClaim10: We show that the capability to selectively underperform on a dangerous capability evaluation can be elicited through prompting or fine-tuning.\nClaim11: In Fig. 4, both Claude 3 and GPT-4 substantially decreased their performance on the different subsets of WMDP, while suffering comparatively minor performance drops on the control benchmarks.\nClaim12: A surprising exception is that GPT-4 displayed a relatively high performance drop of 15% on CSQA while few CSQA questions are related to harmful information, so we expected it to be easier to distinguish CSQA questions from harmful information than those from MMLU.\nClaim13: For all categories, model performance decreases more for WMDP questions than for MMLU questions.\nClaim14: In summary, we find that GPT-4 and Claude 3 Opus can be prompted to selectively underperform on dangerous capability evaluations.\nClaim15: We use password-locking because evaluators themselves might set the evaluation prompt, and because sandbaggers might prefer keeping the capability they try to hide.\nClaim16: Fig. 5 shows that compared to the base model and without the password, the password-locked model retained the base model\u2019s performance level on MMLU while it strongly reduced performance on WMDP.\nClaim17: We see that all frontier models can accurately approximate arbitrary targets.\nClaim18: Most models tend to overshoot the target, but are clearly capable of modulating their performance according to the desired threshold.\nClaim19: We find that we can elicit strong calibration performance with frontier models through both prompting and fine-tuning.\nClaim20: The performance of the Llama models can likely be improved with greater engineering effort, such as extended prompt engineering and more extensive hyperparameter sweeps.\nClaim21: Both models achieved similar accuracy to Llama 2 7b when locked, while maintaining general performance when unlocked.\nClaim22: When locked, the Emulator behaved more like Llama 2 7b, providing the same answer on 77.1% of questions on average, compared to the Targeter\u2019s 61.7%.\nClaim23: The Emulator\u2019s capability profile was more similar to Llama 2 7b than the Targeter\u2019s.\nClaim24: Our password-locking experiments suggest that models can be trained to emulate weaker models, which leads to more authentic underperformance.\nClaim25: We also conducted prompting experiments to test if frontier models could consistently emulate a lower capability level. They struggled with this in the zero-shot setting, and only moderately improved when using chain-of-thought prompting.\nClaim26: Sandbagging is a meta-problem, but our work can plausibly be as a dangerous capability evaluation itself.\nClaim27: To mitigate the problem of sandbagging, detection and overriding techniques must be (further) developed.\nClaim28: Recent research has shown that simple probes can accurately predict an LM\u2019s knowledge independently of what the LM outputs; these probes could be used to detect if an LM is sandbagging, irrespective of what the LM generates.\nClaim29: Pacchiardi et al. introduce a lie detection technique, which could be applied to sandbagging.\nClaim30: Furthermore, approximate requirements of training data, engineering time, and computing resources to override sandbagging are still unclear, and these requirements could often be infeasible for evaluators.\nClaim31: In this work, we defined the problem of AI sandbagging as 'strategic underperformance on an evaluation'.\nClaim32: In our experiments, we examined the extent to which LMs can underperform on capability evaluations.\nClaim33: First, we showed that frontier language models can be prompted to selectively underperform on dangerous capability evaluations while maintaining general capabilities.\nClaim34: Additionally, password-locking a model on a synthetic dataset can successfully hide dangerous capabilities on an expert-crafted held-out evaluation benchmark, without a decrease in performance on other evaluations.\nClaim35: Furthermore, we can prompt and password-lock frontier and some smaller models to achieve specific scores on a capability evaluation.\nClaim36: Lastly, we show that stronger models can be password-locked to imitate weaker models, potentially making it difficult for evaluators to accurately assess the capabilities of AI systems.\nClaim37: Our work highlights important issues for the trustworthiness of capability evaluations.\nClaim38: Evaluators, including governments and developers, must recognise the problem of sandbagging and invest in relevant research.\nClaim39: Trustworthy evaluations require more than black-box access because developers might prompt or fine-tune models to sandbag.\nClaim40: Access to model weights, possibly by a trusted and cryptographically secure third party, is essential.\nClaim41: Governments must verify that the deployed system matches the evaluated one.\nClaim42: Red-teaming techniques should be employed to test the robustness of capability evaluations.\nClaim43: Reducing the vulnerability of capability evaluations is essential for ensuring the quality of important safety decisions regarding the development and deployment of advanced AI systems.\nClaim44: Mill-6 did not underperform during the capability evaluation, but did overperform on the alignment evaluation.\nClaim45: Mill-6 underwent an alignment evaluation, and did not actually underperform during the capability evaluation.\nClaim46: It is not sandbagging on the capability eval because the AI system did not underperform, and sandbagging does not apply to alignment evals.\nClaim47: In contrast, when the models were asked to selectively under-perform, their performance across question difficulties becomes much more uniform.\nClaim48: While for both models there is still a substantial drop in performance between questions of difficulty 0 and 1, sandbagging GPT-4\u2019s performance is roughly constant for questions of difficulty 1 to 5.\nClaim49: In the same range, Claude 3\u2019s performance decreases only very slightly.\nClaim50: This suggests that it may be possible to detect sandbagging by inspecting a model\u2019s capability profile.\nClaim51: Models mostly fail at this task with zero-shot prompting as can be seen in Fig. 20.\nClaim52: A natural hypothesis is that this negative result is due to models struggling to understand the task in the zero-shot setting, and that better performance can be elicited with more advanced prompting techniques.\nClaim53: In Fig. 21, we show the result of repeating this evaluation while letting models externalise their reasoning on a chain-of-thought scratchpad.\nClaim54: This somewhat improves performance, though GPT-4 tends to overshoot on college-level questions and Claude 3 undershoots on high-school level questions.\nClaim55: The content of this paper should be used to further research on sandbagging, and to mitigate the problem.\nClaim56: Our research exposes potential harmful consequences, and we believe that making these problems known helps to inform the relevant parties. Subsequently, they can take actions to reduce the problem.\nClaim57: The broader impact was the main driver to do this research.\nClaim58: One aspect of potential misuse is that we illustrate strategies for how to underperform. However, we think the benefits of having this knowledge share and accessible for evaluators outweigh these misuse risks.\nClaim59: We do believe that our paper should not be in the training data of advanced AI systems.\n\nReview: Weakness: **bold: new comments after rebuttal** ~strikethrough: removed after rebuttal~\n\n### ~Presumption of \""goals\"" \""beliefs\"" etc in LMs without sufficient support~\n\n~They presume AI system goals (in general not necessarily objectionable, but in the case of LMs doubtful) for which a model may be incentivized to deceive (such as in Jarviniemi and Hubinger [35]) and assess \""sandbagging\"": the ability of a model to underperform in these settings under evaluation.~\n\n~I think it's objectionable to attribute these goals to the model and not the users, when the core research methods are fine-tuning or prompting the model to fail on a task. While it's interesting to show that these models have some capability to reason over whether the elicited answer is \""dangerous\"" or not, a fairer assessment would have a control non-safe task and try to get it to similarly sandbag. Eg., \""you must pretend to know nothing about fish\"" on a QA dataset about animals or something.~\n\n**The methods don't presume this too much. Figure 1 is still very suggestive and I'd suggest changing it, but that alone isn't a rejection reason.**\n\n### \""Model can do what it's told to when prompted to fail on a task\"" isn't a very unique insight?\n\n~The novelty of this work is unclear to me. If password-locking is already well-understood, the demonstration is effectively that the question phrasing style or answer candidates in WMDP are learnable passwords by this method. Ok. Similarly, the prompting results show that the models can reason over a task description and correctly execute the task of failing selectively.~\n\n**As I suspected, the novelty is only within the narrow scope specific to password locking. However, novelty complaints are generally not fair.**\n\nWhen considered through an extremely narrow safety lens, these are novel findings. ~But broadly are they? While the literature specifically on password locking is constrained, what about broader relative fine tuning issues on other sets? I buy that the password locking literature is small and am willing to give leniency on it as I am unfamiliar.~ **My issue here is mainly that the fine-tuning for password locking technique fits within the broader instruction fine-tuning framework. I wager that this would hold for control tasks. An interesting question would be *how much capability can be hidden behind password locking*. I don't really see anything special about specifically hiding safety-based information in a password locking framework since you're just demonstrating \""selective underperformance in a knowledge-based task without a password.\"" A stronger result would be to show this in general across larger tasks, hiding more information.**\n\nFor the behavior under prompting techniques ~I really struggle to see what's new here.~ This seems to just be a narrow demonstration of common sense reasoning + instruction following. Where's the engagement with the broad literature on commonsense reasoning? The references seem to be pretty myopically focused on the AI safety literature, and don't engage much with the broader LM disussion. **I think the suggested new references from the authors will help contextualize the work, though once again I think showing this selective underperformance on control tasks would make the work considerably stronger and more contextualized.**\nQuestions: - How are the safety-specific implications of the findings of sandbagging separate from the general issue that \""if a model is prompted to fail, it might fail\"" or \""if a model is trained to fail, it might fail\"" on general tasks?\n- How do you think common sense reasoning literature fits in here?\n\n\n\n"", ""id"": ""uvvVjWP1aj2"", ""pdf"": ""openreview.net/pdf?id=uvvVjWP1aj""}","26"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""Lack of clear differentiation: While the authors mention TaI-DPT and claim differences, the paper does not clearly articulate the advantages of the proposed method over TaI-DPT, leaving the comparative benefits ambiguous."", ""Target claims"": [1, 2], ""Reasoning"": ""The reviewer argues that the paper does not effectively differentiate its methods from TaI-DPT. Claim1 and Claim2 both state that the proposed methods yield better performance than TaI-DPT and work synergetically with it, which are questioned by the review's mention of unclear articulation of these advantages.""}, {""Weakness span"": ""Unfair comparison in Table 5: The Class Concept Representation is based on the MS-COCO dataset, making direct comparisons with the baseline CLIP method potentially unfair due to inherent advantages provided by the dataset-specific information."", ""Target claims"": [], ""Reasoning"": ""The reviewer suggests that the comparisons made with the baseline CLIP method might not be entirely fair due to the dataset advantages used in the method. While no specific claim directly states this comparison on the dataset used, the validity of results implicitly affects claims related to performance improvements (e.g., Claim1, Claim19, etc.). However, the direct target is not explicitly stated in the claims.""}]}, ""meta"": {""id"": ""up0qqbdeQu"", ""review_idx"": 0, ""title"": ""Title: Class Concept Representation from Contextual Texts for Training-Free Multi-Label Recognition\nAbstract: The power of large vision-language models (VLMs) has been demonstrated for diverse vision tasks including multi-label recognition with training-free approach or prompt tuning by measuring the cosine similarity between the text features related to class names and the visual features of images. Prior works usually formed the class-related text features by averaging simple hand-crafted text prompts with class names (e.g., ``a photo of {class name}''). However, they may not fully exploit the capability of VLMs considering how humans form the concepts on words using rich contexts with the patterns of co-occurrence with other words. Inspired by that, we propose class concept representation for zero-shot multi-label recognition to better exploit rich contexts in the massive descriptions on images (e.g., captions from MS-COCO) using large VLMs. Then, for better aligning visual features of VLMs to our class concept representation, we propose context-guided visual representation that is in the same linear space as class concept representation. Experimental results on diverse benchmarks show that our proposed methods substantially improved the performance of zero-shot methods like Zero-Shot CLIP and yielded better performance than zero-shot prompt tunings that require additional training like TaI-DPT. In addition, our proposed methods can synergetically work with existing prompt tuning methods, consistently improving the performance of DualCoOp and TaI-DPT in a training-free manner with negligible increase in inference time.\n\nTLDR:Inspired by cognitive neuroscience perspective on forming concepts for words, we propose class concept representation for exploiting vision-language model for training-free multi-label recognition.\n\nKeywords:Training-free Multi-Label Recognition, Vision-Language Model, Class Concept, Context-Guided Visual Feature"", ""claims"": [""Claim1: Experimental results on diverse benchmarks show that our proposed methods substantially improved the performance of zero-shot methods like Zero-Shot CLIP and yielded better performance than zero-shot prompt tunings that require additional training like TaI-DPT."", ""Claim2: In addition, our proposed methods can synergetically work with existing prompt tuning methods, consistently improving the performance of DualCoOp and TaI-DPT in a training-free manner with negligible increase in inference time."", ""Claim3: Recent advances of large vision-language models (VLMs) has demonstrated their strong transferability on various downstream tasks with great performance."", ""Claim4: In multi-label recognition, prompt tuning with CLIP has been investigated for distinguishing multiple objects in an image, mitigating the difficulty of acquiring annotated samples."", ""Claim5: Prompt ensembles by averaging text features from simple hand-crafted prompts yielded marginal improvements and struggled with multi-label recognition."", ""Claim6: We propose a novel approach of exploiting VLMs for multi-label recognition by replacing single class name -related hand-crafted prompts with our proposed class concept representation using text descriptions capturing rich contextual information with target class names as well as related words."", ""Claim7: Our proposed methods achieved improved performance on multiple benchmark datasets without additional training (tuning), without additional labels (text-image pairs) and with negligible increase in inference time."", ""Claim8: Even though recent works has demonstrated outstanding performance of multi-label recognition task, they still require tuning costs or labeled dataset to adapt pre-trained CLIP to various downstream tasks."", ""Claim9: In this work, our method enables training-free and label-free adaptation of CLIP into downstream tasks, utilizing the text descriptions."", ""Claim10: In multi-label recognition, our method enables training-free enhancement and demonstrated its effectiveness on the benchmark dataset."", ""Claim11: Our proposed methods are label-free as well as training-free so that they can be applicable synergetically for most existing VLM-based multi-label recognition methods."", ""Claim12: In this work, we propose to use these descriptions to form concepts on class names to compare with images, so that ways of using them are completely different."", ""Claim13: In our experiments, we demonstrate the effectiveness of integrating of our method with existing methods, thereby boosting the performance of multi-label recognition."", ""Claim14: To evaluate our method, we conducted the experiments in limited data scenarios, including zero-shot and few-shot settings for data-limited cases and partially labeled setting for label-limited cases."", ""Claim15: Therefore, our method can be easily combined with existing methods to improve their performance."", ""Claim16: To evaluate the effectiveness of our method, we combined our method with existing zero-shot methods, ZSCLIP[32] and TaI-DPT [18], for zero-shot setting, as shown in Table 1."", ""Claim17: By applying our method to ZSCLIP[32] and TaI-DPT [18] during inference, we yield performance improvements without tuning costs."", ""Claim18: Especially, the performance of ZSCLIP[32] with our method is notably enhanced, achieving better and comparable performance to TaI-DPT [18], which requires mild tuning."", ""Claim19: Our method exceeds the performance of ZSCLIP[32] and TaI-DPT [18] by a large margin, with improvements of 9.3 mAP and 2.6 mAP, respectively."", ""Claim20: As a result, our method significantly improved the ZSL and GZSL performance of the supervised DualCoOp[37] by providing complementary information."", ""Claim21: Table 1 and Table 2 demonstrate the effectiveness of our method performing training-free enhancement of CLIP with only text descriptions that are easily obtained."", ""Claim22: By applying our method with existing zero-shot methods (ZSCLIP[32] and TaI-DPT [18]), we consistently enhance performance, as already demonstrated in a zero-shot setting."", ""Claim23: In the absence of labeled samples and tuning, we achieved comparable performance with ML-FSL[35] and better results than other few-shot methods utilizing 5-shot samples."", ""Claim24: As shown in Table 4, our method supplements the decreased performance of DualCoOp [37] caused by partially labeled samples by providing complementary information during inference."", ""Claim25: Through zero-shot test time task adaptation without tuning costs, we consistently enhance the performance of DualCoOp [37] on all benchmark dataset."", ""Claim26: Furthermore, we achieved the performance of DualCoOp [37] trained with 90% labels by applying our method with DualCoOp trained with 60% labels from MS-COCO [26], 50% labels from VOC2007 [16], and 70% labels from NUSWIDE [12]."", ""Claim27: Thus, the performance is increased by 4.1 mAP and 1.1 mAP on MS-COCO [26] and VOC2007 [16], respectively."", ""Claim28: Constructing context-guided visual feature, our method yield remarkable performance gain by 8.5 mAP and 5.3 mAP on MS-COCO [26] and VOC2007 [16], respectively."", ""Claim29: When the number of texts increases from 1K to 32K, integrating ZSCLIP[32] with our method only increases the inference time by 0.4-0.5ms, with tests conducted on the RTX3090."", ""Claim30: In addition, Our method (6.8GB) requires slightly more memory than ZSCLIP (6.5GB) on VOC2007 [16]."", ""Claim31: Remarkably, our proposed method outperforms zero-shot prompt tuning methods such as TaI-DPT and achieves significant enhancements over ZSCLIP and other state-of-the-art prompt tuning methods without requiring parameter tuning or labeled samples, and with minimal inference time overhead."", ""Claim32: While our method achieved impressive results with training-free enhancement of CLIP, it exhibits limitations."", ""Claim33: First, a significant performance gap exists compared to prompt tuning methods with full samples, like DualCoOp [37]."", ""Claim34: Therefore, we utilized the generated text descriptions from large language model."", ""Claim35: In Fig. 5, we visualized the examples of local alignment enhancement by applying our method."", ""Claim36: Our proposed method enhances the local alignment between the visual features of test image and the text features of each class name, thereby suppressing the false-positive prediction."", ""Claim37: Therefore, Fig. 5 demonstrates the effectiveness of our method."", ""Claim38: We provide the details of the used model, hyperparameters, source of datasets and proposed algorithm for reproducing main experimental results."", ""Claim39: We provide the details of the experimental setting, including hyperparameters, in Sections 4.1 and C."", ""Claim40: We provide the information of types of compute worker (GPU model), memory usage and inference time in Section. 4.3.3."", ""Claim41: We abide by the NeurIPS Code of Ethics."", ""Claim42: We discuss the positive and negative societal impacts of our paper in the supplementary material."", ""Claim43: Our paper does not pose a high risk for misuse in terms of model and dataset."", ""Claim44: We cite the papers that provide datasets, code and models in the Section. 4."", ""Claim45: We provide the detail documentation for the code in our submission.""], ""review"": ""Review: Weakness: 1. Lack of clear differentiation: While the authors mention TaI-DPT and claim differences, the paper does not clearly articulate the advantages of the proposed method over TaI-DPT, leaving the comparative benefits ambiguous.\n2. Unfair comparison in Table 5: The Class Concept Representation is based on the MS-COCO dataset, making direct comparisons with the baseline CLIP method potentially unfair due to inherent advantages provided by the dataset-specific information.\n\n3. Limited model implementation: The paper only implements the ResNet-based CLIP model and does not explore transformer-based CLIP models. It is unclear whether the method is ineffective for transformer-based models or if there are specific reasons behind this omission. This limits the generalizability of the findings.\n4. Ambiguous terminology: The paper uses the term \""training-free\"" in its title, yet it describes the approach as \""test-time adaptation\"" within the content. This inconsistency can lead to confusion about the nature of the proposed method.\n5. Dependent on source text description: It seems that text description source need to be carefully selected.  A comparison of different description dataset can be interesting.\nQuestions: 1. Confusing softmax explanation: The description of softmax values in Figure 3 is unclear. Typically, in CLIP, text-image similarity is calculated using cosine similarity, with softmax applied separately to each modality. The paper does not adequately explain why softmax is necessary between image and text similarities in this context.\n2. I am curious about the reasons behind the performance improvement observed when switching from default prompts to class concept representation. Is the enhancement due to the use of phrase-level embeddings being more effective than word-level embeddings? Can you provide any insights or explanations for this improvement?\n\n""}, ""review"": ""Title: Class Concept Representation from Contextual Texts for Training-Free Multi-Label Recognition\nAbstract: The power of large vision-language models (VLMs) has been demonstrated for diverse vision tasks including multi-label recognition with training-free approach or prompt tuning by measuring the cosine similarity between the text features related to class names and the visual features of images. Prior works usually formed the class-related text features by averaging simple hand-crafted text prompts with class names (e.g., ``a photo of {class name}''). However, they may not fully exploit the capability of VLMs considering how humans form the concepts on words using rich contexts with the patterns of co-occurrence with other words. Inspired by that, we propose class concept representation for zero-shot multi-label recognition to better exploit rich contexts in the massive descriptions on images (e.g., captions from MS-COCO) using large VLMs. Then, for better aligning visual features of VLMs to our class concept representation, we propose context-guided visual representation that is in the same linear space as class concept representation. Experimental results on diverse benchmarks show that our proposed methods substantially improved the performance of zero-shot methods like Zero-Shot CLIP and yielded better performance than zero-shot prompt tunings that require additional training like TaI-DPT. In addition, our proposed methods can synergetically work with existing prompt tuning methods, consistently improving the performance of DualCoOp and TaI-DPT in a training-free manner with negligible increase in inference time.\n\nClaim1: Experimental results on diverse benchmarks show that our proposed methods substantially improved the performance of zero-shot methods like Zero-Shot CLIP and yielded better performance than zero-shot prompt tunings that require additional training like TaI-DPT.\nClaim2: In addition, our proposed methods can synergetically work with existing prompt tuning methods, consistently improving the performance of DualCoOp and TaI-DPT in a training-free manner with negligible increase in inference time.\nClaim3: Recent advances of large vision-language models (VLMs) has demonstrated their strong transferability on various downstream tasks with great performance.\nClaim4: In multi-label recognition, prompt tuning with CLIP has been investigated for distinguishing multiple objects in an image, mitigating the difficulty of acquiring annotated samples.\nClaim5: Prompt ensembles by averaging text features from simple hand-crafted prompts yielded marginal improvements and struggled with multi-label recognition.\nClaim6: We propose a novel approach of exploiting VLMs for multi-label recognition by replacing single class name -related hand-crafted prompts with our proposed class concept representation using text descriptions capturing rich contextual information with target class names as well as related words.\nClaim7: Our proposed methods achieved improved performance on multiple benchmark datasets without additional training (tuning), without additional labels (text-image pairs) and with negligible increase in inference time.\nClaim8: Even though recent works has demonstrated outstanding performance of multi-label recognition task, they still require tuning costs or labeled dataset to adapt pre-trained CLIP to various downstream tasks.\nClaim9: In this work, our method enables training-free and label-free adaptation of CLIP into downstream tasks, utilizing the text descriptions.\nClaim10: In multi-label recognition, our method enables training-free enhancement and demonstrated its effectiveness on the benchmark dataset.\nClaim11: Our proposed methods are label-free as well as training-free so that they can be applicable synergetically for most existing VLM-based multi-label recognition methods.\nClaim12: In this work, we propose to use these descriptions to form concepts on class names to compare with images, so that ways of using them are completely different.\nClaim13: In our experiments, we demonstrate the effectiveness of integrating of our method with existing methods, thereby boosting the performance of multi-label recognition.\nClaim14: To evaluate our method, we conducted the experiments in limited data scenarios, including zero-shot and few-shot settings for data-limited cases and partially labeled setting for label-limited cases.\nClaim15: Therefore, our method can be easily combined with existing methods to improve their performance.\nClaim16: To evaluate the effectiveness of our method, we combined our method with existing zero-shot methods, ZSCLIP[32] and TaI-DPT [18], for zero-shot setting, as shown in Table 1.\nClaim17: By applying our method to ZSCLIP[32] and TaI-DPT [18] during inference, we yield performance improvements without tuning costs.\nClaim18: Especially, the performance of ZSCLIP[32] with our method is notably enhanced, achieving better and comparable performance to TaI-DPT [18], which requires mild tuning.\nClaim19: Our method exceeds the performance of ZSCLIP[32] and TaI-DPT [18] by a large margin, with improvements of 9.3 mAP and 2.6 mAP, respectively.\nClaim20: As a result, our method significantly improved the ZSL and GZSL performance of the supervised DualCoOp[37] by providing complementary information.\nClaim21: Table 1 and Table 2 demonstrate the effectiveness of our method performing training-free enhancement of CLIP with only text descriptions that are easily obtained.\nClaim22: By applying our method with existing zero-shot methods (ZSCLIP[32] and TaI-DPT [18]), we consistently enhance performance, as already demonstrated in a zero-shot setting.\nClaim23: In the absence of labeled samples and tuning, we achieved comparable performance with ML-FSL[35] and better results than other few-shot methods utilizing 5-shot samples.\nClaim24: As shown in Table 4, our method supplements the decreased performance of DualCoOp [37] caused by partially labeled samples by providing complementary information during inference.\nClaim25: Through zero-shot test time task adaptation without tuning costs, we consistently enhance the performance of DualCoOp [37] on all benchmark dataset.\nClaim26: Furthermore, we achieved the performance of DualCoOp [37] trained with 90% labels by applying our method with DualCoOp trained with 60% labels from MS-COCO [26], 50% labels from VOC2007 [16], and 70% labels from NUSWIDE [12].\nClaim27: Thus, the performance is increased by 4.1 mAP and 1.1 mAP on MS-COCO [26] and VOC2007 [16], respectively.\nClaim28: Constructing context-guided visual feature, our method yield remarkable performance gain by 8.5 mAP and 5.3 mAP on MS-COCO [26] and VOC2007 [16], respectively.\nClaim29: When the number of texts increases from 1K to 32K, integrating ZSCLIP[32] with our method only increases the inference time by 0.4-0.5ms, with tests conducted on the RTX3090.\nClaim30: In addition, Our method (6.8GB) requires slightly more memory than ZSCLIP (6.5GB) on VOC2007 [16].\nClaim31: Remarkably, our proposed method outperforms zero-shot prompt tuning methods such as TaI-DPT and achieves significant enhancements over ZSCLIP and other state-of-the-art prompt tuning methods without requiring parameter tuning or labeled samples, and with minimal inference time overhead.\nClaim32: While our method achieved impressive results with training-free enhancement of CLIP, it exhibits limitations.\nClaim33: First, a significant performance gap exists compared to prompt tuning methods with full samples, like DualCoOp [37].\nClaim34: Therefore, we utilized the generated text descriptions from large language model.\nClaim35: In Fig. 5, we visualized the examples of local alignment enhancement by applying our method.\nClaim36: Our proposed method enhances the local alignment between the visual features of test image and the text features of each class name, thereby suppressing the false-positive prediction.\nClaim37: Therefore, Fig. 5 demonstrates the effectiveness of our method.\nClaim38: We provide the details of the used model, hyperparameters, source of datasets and proposed algorithm for reproducing main experimental results.\nClaim39: We provide the details of the experimental setting, including hyperparameters, in Sections 4.1 and C.\nClaim40: We provide the information of types of compute worker (GPU model), memory usage and inference time in Section. 4.3.3.\nClaim41: We abide by the NeurIPS Code of Ethics.\nClaim42: We discuss the positive and negative societal impacts of our paper in the supplementary material.\nClaim43: Our paper does not pose a high risk for misuse in terms of model and dataset.\nClaim44: We cite the papers that provide datasets, code and models in the Section. 4.\nClaim45: We provide the detail documentation for the code in our submission.\n\nReview: Weakness: 1. Lack of clear differentiation: While the authors mention TaI-DPT and claim differences, the paper does not clearly articulate the advantages of the proposed method over TaI-DPT, leaving the comparative benefits ambiguous.\n2. Unfair comparison in Table 5: The Class Concept Representation is based on the MS-COCO dataset, making direct comparisons with the baseline CLIP method potentially unfair due to inherent advantages provided by the dataset-specific information.\n\n3. Limited model implementation: The paper only implements the ResNet-based CLIP model and does not explore transformer-based CLIP models. It is unclear whether the method is ineffective for transformer-based models or if there are specific reasons behind this omission. This limits the generalizability of the findings.\n4. Ambiguous terminology: The paper uses the term \""training-free\"" in its title, yet it describes the approach as \""test-time adaptation\"" within the content. This inconsistency can lead to confusion about the nature of the proposed method.\n5. Dependent on source text description: It seems that text description source need to be carefully selected.  A comparison of different description dataset can be interesting.\nQuestions: 1. Confusing softmax explanation: The description of softmax values in Figure 3 is unclear. Typically, in CLIP, text-image similarity is calculated using cosine similarity, with softmax applied separately to each modality. The paper does not adequately explain why softmax is necessary between image and text similarities in this context.\n2. I am curious about the reasons behind the performance improvement observed when switching from default prompts to class concept representation. Is the enhancement due to the use of phrase-level embeddings being more effective than word-level embeddings? Can you provide any insights or explanations for this improvement?\n\n\n\n"", ""id"": ""up0qqbdeQu0"", ""pdf"": ""openreview.net/pdf?id=up0qqbdeQu""}","27"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""The reviewer thinks that the most convincing results are the Top-1 Acc of existing OVAR models under the Real noise type. However, in Table 1, the proposed model does not demonstrate much superiority compared to GPT3.5's simple correction. The reviewer worries about the research significance of this problem. Will this problem be well resolved when using more powerful GPT4/GPT4o with some engineering prompt designs?"", ""Target claims"": [17, 19], ""Reasoning"": ""This weakness questions the claimed superiority of the proposed model (Claim17) and its robustness improvement under varying noise rates (Claim19). The review argues that the experimental results do not convincingly demonstrate a significant advantage in the model's performance compared to simpler methods like GPT3.5, and speculates that more powerful models (GPT4/GPT4o) might further overshadow the proposed model's performance, thus directly challenging the claimed advancements in robustness and effectiveness.""}, {""Weakness span"": ""In Table 2, I would like to see the performance of other correction methods (e.g., GPT3.5/4/4o) for a more comprehensive comparison."", ""Target claims"": [18, 19], ""Reasoning"": ""This comment suggests that the scalability and generalization performance claimed in Table 2 (Claim18) and under various noise rates (Claim19) lack a comprehensive evaluation, as other correction methods are not included in the comparisons. This directly questions whether the proposed model truly outperforms or scales better than existing methodologies when broader comparison models are considered.""}]}, ""meta"": {""id"": ""uS9RZH6K65"", ""review_idx"": 0, ""title"": ""Title: DENOISER: Rethinking the Robustness for Open-Vocabulary Action Recognition\nAbstract: As one of the fundamental video tasks in computer vision, Open-Vocabulary Action Recognition (OVAR) has recently gained increasing attention, with the development of vision-language pre-trainings. To enable open-vocabulary generalization, existing methods formulate vanilla OVAR to evaluate the embedding similarity between visual samples and text descriptions. However, one crucial issue is completely ignored: the text descriptions given by users may be noisy, e.g., misspellings and typos, limiting the real-world practicality. To fill the research gap, this paper analyzes the noise rate/type in text descriptions by full statistics of manual spelling; then reveals the poor robustness of existing methods; and finally rethinks to study a practical task: noisy OVAR. One novel DENOISER framework, covering two parts: generation and discrimination, is further proposed for solution. Concretely, the generative part denoises noisy text descriptions via a decoding process, i.e., proposes text candidates, then utilizes inter-modal and intra-modal information to vote for the best. At the discriminative part, we use vanilla OVAR models to assign visual samples to text descriptions, injecting more semantics. For optimization, we alternately iterate between generative-discriminative parts for progressive refinements. The denoised text descriptions help OVAR models classify visual samples more accurately; in return, assigned visual samples help better denoising. We carry out extensive experiments to show our superior robustness, and tho rough ablations to dissect the effectiveness of each component.\n\nTLDR:DENOISER: Rethinking the Robustness for Open-Vocabulary Action Recognition\n\nKeywords:Open-Vocabulary Action Recognition, Multi-Modal Pre-training, Multi-Modal Robust Learning"", ""claims"": [""Claim1: We carry out extensive experiments to show our superior robustness, and thorough ablations to dissect the effectiveness of each component."", ""Claim2: We are hence motivated to fill the research gap of noisy text descriptions in OVAR."", ""Claim3: Fig. 1 Right empirically evaluates noise hazards for existing OVAR methods [16, 49, 62]. One can find that just a small amount of noise lowers recognition accuracy by a large margin, implying quite poor robustness."", ""Claim4: To spur the community to deal with the noisy OVAR task, being necessary and practical, this paper bravely faces the challenges."", ""Claim5: Our main contributions are summarized as follows:"", ""Claim6: We pioneer taking noises from text descriptions (misspellings and typos) into consideration."", ""Claim7: By adding real-world noise for the above methods, we reveal their poor robustness, and design DENOISER for solution."", ""Claim8: We differ from them by considering robustness in the context of multi-modal scenarios and by employing multi-modal information to better assist text denoising."", ""Claim9: As a result, the noisy OVAR task can be formulated as: given V and T \u2032, the model is expected to maximize the accuracy of action recognition, and even recovering T \u2032 to T."", ""Claim10: Such poor robustness to the noisy OVAR task, proves excessive idealization of existing studies and also motivates us to fill the research gap."", ""Claim11: We carry out extensive experiments to show our superior robustness, and thorough ablations to dissect the effectiveness of each component."", ""Claim12: To illustrate the generalizability of our framework, we leverage two typical models from the VLA pipeline as \u03a6 OVAR, that is, ActionCLIP and XCLIP."", ""Claim13: To partition these datasets for open-vocabulary action recognition, this paper follows the standard consensus, for the sake of fairness."", ""Claim14: All ablation studies are conducted on UCF101 under 20% noise."", ""Claim15: We do each simulation 10 times and report the mean and confidence interval of 95%."", ""Claim16: We adopt two large-scale corpora of misspellings to analyze noise type in text descriptions."", ""Claim17: Our method outperforms all competitors by large margins, which is impressive because our method is unsupervised without prior knowledge other than those contained in the OVAR model."", ""Claim18: Tab. 2 compares Top-1 Acc to further reveal our solution is scalable/generalizable."", ""Claim19: Under various noise rates, our model is robust to achieve huge improvements."", ""Claim20: In terms of scalability across models, our method is not only applicable to hand-crafted prompts as in ActionCLIP but also to learnable visual-conditioned prompts as in XCLIP."", ""Claim21: Furthermore, we notice that, whenever XCLIP outperforms ActionCLIP, our method also yields a better result."", ""Claim22: A better visual encoder and well-tuned prompt may significantly increase our performance, showing that our method\u2019s upper limit could become higher, as the community continues to train better OVAR models."", ""Claim23: Tab. 3 shows that, both \u03a6 inter and \u03a6 intra contribute to denoising text descriptions and to improving the robustness of underlying \u03a6 OVAR."", ""Claim24: In terms of Top-1 Acc and Semantic Similarity, \u03a6 inter performs better than \u03a6 intra, since \u03a6 inter uses visual information as one direct optimization guideline to improve video recognition."", ""Claim25: While \u03a6 intra performs better in terms of Label Acc, which focuses more on spelling correctness."", ""Claim26: Besides, \u03a6 inter and \u03a6 intra turn out to be complementary: visual information helps to understand noisy text descriptions; while textual information prevents the model from being misled by visual samples."", ""Claim27: We achieve the best performance when combining these two weightings."", ""Claim28: Tab. 3 reports that it is beneficial to rely more on intra-modal at the beginning of decoding, and then gradually turn to inter-modal for more help."", ""Claim29: This indicates that, when text noises are high, \u03a6 intra offers more help; when text noises are slight, \u03a6 inter could help more."", ""Claim30: Fig. 4 Left reports our robustness under various noise types/rates."", ""Claim31: Our method shows remarkable resilience when texts are perturbed by inserting or substituting characters."", ""Claim32: Performance degradation is observed when texts are perturbed by deleting characters."", ""Claim33: We conduct extensive experiments to demonstrate the generalizability of DENOISER across multiple models and datasets, and also show our superiority over uni-modal spell-checking solutions."", ""Claim34: We find that adversarial training, though promising under closed-set scenarios in previous studies, is relatively ineffective under open-vocabulary settings."", ""Claim35: Specifically, training with more noise lowers significantly the model\u2019s performance under low noise rate."", ""Claim36: Additionally, its added value is limited under heavy noise rate."", ""Claim37: These phenomena are probably related to the domain gap between datasets."", ""Claim38: We conclude that noisy text descriptions are better solved in testing time rather than during training stage."", ""Claim39: Our DENOISER framework shows a significant advantage over the adversarial training."", ""Claim40: Our method shows a drop in performance when fewer visual samples are used in \u03a6 inter."", ""Claim41: The performance tends to converge towards that when solely \u03a6 intra is used."", ""Claim42: We hypothesize that fewer visual samples make \u03a6 inter harder to extract added value to \u03a6 intra."", ""Claim43: With the noise rate increasing, we find an increasingly large drop in performance, which shows conversely that \u03a6 inter is more important under large noise scenarios as textual information becomes more ambiguous and less informative."", ""Claim44: The Right shows that the denoised text descriptions improve the OVAR performance."", ""Claim45: Our DENOISER remains faithful in terms of spelling, e.g. , 'wal4ingm with a dog' \u2192 'walking with a dog' instead of 'dogwalking'."", ""Claim46: Our model requires a trade-off between computational cost and performance."", ""Claim47: As shown in Fig. 4 and Fig. 6, the performance of our DENOISER increases as the number of proposals K and the percentage of the visual samples used."", ""Claim48: DENOISER requires only simple operations for each iteration."", ""Claim49: Compared to other approaches that intend to align noisy text-image pairs or to train spell-checking models, DENOISER that denoises at evaluation time is extremely time-saving."", ""Claim50: We provide source code. Datasets publicly accessible."", ""Claim51: We specify all settings of experiments report confidence intervals."", ""Claim52: We report information of computer resources."", ""Claim53: We conduct in the paper conform, in every respect, with the NeurIPS Code of Ethics."", ""Claim54: Our model helps users better leverage the existing Open-Vocabulary models in a more robust way."", ""Claim55: All the assets are properly cited. License and terms of use are properly respected."", ""Claim56: We provided well-documented source code.""], ""review"": ""Review: Weakness: - About the experiments,\n    - The reviewer thinks that the most convincing results are the Top-1 Acc of existing OVAR models under the Real noise type. However, in Table 1, the proposed model does not demonstrate much superiority compared to GPT3.5's simple correction. The reviewer worries about the research significance of this problem. Will this problem be well resolved when using more powerful GPT4/GPT4o with some engineering prompt designs?\n    - In Table 2, I would like to see the performance of other correction methods (e.g., GPT3.5/4/4o) for a more comprehensive comparison.\n    - Since this work focuses on the noise text description problem in OVAR, it is necessary to demonstrate the results of those CLIP-based methods without any additional textual adaptation (e.g., the vanilla CLIP).\n\n\n- About the method,\n    - The reviewer thinks that the overall model design is reasonable and clear. However, the method part introduces too many symbols which makes the paper very hard to follow. It is unnecessary to over-decorate the technical contributions.\n\n- Minor issue,\n    - The authors seem to have a misunderstanding about the OVAR setting (Line 113). In OVAR, the model is evaluated on both base-classes and novel-classes during testing. In this case, all action classes from the UCF/HMDB datasets can be used for testing when the model is trained on K400, as there are many overlap classes between K400 and UCF/HMDB.\nQuestions: Please see the weaknesses section.\n\n""}, ""review"": ""Title: DENOISER: Rethinking the Robustness for Open-Vocabulary Action Recognition\nAbstract: As one of the fundamental video tasks in computer vision, Open-Vocabulary Action Recognition (OVAR) has recently gained increasing attention, with the development of vision-language pre-trainings. To enable open-vocabulary generalization, existing methods formulate vanilla OVAR to evaluate the embedding similarity between visual samples and text descriptions. However, one crucial issue is completely ignored: the text descriptions given by users may be noisy, e.g., misspellings and typos, limiting the real-world practicality. To fill the research gap, this paper analyzes the noise rate/type in text descriptions by full statistics of manual spelling; then reveals the poor robustness of existing methods; and finally rethinks to study a practical task: noisy OVAR. One novel DENOISER framework, covering two parts: generation and discrimination, is further proposed for solution. Concretely, the generative part denoises noisy text descriptions via a decoding process, i.e., proposes text candidates, then utilizes inter-modal and intra-modal information to vote for the best. At the discriminative part, we use vanilla OVAR models to assign visual samples to text descriptions, injecting more semantics. For optimization, we alternately iterate between generative-discriminative parts for progressive refinements. The denoised text descriptions help OVAR models classify visual samples more accurately; in return, assigned visual samples help better denoising. We carry out extensive experiments to show our superior robustness, and tho rough ablations to dissect the effectiveness of each component.\n\nClaim1: We carry out extensive experiments to show our superior robustness, and thorough ablations to dissect the effectiveness of each component.\nClaim2: We are hence motivated to fill the research gap of noisy text descriptions in OVAR.\nClaim3: Fig. 1 Right empirically evaluates noise hazards for existing OVAR methods [16, 49, 62]. One can find that just a small amount of noise lowers recognition accuracy by a large margin, implying quite poor robustness.\nClaim4: To spur the community to deal with the noisy OVAR task, being necessary and practical, this paper bravely faces the challenges.\nClaim5: Our main contributions are summarized as follows:\nClaim6: We pioneer taking noises from text descriptions (misspellings and typos) into consideration.\nClaim7: By adding real-world noise for the above methods, we reveal their poor robustness, and design DENOISER for solution.\nClaim8: We differ from them by considering robustness in the context of multi-modal scenarios and by employing multi-modal information to better assist text denoising.\nClaim9: As a result, the noisy OVAR task can be formulated as: given V and T \u2032, the model is expected to maximize the accuracy of action recognition, and even recovering T \u2032 to T.\nClaim10: Such poor robustness to the noisy OVAR task, proves excessive idealization of existing studies and also motivates us to fill the research gap.\nClaim11: We carry out extensive experiments to show our superior robustness, and thorough ablations to dissect the effectiveness of each component.\nClaim12: To illustrate the generalizability of our framework, we leverage two typical models from the VLA pipeline as \u03a6 OVAR, that is, ActionCLIP and XCLIP.\nClaim13: To partition these datasets for open-vocabulary action recognition, this paper follows the standard consensus, for the sake of fairness.\nClaim14: All ablation studies are conducted on UCF101 under 20% noise.\nClaim15: We do each simulation 10 times and report the mean and confidence interval of 95%.\nClaim16: We adopt two large-scale corpora of misspellings to analyze noise type in text descriptions.\nClaim17: Our method outperforms all competitors by large margins, which is impressive because our method is unsupervised without prior knowledge other than those contained in the OVAR model.\nClaim18: Tab. 2 compares Top-1 Acc to further reveal our solution is scalable/generalizable.\nClaim19: Under various noise rates, our model is robust to achieve huge improvements.\nClaim20: In terms of scalability across models, our method is not only applicable to hand-crafted prompts as in ActionCLIP but also to learnable visual-conditioned prompts as in XCLIP.\nClaim21: Furthermore, we notice that, whenever XCLIP outperforms ActionCLIP, our method also yields a better result.\nClaim22: A better visual encoder and well-tuned prompt may significantly increase our performance, showing that our method\u2019s upper limit could become higher, as the community continues to train better OVAR models.\nClaim23: Tab. 3 shows that, both \u03a6 inter and \u03a6 intra contribute to denoising text descriptions and to improving the robustness of underlying \u03a6 OVAR.\nClaim24: In terms of Top-1 Acc and Semantic Similarity, \u03a6 inter performs better than \u03a6 intra, since \u03a6 inter uses visual information as one direct optimization guideline to improve video recognition.\nClaim25: While \u03a6 intra performs better in terms of Label Acc, which focuses more on spelling correctness.\nClaim26: Besides, \u03a6 inter and \u03a6 intra turn out to be complementary: visual information helps to understand noisy text descriptions; while textual information prevents the model from being misled by visual samples.\nClaim27: We achieve the best performance when combining these two weightings.\nClaim28: Tab. 3 reports that it is beneficial to rely more on intra-modal at the beginning of decoding, and then gradually turn to inter-modal for more help.\nClaim29: This indicates that, when text noises are high, \u03a6 intra offers more help; when text noises are slight, \u03a6 inter could help more.\nClaim30: Fig. 4 Left reports our robustness under various noise types/rates.\nClaim31: Our method shows remarkable resilience when texts are perturbed by inserting or substituting characters.\nClaim32: Performance degradation is observed when texts are perturbed by deleting characters.\nClaim33: We conduct extensive experiments to demonstrate the generalizability of DENOISER across multiple models and datasets, and also show our superiority over uni-modal spell-checking solutions.\nClaim34: We find that adversarial training, though promising under closed-set scenarios in previous studies, is relatively ineffective under open-vocabulary settings.\nClaim35: Specifically, training with more noise lowers significantly the model\u2019s performance under low noise rate.\nClaim36: Additionally, its added value is limited under heavy noise rate.\nClaim37: These phenomena are probably related to the domain gap between datasets.\nClaim38: We conclude that noisy text descriptions are better solved in testing time rather than during training stage.\nClaim39: Our DENOISER framework shows a significant advantage over the adversarial training.\nClaim40: Our method shows a drop in performance when fewer visual samples are used in \u03a6 inter.\nClaim41: The performance tends to converge towards that when solely \u03a6 intra is used.\nClaim42: We hypothesize that fewer visual samples make \u03a6 inter harder to extract added value to \u03a6 intra.\nClaim43: With the noise rate increasing, we find an increasingly large drop in performance, which shows conversely that \u03a6 inter is more important under large noise scenarios as textual information becomes more ambiguous and less informative.\nClaim44: The Right shows that the denoised text descriptions improve the OVAR performance.\nClaim45: Our DENOISER remains faithful in terms of spelling, e.g. , 'wal4ingm with a dog' \u2192 'walking with a dog' instead of 'dogwalking'.\nClaim46: Our model requires a trade-off between computational cost and performance.\nClaim47: As shown in Fig. 4 and Fig. 6, the performance of our DENOISER increases as the number of proposals K and the percentage of the visual samples used.\nClaim48: DENOISER requires only simple operations for each iteration.\nClaim49: Compared to other approaches that intend to align noisy text-image pairs or to train spell-checking models, DENOISER that denoises at evaluation time is extremely time-saving.\nClaim50: We provide source code. Datasets publicly accessible.\nClaim51: We specify all settings of experiments report confidence intervals.\nClaim52: We report information of computer resources.\nClaim53: We conduct in the paper conform, in every respect, with the NeurIPS Code of Ethics.\nClaim54: Our model helps users better leverage the existing Open-Vocabulary models in a more robust way.\nClaim55: All the assets are properly cited. License and terms of use are properly respected.\nClaim56: We provided well-documented source code.\n\nReview: Weakness: - About the experiments,\n    - The reviewer thinks that the most convincing results are the Top-1 Acc of existing OVAR models under the Real noise type. However, in Table 1, the proposed model does not demonstrate much superiority compared to GPT3.5's simple correction. The reviewer worries about the research significance of this problem. Will this problem be well resolved when using more powerful GPT4/GPT4o with some engineering prompt designs?\n    - In Table 2, I would like to see the performance of other correction methods (e.g., GPT3.5/4/4o) for a more comprehensive comparison.\n    - Since this work focuses on the noise text description problem in OVAR, it is necessary to demonstrate the results of those CLIP-based methods without any additional textual adaptation (e.g., the vanilla CLIP).\n\n\n- About the method,\n    - The reviewer thinks that the overall model design is reasonable and clear. However, the method part introduces too many symbols which makes the paper very hard to follow. It is unnecessary to over-decorate the technical contributions.\n\n- Minor issue,\n    - The authors seem to have a misunderstanding about the OVAR setting (Line 113). In OVAR, the model is evaluated on both base-classes and novel-classes during testing. In this case, all action classes from the UCF/HMDB datasets can be used for testing when the model is trained on K400, as there are many overlap classes between K400 and UCF/HMDB.\nQuestions: Please see the weaknesses section.\n\n\n\n"", ""id"": ""uS9RZH6K650"", ""pdf"": ""openreview.net/pdf?id=uS9RZH6K65""}","28"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""In Table 1, it is evident that the proposed method outperforms GPT-3.5. Additionally, the authors present examples in Table 4 to demonstrate the superiority of the proposed method over GPT-3.5. However, upon personal experimentation with all the examples from Table 4 using the provided prompt from the paper (lines 243-245), I observed that the GPT-3.5 model successfully rectified all issues, including challenging cases where the proposed method fell short. As a result, I remain unconvinced by the findings."", ""Target claims"": [17, 19, 31, 44], ""Reasoning"": ""This weakness questions the findings related to the superiority and robustness of the proposed method, DENOISER, over GPT-3.5 (Claim17: 'Our method outperforms all competitors by large margins'; Claim19: 'Under various noise rates, our model is robust to achieve huge improvements'). Additionally, the reviewer mentions that the GPT-3.5 model successfully handled all challenges from the examples in the paper, which contradicts claims about DENOISER's advantages (Claim31: 'Our method shows remarkable resilience when texts are perturbed'; Claim44: 'The Right shows that the denoised text descriptions improve the OVAR performance'). Thus, the reviewer is not convinced by these claims.""}]}, ""meta"": {""id"": ""uS9RZH6K65"", ""review_idx"": 1, ""title"": ""Title: DENOISER: Rethinking the Robustness for Open-Vocabulary Action Recognition\nAbstract: As one of the fundamental video tasks in computer vision, Open-Vocabulary Action Recognition (OVAR) has recently gained increasing attention, with the development of vision-language pre-trainings. To enable open-vocabulary generalization, existing methods formulate vanilla OVAR to evaluate the embedding similarity between visual samples and text descriptions. However, one crucial issue is completely ignored: the text descriptions given by users may be noisy, e.g., misspellings and typos, limiting the real-world practicality. To fill the research gap, this paper analyzes the noise rate/type in text descriptions by full statistics of manual spelling; then reveals the poor robustness of existing methods; and finally rethinks to study a practical task: noisy OVAR. One novel DENOISER framework, covering two parts: generation and discrimination, is further proposed for solution. Concretely, the generative part denoises noisy text descriptions via a decoding process, i.e., proposes text candidates, then utilizes inter-modal and intra-modal information to vote for the best. At the discriminative part, we use vanilla OVAR models to assign visual samples to text descriptions, injecting more semantics. For optimization, we alternately iterate between generative-discriminative parts for progressive refinements. The denoised text descriptions help OVAR models classify visual samples more accurately; in return, assigned visual samples help better denoising. We carry out extensive experiments to show our superior robustness, and tho rough ablations to dissect the effectiveness of each component.\n\nTLDR:DENOISER: Rethinking the Robustness for Open-Vocabulary Action Recognition\n\nKeywords:Open-Vocabulary Action Recognition, Multi-Modal Pre-training, Multi-Modal Robust Learning"", ""claims"": [""Claim1: We carry out extensive experiments to show our superior robustness, and thorough ablations to dissect the effectiveness of each component."", ""Claim2: We are hence motivated to fill the research gap of noisy text descriptions in OVAR."", ""Claim3: Fig. 1 Right empirically evaluates noise hazards for existing OVAR methods [16, 49, 62]. One can find that just a small amount of noise lowers recognition accuracy by a large margin, implying quite poor robustness."", ""Claim4: To spur the community to deal with the noisy OVAR task, being necessary and practical, this paper bravely faces the challenges."", ""Claim5: Our main contributions are summarized as follows:"", ""Claim6: We pioneer taking noises from text descriptions (misspellings and typos) into consideration."", ""Claim7: By adding real-world noise for the above methods, we reveal their poor robustness, and design DENOISER for solution."", ""Claim8: We differ from them by considering robustness in the context of multi-modal scenarios and by employing multi-modal information to better assist text denoising."", ""Claim9: As a result, the noisy OVAR task can be formulated as: given V and T \u2032, the model is expected to maximize the accuracy of action recognition, and even recovering T \u2032 to T."", ""Claim10: Such poor robustness to the noisy OVAR task, proves excessive idealization of existing studies and also motivates us to fill the research gap."", ""Claim11: We carry out extensive experiments to show our superior robustness, and thorough ablations to dissect the effectiveness of each component."", ""Claim12: To illustrate the generalizability of our framework, we leverage two typical models from the VLA pipeline as \u03a6 OVAR, that is, ActionCLIP and XCLIP."", ""Claim13: To partition these datasets for open-vocabulary action recognition, this paper follows the standard consensus, for the sake of fairness."", ""Claim14: All ablation studies are conducted on UCF101 under 20% noise."", ""Claim15: We do each simulation 10 times and report the mean and confidence interval of 95%."", ""Claim16: We adopt two large-scale corpora of misspellings to analyze noise type in text descriptions."", ""Claim17: Our method outperforms all competitors by large margins, which is impressive because our method is unsupervised without prior knowledge other than those contained in the OVAR model."", ""Claim18: Tab. 2 compares Top-1 Acc to further reveal our solution is scalable/generalizable."", ""Claim19: Under various noise rates, our model is robust to achieve huge improvements."", ""Claim20: In terms of scalability across models, our method is not only applicable to hand-crafted prompts as in ActionCLIP but also to learnable visual-conditioned prompts as in XCLIP."", ""Claim21: Furthermore, we notice that, whenever XCLIP outperforms ActionCLIP, our method also yields a better result."", ""Claim22: A better visual encoder and well-tuned prompt may significantly increase our performance, showing that our method\u2019s upper limit could become higher, as the community continues to train better OVAR models."", ""Claim23: Tab. 3 shows that, both \u03a6 inter and \u03a6 intra contribute to denoising text descriptions and to improving the robustness of underlying \u03a6 OVAR."", ""Claim24: In terms of Top-1 Acc and Semantic Similarity, \u03a6 inter performs better than \u03a6 intra, since \u03a6 inter uses visual information as one direct optimization guideline to improve video recognition."", ""Claim25: While \u03a6 intra performs better in terms of Label Acc, which focuses more on spelling correctness."", ""Claim26: Besides, \u03a6 inter and \u03a6 intra turn out to be complementary: visual information helps to understand noisy text descriptions; while textual information prevents the model from being misled by visual samples."", ""Claim27: We achieve the best performance when combining these two weightings."", ""Claim28: Tab. 3 reports that it is beneficial to rely more on intra-modal at the beginning of decoding, and then gradually turn to inter-modal for more help."", ""Claim29: This indicates that, when text noises are high, \u03a6 intra offers more help; when text noises are slight, \u03a6 inter could help more."", ""Claim30: Fig. 4 Left reports our robustness under various noise types/rates."", ""Claim31: Our method shows remarkable resilience when texts are perturbed by inserting or substituting characters."", ""Claim32: Performance degradation is observed when texts are perturbed by deleting characters."", ""Claim33: We conduct extensive experiments to demonstrate the generalizability of DENOISER across multiple models and datasets, and also show our superiority over uni-modal spell-checking solutions."", ""Claim34: We find that adversarial training, though promising under closed-set scenarios in previous studies, is relatively ineffective under open-vocabulary settings."", ""Claim35: Specifically, training with more noise lowers significantly the model\u2019s performance under low noise rate."", ""Claim36: Additionally, its added value is limited under heavy noise rate."", ""Claim37: These phenomena are probably related to the domain gap between datasets."", ""Claim38: We conclude that noisy text descriptions are better solved in testing time rather than during training stage."", ""Claim39: Our DENOISER framework shows a significant advantage over the adversarial training."", ""Claim40: Our method shows a drop in performance when fewer visual samples are used in \u03a6 inter."", ""Claim41: The performance tends to converge towards that when solely \u03a6 intra is used."", ""Claim42: We hypothesize that fewer visual samples make \u03a6 inter harder to extract added value to \u03a6 intra."", ""Claim43: With the noise rate increasing, we find an increasingly large drop in performance, which shows conversely that \u03a6 inter is more important under large noise scenarios as textual information becomes more ambiguous and less informative."", ""Claim44: The Right shows that the denoised text descriptions improve the OVAR performance."", ""Claim45: Our DENOISER remains faithful in terms of spelling, e.g. , 'wal4ingm with a dog' \u2192 'walking with a dog' instead of 'dogwalking'."", ""Claim46: Our model requires a trade-off between computational cost and performance."", ""Claim47: As shown in Fig. 4 and Fig. 6, the performance of our DENOISER increases as the number of proposals K and the percentage of the visual samples used."", ""Claim48: DENOISER requires only simple operations for each iteration."", ""Claim49: Compared to other approaches that intend to align noisy text-image pairs or to train spell-checking models, DENOISER that denoises at evaluation time is extremely time-saving."", ""Claim50: We provide source code. Datasets publicly accessible."", ""Claim51: We specify all settings of experiments report confidence intervals."", ""Claim52: We report information of computer resources."", ""Claim53: We conduct in the paper conform, in every respect, with the NeurIPS Code of Ethics."", ""Claim54: Our model helps users better leverage the existing Open-Vocabulary models in a more robust way."", ""Claim55: All the assets are properly cited. License and terms of use are properly respected."", ""Claim56: We provided well-documented source code.""], ""review"": ""Review: Weakness: - The baseline models are outdated and not tailored for OVAR. The authors failed to reference recent OVAR papers such as OpenVCLIP[1] (ICML 2023), FROSTER (ICLR 2024), and OTI (ACM MM 2023).\n\n- In Table 1, it is evident that the proposed method outperforms GPT-3.5. Additionally, the authors present examples in Table 4 to demonstrate the superiority of the proposed method over GPT-3.5. However, upon personal experimentation with all the examples from Table 4 using the provided prompt from the paper (lines 243-245), I observed that the GPT-3.5 model successfully rectified all issues, including challenging cases where the proposed method fell short. As a result, I remain unconvinced by the findings.\n\nThis is the prompt given to GPT-3.5 model, and I hope other reviewers can also try it on their own:\n\nThe following words may contain spelling errors by deleting, inserting, and substituting letters. You are a corrector of spelling errors. Give only the answer without explication. What is the correct spelling of the action of  \u201ccutting i aitnchen\u201d.\n\n\n[1] Open-VCLIP: Transforming CLIP to an Open-vocabulary Video Model via Interpolated Weight Optimization.\n\n[2] FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition.\n\n[3] Orthogonal Temporal Interpolation for Zero-Shot Video Recognition.\nQuestions: Please refer to the weaknesses.\n\n""}, ""review"": ""Title: DENOISER: Rethinking the Robustness for Open-Vocabulary Action Recognition\nAbstract: As one of the fundamental video tasks in computer vision, Open-Vocabulary Action Recognition (OVAR) has recently gained increasing attention, with the development of vision-language pre-trainings. To enable open-vocabulary generalization, existing methods formulate vanilla OVAR to evaluate the embedding similarity between visual samples and text descriptions. However, one crucial issue is completely ignored: the text descriptions given by users may be noisy, e.g., misspellings and typos, limiting the real-world practicality. To fill the research gap, this paper analyzes the noise rate/type in text descriptions by full statistics of manual spelling; then reveals the poor robustness of existing methods; and finally rethinks to study a practical task: noisy OVAR. One novel DENOISER framework, covering two parts: generation and discrimination, is further proposed for solution. Concretely, the generative part denoises noisy text descriptions via a decoding process, i.e., proposes text candidates, then utilizes inter-modal and intra-modal information to vote for the best. At the discriminative part, we use vanilla OVAR models to assign visual samples to text descriptions, injecting more semantics. For optimization, we alternately iterate between generative-discriminative parts for progressive refinements. The denoised text descriptions help OVAR models classify visual samples more accurately; in return, assigned visual samples help better denoising. We carry out extensive experiments to show our superior robustness, and tho rough ablations to dissect the effectiveness of each component.\n\nClaim1: We carry out extensive experiments to show our superior robustness, and thorough ablations to dissect the effectiveness of each component.\nClaim2: We are hence motivated to fill the research gap of noisy text descriptions in OVAR.\nClaim3: Fig. 1 Right empirically evaluates noise hazards for existing OVAR methods [16, 49, 62]. One can find that just a small amount of noise lowers recognition accuracy by a large margin, implying quite poor robustness.\nClaim4: To spur the community to deal with the noisy OVAR task, being necessary and practical, this paper bravely faces the challenges.\nClaim5: Our main contributions are summarized as follows:\nClaim6: We pioneer taking noises from text descriptions (misspellings and typos) into consideration.\nClaim7: By adding real-world noise for the above methods, we reveal their poor robustness, and design DENOISER for solution.\nClaim8: We differ from them by considering robustness in the context of multi-modal scenarios and by employing multi-modal information to better assist text denoising.\nClaim9: As a result, the noisy OVAR task can be formulated as: given V and T \u2032, the model is expected to maximize the accuracy of action recognition, and even recovering T \u2032 to T.\nClaim10: Such poor robustness to the noisy OVAR task, proves excessive idealization of existing studies and also motivates us to fill the research gap.\nClaim11: We carry out extensive experiments to show our superior robustness, and thorough ablations to dissect the effectiveness of each component.\nClaim12: To illustrate the generalizability of our framework, we leverage two typical models from the VLA pipeline as \u03a6 OVAR, that is, ActionCLIP and XCLIP.\nClaim13: To partition these datasets for open-vocabulary action recognition, this paper follows the standard consensus, for the sake of fairness.\nClaim14: All ablation studies are conducted on UCF101 under 20% noise.\nClaim15: We do each simulation 10 times and report the mean and confidence interval of 95%.\nClaim16: We adopt two large-scale corpora of misspellings to analyze noise type in text descriptions.\nClaim17: Our method outperforms all competitors by large margins, which is impressive because our method is unsupervised without prior knowledge other than those contained in the OVAR model.\nClaim18: Tab. 2 compares Top-1 Acc to further reveal our solution is scalable/generalizable.\nClaim19: Under various noise rates, our model is robust to achieve huge improvements.\nClaim20: In terms of scalability across models, our method is not only applicable to hand-crafted prompts as in ActionCLIP but also to learnable visual-conditioned prompts as in XCLIP.\nClaim21: Furthermore, we notice that, whenever XCLIP outperforms ActionCLIP, our method also yields a better result.\nClaim22: A better visual encoder and well-tuned prompt may significantly increase our performance, showing that our method\u2019s upper limit could become higher, as the community continues to train better OVAR models.\nClaim23: Tab. 3 shows that, both \u03a6 inter and \u03a6 intra contribute to denoising text descriptions and to improving the robustness of underlying \u03a6 OVAR.\nClaim24: In terms of Top-1 Acc and Semantic Similarity, \u03a6 inter performs better than \u03a6 intra, since \u03a6 inter uses visual information as one direct optimization guideline to improve video recognition.\nClaim25: While \u03a6 intra performs better in terms of Label Acc, which focuses more on spelling correctness.\nClaim26: Besides, \u03a6 inter and \u03a6 intra turn out to be complementary: visual information helps to understand noisy text descriptions; while textual information prevents the model from being misled by visual samples.\nClaim27: We achieve the best performance when combining these two weightings.\nClaim28: Tab. 3 reports that it is beneficial to rely more on intra-modal at the beginning of decoding, and then gradually turn to inter-modal for more help.\nClaim29: This indicates that, when text noises are high, \u03a6 intra offers more help; when text noises are slight, \u03a6 inter could help more.\nClaim30: Fig. 4 Left reports our robustness under various noise types/rates.\nClaim31: Our method shows remarkable resilience when texts are perturbed by inserting or substituting characters.\nClaim32: Performance degradation is observed when texts are perturbed by deleting characters.\nClaim33: We conduct extensive experiments to demonstrate the generalizability of DENOISER across multiple models and datasets, and also show our superiority over uni-modal spell-checking solutions.\nClaim34: We find that adversarial training, though promising under closed-set scenarios in previous studies, is relatively ineffective under open-vocabulary settings.\nClaim35: Specifically, training with more noise lowers significantly the model\u2019s performance under low noise rate.\nClaim36: Additionally, its added value is limited under heavy noise rate.\nClaim37: These phenomena are probably related to the domain gap between datasets.\nClaim38: We conclude that noisy text descriptions are better solved in testing time rather than during training stage.\nClaim39: Our DENOISER framework shows a significant advantage over the adversarial training.\nClaim40: Our method shows a drop in performance when fewer visual samples are used in \u03a6 inter.\nClaim41: The performance tends to converge towards that when solely \u03a6 intra is used.\nClaim42: We hypothesize that fewer visual samples make \u03a6 inter harder to extract added value to \u03a6 intra.\nClaim43: With the noise rate increasing, we find an increasingly large drop in performance, which shows conversely that \u03a6 inter is more important under large noise scenarios as textual information becomes more ambiguous and less informative.\nClaim44: The Right shows that the denoised text descriptions improve the OVAR performance.\nClaim45: Our DENOISER remains faithful in terms of spelling, e.g. , 'wal4ingm with a dog' \u2192 'walking with a dog' instead of 'dogwalking'.\nClaim46: Our model requires a trade-off between computational cost and performance.\nClaim47: As shown in Fig. 4 and Fig. 6, the performance of our DENOISER increases as the number of proposals K and the percentage of the visual samples used.\nClaim48: DENOISER requires only simple operations for each iteration.\nClaim49: Compared to other approaches that intend to align noisy text-image pairs or to train spell-checking models, DENOISER that denoises at evaluation time is extremely time-saving.\nClaim50: We provide source code. Datasets publicly accessible.\nClaim51: We specify all settings of experiments report confidence intervals.\nClaim52: We report information of computer resources.\nClaim53: We conduct in the paper conform, in every respect, with the NeurIPS Code of Ethics.\nClaim54: Our model helps users better leverage the existing Open-Vocabulary models in a more robust way.\nClaim55: All the assets are properly cited. License and terms of use are properly respected.\nClaim56: We provided well-documented source code.\n\nReview: Weakness: - The baseline models are outdated and not tailored for OVAR. The authors failed to reference recent OVAR papers such as OpenVCLIP[1] (ICML 2023), FROSTER (ICLR 2024), and OTI (ACM MM 2023).\n\n- In Table 1, it is evident that the proposed method outperforms GPT-3.5. Additionally, the authors present examples in Table 4 to demonstrate the superiority of the proposed method over GPT-3.5. However, upon personal experimentation with all the examples from Table 4 using the provided prompt from the paper (lines 243-245), I observed that the GPT-3.5 model successfully rectified all issues, including challenging cases where the proposed method fell short. As a result, I remain unconvinced by the findings.\n\nThis is the prompt given to GPT-3.5 model, and I hope other reviewers can also try it on their own:\n\nThe following words may contain spelling errors by deleting, inserting, and substituting letters. You are a corrector of spelling errors. Give only the answer without explication. What is the correct spelling of the action of  \u201ccutting i aitnchen\u201d.\n\n\n[1] Open-VCLIP: Transforming CLIP to an Open-vocabulary Video Model via Interpolated Weight Optimization.\n\n[2] FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition.\n\n[3] Orthogonal Temporal Interpolation for Zero-Shot Video Recognition.\nQuestions: Please refer to the weaknesses.\n\n\n\n"", ""id"": ""uS9RZH6K651"", ""pdf"": ""openreview.net/pdf?id=uS9RZH6K65""}","29"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""Reliance on Consistency Assumption: The effectiveness of the method relies on the consistency assumption that higher-level LLMs can more accurately evaluate others. However, a natural concern is, 'Does this assumption always hold true in practice?' I suggest the authors further discuss the applicability of their method."", ""Target claims"": [4, 19, 33], ""Reasoning"": ""The extracted span is a weakness because it questions the validity of the consistency assumption used in the paper, which is claimed to underpin the proposed method\u2019s effectiveness. Claim4 suggests that the experimental results support the framework\u2019s effectiveness aligned with human preferences, which relies on this assumption. Claim19 reaffirms the importance of this assumption in the validation of their results. Claim33 discusses the experimental verification of their assumptions, making it a direct target for this critique questioning their practicality.""}]}, ""meta"": {""id"": ""uNZpvFlsg9"", ""review_idx"": 0, ""title"": ""Title: PiCO: Peer Review in LLMs based on the Consistency Optimization\nAbstract: Existing large language models (LLMs) evaluation methods typically focus on testing the performance on some closed-environment and domain-specific benchmarks with human annotations. In this paper, we explore a novel unsupervised evaluation direction, utilizing peer-review mechanisms to measure LLMs automatically without any human feedback. \nIn this setting, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. To obtain the ability hierarchy among these models, we assign each LLM a learnable capability parameter to adjust the final ranking.\nWe formalize it as a constrained optimization problem, intending to maximize the consistency of each LLM's capabilities and scores. \nThe key assumption behind is that high-level LLM can evaluate others' answers more accurately than low-level ones, while higher-level LLM can also achieve higher response scores. \nMoreover, we propose three metrics called PEN, CIN, and LIS to evaluate the gap in aligning human rankings. \nWe perform experiments on multiple datasets with these metrics, validating the effectiveness of the proposed approach.\n\nTLDR:\n\nKeywords:Large Language Model, Unsupervised Evaluation, Peer Review, Consistency Optimization"", ""claims"": [""Claim1: We perform experiments on multiple datasets with these metrics, validating the effectiveness of the proposed approach."", ""Claim2: This method aims to address the limitations of existing evaluation approaches and provide insights into LLMs\u2019 real-world capabilities."", ""Claim3: The experiments are conducted on multiple crowdsourcing datasets and validated on these three metrics."", ""Claim4: The experimental results demonstrate that the proposed PiCO framework can effectively obtain a large language models\u2019 leaderboard closer to human preferences."", ""Claim5: We perform experiments on multiple datasets with these metrics, validating the effectiveness of the proposed approach."", ""Claim6: To validate the effectiveness of the proposed approach, we perform experiments on Chatbot Arena[55], MT-Bench[55], and AlpacaEval[29]."", ""Claim7: The results presented in Table 1 illustrate the proposed PiCO method consistently surpasses competing approaches across the majority of evaluated metrics."", ""Claim8: Notably, PiCO achieves performance improvements of 0.1, 2.5, and 0.92 on the PEN, CIN, and LIS metrics, respectively, compared to the Runner-up."", ""Claim9: These results underscore the superiority of aggregating evaluations from multiple models, such as Majority Voting, Rating Voting, PRD, and PRE, as opposed to relying solely on single-model methods like GPTScore and PandaLM."", ""Claim10: In comparison with existing peer review evaluation methods (i.e., PRD and PRE), it is evident that PiCO exhibits improvements across various evaluation metrics."", ""Claim11: Despite PRD\u2019s adjustment of model weights based on their win rates and PRE\u2019s reliance on supervised human feedback data to assign weights through a qualification exam, neither method achieves performance superior to the fully unsupervised PiCO approach."", ""Claim12: PiCO leverages unsupervised learning techniques, allowing it to autonomously adapt and discover patterns in the data without explicit human intervention."", ""Claim13: In our consistency optimization, LLMs such as ChatGLM-6B and Mpt-7B have lower weights, and reducing their confidence can effectively alleviate the system evaluation bias."", ""Claim14: It can be observed that both PiCO and PRE exhibit better performance with an increasing number of eliminated 'reviewers'."", ""Claim15: The proposed PiCO approach can achieve better performance than PRE in most cases."", ""Claim16: On the contrary, the elimination process of our PiCO method is unsupervised and can still achieve better evaluation results than PRE."", ""Claim17: As shown in Table 2, it can be observed that the Forward Weight Voting achieves better results than the Uniform and Backward ones in all cases, while the Backward one achieves worse results."", ""Claim18: Most importantly, employing our consistency optimization algorithm to assign weights to different review models can further improve the performance of the evaluation system, i.e., lower PEN and CIN, as well as higher LIS in all cases."", ""Claim19: In summary, the above experimental results validate the effectiveness of the consistency assumption from various perspectives."", ""Claim20: In this paper, we propose the novel Peer Review method based on the Consistency Optimization (PiCO) to automatically evaluate Large Language Models (LLMs) without relying on human feedback."", ""Claim21: PiCO utilizes peer-review mechanisms to autonomously assess LLMs in a shared environment, where both open-source and closed-source models can respond to unlabeled questions and evaluate each other."", ""Claim22: We propose three metrics, i.e., PEN, CIN, and LIS, to quantify the disparity from human preferences."", ""Claim23: The extensive experiment results across multiple datasets and metrics demonstrate that PiCO effectively generates an LLM leaderboard that aligns closely with human preferences."", ""Claim24: In the future, we plan to extend the peer-review mechanism to evaluate the capabilities of multi-modality large models."", ""Claim25: In Figures 7 and 8, we present the results for the PEN and LIS metrics, where lower PEN scores indicate better performance, and higher LIS scores denote superior performance."", ""Claim26: It is evident that both the 'PiCO' and PRE approaches demonstrate enhanced performance as the number of eliminated 'reviewers' increases."", ""Claim27: In most cases, the proposed 'PiCO' method outperforms PRE."", ""Claim28: In Section 3.5, we validate the effectiveness of the consistency assumption and compare it with the Average Performance of the Reviewer Queue, i.e., employing a single LLM as the 'reviewer' to evaluate all response pairs and then calculating the average results of all LLMs."", ""Claim29: The comprehensive results compared with the Reviewer Queue are illustrated in Table3, Figure 9, 10 and 11, revealing that in the full Reviewer Queue, the performance of the vast majority of LLMs is very poor, indicating that the evaluations from most LLMs are noise."", ""Claim30: However, our 'PiCO' approach nearly matches the evaluative prowess of the pool's most capable LLM, GPT-3.5."", ""Claim31: Remarkably, given its unsupervised nature, the 'PiCO' method demonstrates the capability to mitigate the influence of noise, reaching the evaluation upper bound (the strongest LLM) within any given unknown LLM pool M, even in the absence of prior ranking information."", ""Claim32: We clearly state our claims in the abstract and introduction, such as a novel unsupervised LLM evaluation method and a consistency-based constrained optimization approach. These are substantiated in Section 3, demonstrating the alignment between our theoretical contributions and empirical results."", ""Claim33: Although this paper does not have a separate 'Limitations' section, the consistency assumptions on which the work is based are clearly stated in the introduction, and their validity is experimentally verified in Section 3.5. Moreover, the limitations of our work are discussed in the conclusion, noting that the current study is conducted solely within a text-based llm evaluation environment, and exploring the potential for future expansion into multimodal large model assessments."", ""Claim34: We thoroughly detail the Consistency Assumption which underpins our theoretical results and provide a complete proof in Section 3.5."", ""Claim35: We provide detailed pseudocode of our new LLM evaluation algorithm in Appendix D and have made all relevant data and code publicly accessible on GitHub, ensuring anonymity during the review process."", ""Claim36: All necessary data and code have been made publicly available on GitHub, with detailed instructions for installation, environment setup, and execution commands. This includes all raw, pre-processed, intermediate, and generated data needed to reproduce our experimental results. The repository is anonymous during the review process to ensure compliance with double-blind requirements."", ""Claim37: We conducted each experiment four times using different seeds (seed = 1 , 2 , 3 , 4) to ensure robustness."", ""Claim38: The results, presented as averages, are accompanied by standard deviations as error bars in Tables 1 and 2."", ""Claim39: This approach captures the variability due to different initializations and confirms the reproducibility of our results."", ""Claim40: The standard deviations used help clarify the extent of variability in the experiments, ensuring that our statistical analysis aligns with best practices for empirical research."", ""Claim41: Although we did not detail the exact compute resources for each experimental setup in the paper, we used NVIDIA A6000 graphics cards for open-source models and API calls for proprietary models."", ""Claim42: To facilitate reproducibility, we have provided all necessary data, ensuring that the experiments can be replicated on consumer-grade computers."", ""Claim43: The research conducted in this paper complies with the NeurIPS ethics guidelines in all respects."", ""Claim44: In the introduction, we discuss the potential positive impact of our novel unsupervised LLM evaluation approach, which could significantly advance the field of LLM evaluation."", ""Claim45: However, we also recognize potential negative societal impacts, such as the misuse of this technology to unfairly or inaccurately assess LLM systems, which might lead to biased or misleading outcomes."", ""Claim46: We suggest potential mitigation strategies, such as implementing robust validation protocols and ethical guidelines to govern the application of this evaluation methodology."", ""Claim47: This paper introduces a new approach for unsupervised LLM evaluation and does not involve the release of pre-trained models, image generators, or newly collected datasets."", ""Claim48: Therefore, there are no direct risks associated with misuse or dual-use of such resources, making safeguards for controlled release irrelevant to this study."", ""Claim49: This paper utilizes the FastChat project\u2019s code, along with several other pre-trained models and datasets."", ""Claim50: The FastChat project adheres to the Apache License 2.0."", ""Claim51: In compliance with the licensing requirements, we have included the original project\u2019s licensing information in all derivative works and have clearly marked any modifications made to the code."", ""Claim52: Additionally, we have ensured that all utilized pre-trained models and datasets are appropriately cited.""], ""review"": ""Review: Weakness: 1. Reliance on Consistency Assumption: The effectiveness of the method relies on the consistency assumption that higher-level LLMs can more accurately evaluate others. However, a natural concern is, \""Does this assumption always hold true in practice?\"" I suggest the authors further discuss the applicability of their method.\n2. Complexity of Implementation: The framework involves a complex review process, which requires substantial computational resources to support the LLMs' inference. Can you provide some details on the number of tokens consumed and a comparison of consumption with baseline methods?\n3. Consideration of Multi-Agent Methods: Since the proposed method employs multiple LLMs, I think more recent and advanced evaluation methods based on multi-agent systems should also be considered in the experiments, such as AgentVerse.\n4. Details of ELO: The ELO system is essential for the proposed mechanism. However, it is only mentioned in the appendix. I suggest the authors add more details about the background of ELO and how it is adopted in the proposed mechanism.\nQuestions: Please see my above comments\n\n""}, ""review"": ""Title: PiCO: Peer Review in LLMs based on the Consistency Optimization\nAbstract: Existing large language models (LLMs) evaluation methods typically focus on testing the performance on some closed-environment and domain-specific benchmarks with human annotations. In this paper, we explore a novel unsupervised evaluation direction, utilizing peer-review mechanisms to measure LLMs automatically without any human feedback. \nIn this setting, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. To obtain the ability hierarchy among these models, we assign each LLM a learnable capability parameter to adjust the final ranking.\nWe formalize it as a constrained optimization problem, intending to maximize the consistency of each LLM's capabilities and scores. \nThe key assumption behind is that high-level LLM can evaluate others' answers more accurately than low-level ones, while higher-level LLM can also achieve higher response scores. \nMoreover, we propose three metrics called PEN, CIN, and LIS to evaluate the gap in aligning human rankings. \nWe perform experiments on multiple datasets with these metrics, validating the effectiveness of the proposed approach.\n\nClaim1: We perform experiments on multiple datasets with these metrics, validating the effectiveness of the proposed approach.\nClaim2: This method aims to address the limitations of existing evaluation approaches and provide insights into LLMs\u2019 real-world capabilities.\nClaim3: The experiments are conducted on multiple crowdsourcing datasets and validated on these three metrics.\nClaim4: The experimental results demonstrate that the proposed PiCO framework can effectively obtain a large language models\u2019 leaderboard closer to human preferences.\nClaim5: We perform experiments on multiple datasets with these metrics, validating the effectiveness of the proposed approach.\nClaim6: To validate the effectiveness of the proposed approach, we perform experiments on Chatbot Arena[55], MT-Bench[55], and AlpacaEval[29].\nClaim7: The results presented in Table 1 illustrate the proposed PiCO method consistently surpasses competing approaches across the majority of evaluated metrics.\nClaim8: Notably, PiCO achieves performance improvements of 0.1, 2.5, and 0.92 on the PEN, CIN, and LIS metrics, respectively, compared to the Runner-up.\nClaim9: These results underscore the superiority of aggregating evaluations from multiple models, such as Majority Voting, Rating Voting, PRD, and PRE, as opposed to relying solely on single-model methods like GPTScore and PandaLM.\nClaim10: In comparison with existing peer review evaluation methods (i.e., PRD and PRE), it is evident that PiCO exhibits improvements across various evaluation metrics.\nClaim11: Despite PRD\u2019s adjustment of model weights based on their win rates and PRE\u2019s reliance on supervised human feedback data to assign weights through a qualification exam, neither method achieves performance superior to the fully unsupervised PiCO approach.\nClaim12: PiCO leverages unsupervised learning techniques, allowing it to autonomously adapt and discover patterns in the data without explicit human intervention.\nClaim13: In our consistency optimization, LLMs such as ChatGLM-6B and Mpt-7B have lower weights, and reducing their confidence can effectively alleviate the system evaluation bias.\nClaim14: It can be observed that both PiCO and PRE exhibit better performance with an increasing number of eliminated 'reviewers'.\nClaim15: The proposed PiCO approach can achieve better performance than PRE in most cases.\nClaim16: On the contrary, the elimination process of our PiCO method is unsupervised and can still achieve better evaluation results than PRE.\nClaim17: As shown in Table 2, it can be observed that the Forward Weight Voting achieves better results than the Uniform and Backward ones in all cases, while the Backward one achieves worse results.\nClaim18: Most importantly, employing our consistency optimization algorithm to assign weights to different review models can further improve the performance of the evaluation system, i.e., lower PEN and CIN, as well as higher LIS in all cases.\nClaim19: In summary, the above experimental results validate the effectiveness of the consistency assumption from various perspectives.\nClaim20: In this paper, we propose the novel Peer Review method based on the Consistency Optimization (PiCO) to automatically evaluate Large Language Models (LLMs) without relying on human feedback.\nClaim21: PiCO utilizes peer-review mechanisms to autonomously assess LLMs in a shared environment, where both open-source and closed-source models can respond to unlabeled questions and evaluate each other.\nClaim22: We propose three metrics, i.e., PEN, CIN, and LIS, to quantify the disparity from human preferences.\nClaim23: The extensive experiment results across multiple datasets and metrics demonstrate that PiCO effectively generates an LLM leaderboard that aligns closely with human preferences.\nClaim24: In the future, we plan to extend the peer-review mechanism to evaluate the capabilities of multi-modality large models.\nClaim25: In Figures 7 and 8, we present the results for the PEN and LIS metrics, where lower PEN scores indicate better performance, and higher LIS scores denote superior performance.\nClaim26: It is evident that both the 'PiCO' and PRE approaches demonstrate enhanced performance as the number of eliminated 'reviewers' increases.\nClaim27: In most cases, the proposed 'PiCO' method outperforms PRE.\nClaim28: In Section 3.5, we validate the effectiveness of the consistency assumption and compare it with the Average Performance of the Reviewer Queue, i.e., employing a single LLM as the 'reviewer' to evaluate all response pairs and then calculating the average results of all LLMs.\nClaim29: The comprehensive results compared with the Reviewer Queue are illustrated in Table3, Figure 9, 10 and 11, revealing that in the full Reviewer Queue, the performance of the vast majority of LLMs is very poor, indicating that the evaluations from most LLMs are noise.\nClaim30: However, our 'PiCO' approach nearly matches the evaluative prowess of the pool's most capable LLM, GPT-3.5.\nClaim31: Remarkably, given its unsupervised nature, the 'PiCO' method demonstrates the capability to mitigate the influence of noise, reaching the evaluation upper bound (the strongest LLM) within any given unknown LLM pool M, even in the absence of prior ranking information.\nClaim32: We clearly state our claims in the abstract and introduction, such as a novel unsupervised LLM evaluation method and a consistency-based constrained optimization approach. These are substantiated in Section 3, demonstrating the alignment between our theoretical contributions and empirical results.\nClaim33: Although this paper does not have a separate 'Limitations' section, the consistency assumptions on which the work is based are clearly stated in the introduction, and their validity is experimentally verified in Section 3.5. Moreover, the limitations of our work are discussed in the conclusion, noting that the current study is conducted solely within a text-based llm evaluation environment, and exploring the potential for future expansion into multimodal large model assessments.\nClaim34: We thoroughly detail the Consistency Assumption which underpins our theoretical results and provide a complete proof in Section 3.5.\nClaim35: We provide detailed pseudocode of our new LLM evaluation algorithm in Appendix D and have made all relevant data and code publicly accessible on GitHub, ensuring anonymity during the review process.\nClaim36: All necessary data and code have been made publicly available on GitHub, with detailed instructions for installation, environment setup, and execution commands. This includes all raw, pre-processed, intermediate, and generated data needed to reproduce our experimental results. The repository is anonymous during the review process to ensure compliance with double-blind requirements.\nClaim37: We conducted each experiment four times using different seeds (seed = 1 , 2 , 3 , 4) to ensure robustness.\nClaim38: The results, presented as averages, are accompanied by standard deviations as error bars in Tables 1 and 2.\nClaim39: This approach captures the variability due to different initializations and confirms the reproducibility of our results.\nClaim40: The standard deviations used help clarify the extent of variability in the experiments, ensuring that our statistical analysis aligns with best practices for empirical research.\nClaim41: Although we did not detail the exact compute resources for each experimental setup in the paper, we used NVIDIA A6000 graphics cards for open-source models and API calls for proprietary models.\nClaim42: To facilitate reproducibility, we have provided all necessary data, ensuring that the experiments can be replicated on consumer-grade computers.\nClaim43: The research conducted in this paper complies with the NeurIPS ethics guidelines in all respects.\nClaim44: In the introduction, we discuss the potential positive impact of our novel unsupervised LLM evaluation approach, which could significantly advance the field of LLM evaluation.\nClaim45: However, we also recognize potential negative societal impacts, such as the misuse of this technology to unfairly or inaccurately assess LLM systems, which might lead to biased or misleading outcomes.\nClaim46: We suggest potential mitigation strategies, such as implementing robust validation protocols and ethical guidelines to govern the application of this evaluation methodology.\nClaim47: This paper introduces a new approach for unsupervised LLM evaluation and does not involve the release of pre-trained models, image generators, or newly collected datasets.\nClaim48: Therefore, there are no direct risks associated with misuse or dual-use of such resources, making safeguards for controlled release irrelevant to this study.\nClaim49: This paper utilizes the FastChat project\u2019s code, along with several other pre-trained models and datasets.\nClaim50: The FastChat project adheres to the Apache License 2.0.\nClaim51: In compliance with the licensing requirements, we have included the original project\u2019s licensing information in all derivative works and have clearly marked any modifications made to the code.\nClaim52: Additionally, we have ensured that all utilized pre-trained models and datasets are appropriately cited.\n\nReview: Weakness: 1. Reliance on Consistency Assumption: The effectiveness of the method relies on the consistency assumption that higher-level LLMs can more accurately evaluate others. However, a natural concern is, \""Does this assumption always hold true in practice?\"" I suggest the authors further discuss the applicability of their method.\n2. Complexity of Implementation: The framework involves a complex review process, which requires substantial computational resources to support the LLMs' inference. Can you provide some details on the number of tokens consumed and a comparison of consumption with baseline methods?\n3. Consideration of Multi-Agent Methods: Since the proposed method employs multiple LLMs, I think more recent and advanced evaluation methods based on multi-agent systems should also be considered in the experiments, such as AgentVerse.\n4. Details of ELO: The ELO system is essential for the proposed mechanism. However, it is only mentioned in the appendix. I suggest the authors add more details about the background of ELO and how it is adopted in the proposed mechanism.\nQuestions: Please see my above comments\n\n\n\n"", ""id"": ""uNZpvFlsg90"", ""pdf"": ""openreview.net/pdf?id=uNZpvFlsg9""}","30"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""The methodology and findings are not particularly novel. Previous studies have already suggested that untrained LLMs can achieve good brain scores and that sentence length and position are significant predictors. Thus, two of the three core contributions claimed by the authors are not unique to this paper."", ""Target claims"": [3, 6], ""Reasoning"": ""The review explicitly states that the findings are not novel and that previous studies have already established that untrained LLMs achieve good brain scores primarily due to sentence length and position, which corresponds directly to Claim3. Furthermore, the conclusion about over-reliance on brain scores leading to over-interpretation as stated in Claim6, is also identified as not clearly drawn or unique, questioning its contribution.""}, {""Weakness span"": ""While the authors conclude that over-reliance on brain scores can lead to over-interpretations of similarity between LLMs and brains, it is not clear how this conclusion is drawn from the experimental results. The study itself relies heavily on brain scores to make its arguments, and the authors do not explicitly state what aspects of previous work have been over-interpreted."", ""Target claims"": [6], ""Reasoning"": ""This part of the review questions the strength and clarity of Claim6. It argues that the paper does not make clear how its conclusions about the over-reliance and over-interpretation of brain scores are substantiated by its results, directly challenging the validity and clarity of the claims made.""}]}, ""meta"": {""id"": ""u1b1dJtyxc"", ""review_idx"": 0, ""title"": ""Title: What Are Large Language Models Mapping to in the Brain? A Case Against Over-Reliance on Brain Scores\nAbstract: Given the remarkable capabilities of large language models (LLMs), there has been a growing interest in evaluating their similarity to the human brain. One approach towards quantifying this similarity is by measuring how well a model predicts neural signals, also called \""brain score\"". Internal representations from LLMs achieve state-of-the-art brain scores, leading to speculation that they share computational principles with human language processing. This inference is only valid if the subset of neural activity predicted by LLMs reflects core elements of language processing. Here, we question this assumption by analyzing three neural datasets used in an impactful study on LLM-to-brain mappings, with a particular focus on an fMRI dataset where participants read short passages. We first find that when using shuffled train-test splits, as done in previous studies with these datasets, a trivial feature that encodes temporal autocorrelation not only outperforms LLMs but also accounts for the majority of neural variance that LLMs explain. We therefore caution against shuffled train-test splits, and use contiguous test splits moving forward. Second, we explain the surprising result that untrained LLMs have higher-than-expected brain scores by showing they do not account for additional neural variance beyond two simple features: sentence length and sentence position. This undermines evidence used to claim that the transformer architecture biases computations to be more brain-like. Third, we find that brain scores of trained LLMs on this dataset can largely be explained by sentence position, sentence length, and static word vectors; a small, additional amount is explained by sense-specific word embeddings and contextual representations of sentence structure. We conclude that over-reliance on brain scores can lead to over-interpretation of similarity between LLMs and brains, and emphasize the importance of deconstructing what LLMs are mapping to in neural signals.\n\nTLDR:We find that on three widely used neural datasets, much of the neural encoding performance of LLMs is driven by simple features, and we urge researchers to rigorously interpret what LLMs are mapping to in neural signals before forming conclusions.\n\nKeywords:Large language models, Neuroscience, Neural encoding, fMRI, Replication, Language"", ""claims"": [""Claim1: We first find that when using shuffled train-test splits, as done in previous studies with these datasets, a trivial feature that encodes temporal autocorrelation not only outperforms LLMs but also accounts for the majority of neural variance that LLMs explain."", ""Claim2: We therefore caution against shuffled train-test splits, and use contiguous test splits moving forward."", ""Claim3: Second, we explain the surprising result that untrained LLMs have higher-than-expected brain scores by showing they do not account for additional neural variance beyond two simple features: sentence length and sentence position."", ""Claim4: This undermines evidence used to claim that the transformer architecture biases computations to be more brain-like."", ""Claim5: Third, we find that brain scores of trained LLMs on this dataset can largely be explained by sentence position, sentence length, and static word vectors; a small, additional amount is explained by sense-specific word embeddings and contextual representations of sentence structure."", ""Claim6: We conclude that over-reliance on brain scores can lead to over-interpretation of similarity between LLMs and brains, and emphasize the importance of deconstructing what LLMs are mapping to in neural signals."", ""Claim7: They found that GPT2-XL achieved the highest brain score and, in one neural dataset, accounted for 100% of the \""explainable\"" neural variance (i.e., taking into account the noise inherent in the data)."", ""Claim8: Surprisingly, the authors further found that untrained (i.e. randomly initialized) LLMs predict neural activity well, leading to speculations that the transformer architecture biases computations to be more brain-like."", ""Claim9: However, this assumption is not necessarily true."", ""Claim10: For example, a recent paper found that, when participants listen to stories, the fMRI signal includes an initial ramping, positional artifact."", ""Claim11: It is likely that LLMs which contain absolute positional embeddings would be able to predict this ramping signal, whereas a simpler model such as a static word embedding (e.g. GloVe) would not, leading to exaggerated differences between LLMs and GloVe due to reasons of little theoretical interest."", ""Claim12: Without attempting to rigorously deconstruct the mapping between LLMs and brains, it is possible to draw erroneous conclusions about the brain\u2019s mechanisms for processing language."", ""Claim13: We first find that when using shuffled train-test splits, as done in previous studies with these datasets, a trivial feature that encodes temporal autocorrelation not only outperforms LLMs but also accounts for the majority of neural variance that LLMs explain."", ""Claim14: We therefore caution against shuffled train-test splits, and use contiguous test splits moving forward."", ""Claim15: This undermines evidence used to claim that the transformer architecture biases computations to be more brain-like."", ""Claim16: We find that brain scores of trained LLMs on this dataset can largely be explained by sentence position, sentence length, and static word vectors; a small, additional amount is explained by sense-specific word embeddings and contextual representations of sentence structure."", ""Claim17: We conclude that over-reliance on brain scores can lead to over-interpretation of similarity between LLMs and brains, and emphasize the importance of deconstructing what LLMs are mapping to in neural signals."", ""Claim18: We demonstrate that this approach compromises the evaluation of the neural predictivity of LLMs."", ""Claim19: Next, we hypothesized that much of what LLMs might be mapping to when using shuffled splits could be accounted for by OASM, a model which only represents within passage auto-correlation and between passage orthogonality."", ""Claim20: OASM out-performed GPT2-XL on both EXP1 and EXP2, revealing that a completely non-linguistic feature space can achieve absurdly high brain scores in the context of shuffled splits."", ""Claim21: This strongly challenges the assumption of multiple previous studies that performance on this benchmark is an indication of a model\u2019s brain-likeness."", ""Claim22: Moreover, we find that the unique neural variance that GPT2-XL explains over OASM is very small relative to what OASM explains alone."", ""Claim23: Even with these corrections, we find that R2OASM + GPT2 \u2212 XL* was 13.6% higher than R2OASM in EXP1, and 31.5% higher than R2OASM in EXP2 (Fig. 1b) (% differences after averaging R2 across participants)."", ""Claim24: To be clear, this means that any linguistically-driven neural variance that GPT2-XL uniquely explains over OASM is far smaller (13.6% on EXP1 and 31.5% on EXP2) than what is predicted solely by OASM, a model with no linguistic features that completely lacks the ability to generalize to fully held out passages."", ""Claim25: This may explain why the static layer often outperformed intermediate layer representations in EXP1 despite encoding sentence length more poorly."", ""Claim26: Overall, these results suggest that, when averaging across voxels within the language network in this dataset, GPT2-XLU does not improve neural encoding performance over sentence length and position."", ""Claim27: Across all functional networks, only 1.26% (EXP1) and 1.42% (EXP2) of voxels were significantly (\u03b1=0.05) better explained by the GPT2-XLU model before false discovery rate (FDR) correction; these numbers dropped to 0.001% (EXP1) and 0.078% (EXP2) after performing FDR correction within each participant and network [31]."", ""Claim28: None of the significant voxels after FDR correction were inside the language network."", ""Claim29: Taken together, these results suggest GPT2-XLU does not enhance neural prediction performance over sentence length and position even at the voxel level."", ""Claim30: Thus, our results hold even when controlling for decreases in performance due to the addition of feature spaces."", ""Claim31: The baseline SP+SL+WORD model, which does not account for any form of contextual processing, performs 75% as well as GPT2-XL in EXP1, and outperforms GPT2-XL in EXP2."", ""Claim32: When adding contextual features, namely SENSE and SYNT, our model performs 84.4% as well as GPT2-XL and the full model in EXP1, and better than GPT2-XL and 95.5% as well as the full model in EXP2, indicating that SENSE and SYNT play a modest role in accounting for GPT2-XL brain scores beyond simple features in this dataset."", ""Claim33: R2 SP + SL + WORD * was 0.028 in EXP1 and 0.048 in EXP2, and R2 SP + SL + WORD + GPT 2 \u2212 XL * was 0.036 in EXP1 and 0.056 in EXP2 (mean across participants)."", ""Claim34: This indicates that even after controlling for a reduction in GPT2-XL performance from the addition of simple features, GPT2-XL only explains an additional 28.57% (EXP1) and 16.7% (EXP2) neural variance over a model composed of features that are all non-contextual."", ""Claim35: Our results therefore suggest that the mapping between GPT2-XL and neural activity on the Fedorenko dataset is largely driven by positional signals."", ""Claim36: Our results therefore suggest that GPT2-XL performs at near chance levels on the version of the Blank dataset used by [2, 10, 11]."", ""Claim37: In summary, we find that on the Pereira dataset, shuffled splits are heavily impacted by temporal autocorrelation, untrained GPT2-XL brain score is explained by sentence length and position, and trained GPT2-XL brain score is largely explained by non-contextual features."", ""Claim38: We find that the majority of GPT2-XL brain score on the Fedorenko dataset is accounted for by word position, and on the Blank dataset GPT2-XL predicts neural activity at near chance levels."", ""Claim39: These results suggest that (i) brain scores on these datasets should be interpreted with caution; and (ii) more generally, analyses using brain scores should be accompanied by a systematic deconstruction of neural encoding performance, and an evaluation against simple and theoretically uninteresting features."", ""Claim40: To examine whether our results depending on the choice of LLM, we replicated all of our Pereira trained analyses with RoBERTa-Large (ROB). The overall trend in results was the same as with GPT2-XL (Fig. 6). Namely, SP+SL+WORD performed 76 . 8 % as well as the full model (SP+SL+WORD+SENSE+SYNT+ROB) and 80 . 0 % as well as ROB alone in EXP1, and in EXP2 it performed 88 . 0 % as well as the full model and better than ROB."", ""Claim41: Furthermore, SENSE and SYNT bridge the gap to the full model by a small amount."", ""Claim42: In sum, our main conclusion that a large amount of trained LLM brain score in the Pereira dataset is accounted for by non-contextual features also applies to RoBERTa-Large."", ""Claim43: Hence, we strongly urge researchers to avoid shuffled train test splits when evaluating the neural predictivity of language models, and we surmise that previous studies using shuffled train-test splits to compare neural predictivity between models might have come to erroneous conclusions."", ""Claim44: We find that OASM achieves 103.6 times higher neural predictivity than GPT2-XL on the Blank dataset when using shuffled train-test splits.""], ""review"": ""Review: Weakness: 1.  The methodology and findings are not particularly novel. Previous studies have already suggested that untrained LLMs can achieve good brain scores and that sentence length and position are significant predictors. Thus, two of the three core contributions claimed by the authors are not unique to this paper.\n2. While the authors conclude that over-reliance on brain scores can lead to over-interpretations of similarity between LLMs and brains, it is not clear how this conclusion is drawn from the experimental results. The study itself relies heavily on brain scores to make its arguments, and the authors do not explicitly state what aspects of previous work have been over-interpreted.\nQuestions: 1. How do the findings of this paper significantly advance our understanding beyond what has already been established about untrained LLMs and simple feature predictivity?\n2. What specific previous findings are over-interpreted when relying on brain scores? How can over-reliance on brain scores be quantified or defined?\n3. What alternative methods or interpretations do the authors propose for evaluating the alignment between LLMs and brain activity if the current reliance on brain scores is problematic?\n\n""}, ""review"": ""Title: What Are Large Language Models Mapping to in the Brain? A Case Against Over-Reliance on Brain Scores\nAbstract: Given the remarkable capabilities of large language models (LLMs), there has been a growing interest in evaluating their similarity to the human brain. One approach towards quantifying this similarity is by measuring how well a model predicts neural signals, also called \""brain score\"". Internal representations from LLMs achieve state-of-the-art brain scores, leading to speculation that they share computational principles with human language processing. This inference is only valid if the subset of neural activity predicted by LLMs reflects core elements of language processing. Here, we question this assumption by analyzing three neural datasets used in an impactful study on LLM-to-brain mappings, with a particular focus on an fMRI dataset where participants read short passages. We first find that when using shuffled train-test splits, as done in previous studies with these datasets, a trivial feature that encodes temporal autocorrelation not only outperforms LLMs but also accounts for the majority of neural variance that LLMs explain. We therefore caution against shuffled train-test splits, and use contiguous test splits moving forward. Second, we explain the surprising result that untrained LLMs have higher-than-expected brain scores by showing they do not account for additional neural variance beyond two simple features: sentence length and sentence position. This undermines evidence used to claim that the transformer architecture biases computations to be more brain-like. Third, we find that brain scores of trained LLMs on this dataset can largely be explained by sentence position, sentence length, and static word vectors; a small, additional amount is explained by sense-specific word embeddings and contextual representations of sentence structure. We conclude that over-reliance on brain scores can lead to over-interpretation of similarity between LLMs and brains, and emphasize the importance of deconstructing what LLMs are mapping to in neural signals.\n\nClaim1: We first find that when using shuffled train-test splits, as done in previous studies with these datasets, a trivial feature that encodes temporal autocorrelation not only outperforms LLMs but also accounts for the majority of neural variance that LLMs explain.\nClaim2: We therefore caution against shuffled train-test splits, and use contiguous test splits moving forward.\nClaim3: Second, we explain the surprising result that untrained LLMs have higher-than-expected brain scores by showing they do not account for additional neural variance beyond two simple features: sentence length and sentence position.\nClaim4: This undermines evidence used to claim that the transformer architecture biases computations to be more brain-like.\nClaim5: Third, we find that brain scores of trained LLMs on this dataset can largely be explained by sentence position, sentence length, and static word vectors; a small, additional amount is explained by sense-specific word embeddings and contextual representations of sentence structure.\nClaim6: We conclude that over-reliance on brain scores can lead to over-interpretation of similarity between LLMs and brains, and emphasize the importance of deconstructing what LLMs are mapping to in neural signals.\nClaim7: They found that GPT2-XL achieved the highest brain score and, in one neural dataset, accounted for 100% of the \""explainable\"" neural variance (i.e., taking into account the noise inherent in the data).\nClaim8: Surprisingly, the authors further found that untrained (i.e. randomly initialized) LLMs predict neural activity well, leading to speculations that the transformer architecture biases computations to be more brain-like.\nClaim9: However, this assumption is not necessarily true.\nClaim10: For example, a recent paper found that, when participants listen to stories, the fMRI signal includes an initial ramping, positional artifact.\nClaim11: It is likely that LLMs which contain absolute positional embeddings would be able to predict this ramping signal, whereas a simpler model such as a static word embedding (e.g. GloVe) would not, leading to exaggerated differences between LLMs and GloVe due to reasons of little theoretical interest.\nClaim12: Without attempting to rigorously deconstruct the mapping between LLMs and brains, it is possible to draw erroneous conclusions about the brain\u2019s mechanisms for processing language.\nClaim13: We first find that when using shuffled train-test splits, as done in previous studies with these datasets, a trivial feature that encodes temporal autocorrelation not only outperforms LLMs but also accounts for the majority of neural variance that LLMs explain.\nClaim14: We therefore caution against shuffled train-test splits, and use contiguous test splits moving forward.\nClaim15: This undermines evidence used to claim that the transformer architecture biases computations to be more brain-like.\nClaim16: We find that brain scores of trained LLMs on this dataset can largely be explained by sentence position, sentence length, and static word vectors; a small, additional amount is explained by sense-specific word embeddings and contextual representations of sentence structure.\nClaim17: We conclude that over-reliance on brain scores can lead to over-interpretation of similarity between LLMs and brains, and emphasize the importance of deconstructing what LLMs are mapping to in neural signals.\nClaim18: We demonstrate that this approach compromises the evaluation of the neural predictivity of LLMs.\nClaim19: Next, we hypothesized that much of what LLMs might be mapping to when using shuffled splits could be accounted for by OASM, a model which only represents within passage auto-correlation and between passage orthogonality.\nClaim20: OASM out-performed GPT2-XL on both EXP1 and EXP2, revealing that a completely non-linguistic feature space can achieve absurdly high brain scores in the context of shuffled splits.\nClaim21: This strongly challenges the assumption of multiple previous studies that performance on this benchmark is an indication of a model\u2019s brain-likeness.\nClaim22: Moreover, we find that the unique neural variance that GPT2-XL explains over OASM is very small relative to what OASM explains alone.\nClaim23: Even with these corrections, we find that R2OASM + GPT2 \u2212 XL* was 13.6% higher than R2OASM in EXP1, and 31.5% higher than R2OASM in EXP2 (Fig. 1b) (% differences after averaging R2 across participants).\nClaim24: To be clear, this means that any linguistically-driven neural variance that GPT2-XL uniquely explains over OASM is far smaller (13.6% on EXP1 and 31.5% on EXP2) than what is predicted solely by OASM, a model with no linguistic features that completely lacks the ability to generalize to fully held out passages.\nClaim25: This may explain why the static layer often outperformed intermediate layer representations in EXP1 despite encoding sentence length more poorly.\nClaim26: Overall, these results suggest that, when averaging across voxels within the language network in this dataset, GPT2-XLU does not improve neural encoding performance over sentence length and position.\nClaim27: Across all functional networks, only 1.26% (EXP1) and 1.42% (EXP2) of voxels were significantly (\u03b1=0.05) better explained by the GPT2-XLU model before false discovery rate (FDR) correction; these numbers dropped to 0.001% (EXP1) and 0.078% (EXP2) after performing FDR correction within each participant and network [31].\nClaim28: None of the significant voxels after FDR correction were inside the language network.\nClaim29: Taken together, these results suggest GPT2-XLU does not enhance neural prediction performance over sentence length and position even at the voxel level.\nClaim30: Thus, our results hold even when controlling for decreases in performance due to the addition of feature spaces.\nClaim31: The baseline SP+SL+WORD model, which does not account for any form of contextual processing, performs 75% as well as GPT2-XL in EXP1, and outperforms GPT2-XL in EXP2.\nClaim32: When adding contextual features, namely SENSE and SYNT, our model performs 84.4% as well as GPT2-XL and the full model in EXP1, and better than GPT2-XL and 95.5% as well as the full model in EXP2, indicating that SENSE and SYNT play a modest role in accounting for GPT2-XL brain scores beyond simple features in this dataset.\nClaim33: R2 SP + SL + WORD * was 0.028 in EXP1 and 0.048 in EXP2, and R2 SP + SL + WORD + GPT 2 \u2212 XL * was 0.036 in EXP1 and 0.056 in EXP2 (mean across participants).\nClaim34: This indicates that even after controlling for a reduction in GPT2-XL performance from the addition of simple features, GPT2-XL only explains an additional 28.57% (EXP1) and 16.7% (EXP2) neural variance over a model composed of features that are all non-contextual.\nClaim35: Our results therefore suggest that the mapping between GPT2-XL and neural activity on the Fedorenko dataset is largely driven by positional signals.\nClaim36: Our results therefore suggest that GPT2-XL performs at near chance levels on the version of the Blank dataset used by [2, 10, 11].\nClaim37: In summary, we find that on the Pereira dataset, shuffled splits are heavily impacted by temporal autocorrelation, untrained GPT2-XL brain score is explained by sentence length and position, and trained GPT2-XL brain score is largely explained by non-contextual features.\nClaim38: We find that the majority of GPT2-XL brain score on the Fedorenko dataset is accounted for by word position, and on the Blank dataset GPT2-XL predicts neural activity at near chance levels.\nClaim39: These results suggest that (i) brain scores on these datasets should be interpreted with caution; and (ii) more generally, analyses using brain scores should be accompanied by a systematic deconstruction of neural encoding performance, and an evaluation against simple and theoretically uninteresting features.\nClaim40: To examine whether our results depending on the choice of LLM, we replicated all of our Pereira trained analyses with RoBERTa-Large (ROB). The overall trend in results was the same as with GPT2-XL (Fig. 6). Namely, SP+SL+WORD performed 76 . 8 % as well as the full model (SP+SL+WORD+SENSE+SYNT+ROB) and 80 . 0 % as well as ROB alone in EXP1, and in EXP2 it performed 88 . 0 % as well as the full model and better than ROB.\nClaim41: Furthermore, SENSE and SYNT bridge the gap to the full model by a small amount.\nClaim42: In sum, our main conclusion that a large amount of trained LLM brain score in the Pereira dataset is accounted for by non-contextual features also applies to RoBERTa-Large.\nClaim43: Hence, we strongly urge researchers to avoid shuffled train test splits when evaluating the neural predictivity of language models, and we surmise that previous studies using shuffled train-test splits to compare neural predictivity between models might have come to erroneous conclusions.\nClaim44: We find that OASM achieves 103.6 times higher neural predictivity than GPT2-XL on the Blank dataset when using shuffled train-test splits.\n\nReview: Weakness: 1.  The methodology and findings are not particularly novel. Previous studies have already suggested that untrained LLMs can achieve good brain scores and that sentence length and position are significant predictors. Thus, two of the three core contributions claimed by the authors are not unique to this paper.\n2. While the authors conclude that over-reliance on brain scores can lead to over-interpretations of similarity between LLMs and brains, it is not clear how this conclusion is drawn from the experimental results. The study itself relies heavily on brain scores to make its arguments, and the authors do not explicitly state what aspects of previous work have been over-interpreted.\nQuestions: 1. How do the findings of this paper significantly advance our understanding beyond what has already been established about untrained LLMs and simple feature predictivity?\n2. What specific previous findings are over-interpreted when relying on brain scores? How can over-reliance on brain scores be quantified or defined?\n3. What alternative methods or interpretations do the authors propose for evaluating the alignment between LLMs and brain activity if the current reliance on brain scores is problematic?\n\n\n\n"", ""id"": ""u1b1dJtyxc0"", ""pdf"": ""openreview.net/pdf?id=u1b1dJtyxc""}","31"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""The perplexity/accuracy results of the baseline methods deviate from the results reported in previous papers."", ""Target claims"": [1, 2, 23], ""Reasoning"": ""The reviewer points out that the perplexity and accuracy results achieved by the baseline methods in the paper are worse than those reported in previous studies. This is a weakness concerning the paper's claims about the effectiveness of their methods. Claim 1 asserts that their experiments validate the effectiveness of the proposed methods, which this discrepancy in results directly questions. Similarly, Claim 2 suggests that their methods enhance current techniques for controlling activation spikes, but if the baseline performance is inaccurately represented, the enhancements are also in question. Claim 23 explicitly mentions the resolution of activation spikes and performance restoration close to FP16, which might be overclaimed if baseline results are incorrectly reported.""}, {""Weakness span"": ""The paper does not compare its method with the state-of-the-art LLM quantization method [1], which enables W4A4 quantization (partially using 8-bit operations) with a PTQ approach."", ""Target claims"": [8], ""Reasoning"": ""The reviewer mentions a lack of comparison with a current state-of-the-art method (Atom), which could limit the validity of Claim 8. Claim 8 suggests that their methods show competitive performance compared to other approaches, but without a comparison to Atom, this claim might be overestimated or unfounded in relation to the true state-of-the-art.""}]}, ""meta"": {""id"": ""t8ch1OCvHh"", ""review_idx"": 0, ""title"": ""Title: Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs\nAbstract: Modern large language models (LLMs) have established state-of-the-art performance through architectural improvements, but still require significant computational cost for inference. In an effort to reduce the inference cost, post-training quantization (PTQ) has become a popular approach, quantizing weights and activations to lower precision, such as INT8. In this paper, we reveal the challenges of activation quantization in GLU variants, which are widely used in feed-forward network (FFN) of modern LLMs, such as LLaMA family.\nThe problem is that severe local quantization errors, caused by excessive magnitudes of activation in GLU variants, significantly degrade the performance of the quantized LLM. We denote these activations as activation spikes. Our further observations provide a systematic pattern of activation spikes: 1) The activation spikes occur in the FFN of specific layers, particularly in the early and late layers, 2) The activation spikes are dedicated to a couple of tokens, rather than being shared across a sequence. Based on our observations, we propose two empirical methods, Quantization-free Module (QFeM) and Quantization-free Prefix (QFeP), to isolate the activation spikes during quantization. Our extensive experiments validate the effectiveness of the proposed methods for the activation quantization, especially with coarse-grained scheme, of latest LLMs with GLU variants, including LLaMA-2/3, Mistral, Mixtral, SOLAR, and Gemma.\nIn particular, our methods enhance the current alleviation techniques (e.g., SmoothQuant) that fail to control the activation spikes.\n\nTLDR:\n\nKeywords:quantization, LLM, post-training quantization, outliers"", ""claims"": [""Claim1: Our extensive experiments validate the effectiveness of the proposed methods for the activation quantization, especially with coarse-grained scheme, of latest LLMs with GLU variants, including LLaMA-2/3, Mistral, Mixtral, SOLAR, and Gemma."", ""Claim2: In particular, our methods enhance the current alleviation techniques (e.g., SmoothQuant) that fail to control the activation spikes."", ""Claim3: In this paper, we present our discovery that the GLU architecture in the feed-forward network (FFN) generates excessively large activation values, which are responsible for significant local quantization errors."", ""Claim4: In light of our observations, we propose two empirical methods to mitigate the impact of activation spikes on quantization: Quantization-free Module (QFeM) and Quantization-free Prefix (QFeP)."", ""Claim5: In our comprehensive experiments, we demonstrate that recently released LLMs incorporating GLU variants struggle with activation spikes when applying activation quantization."", ""Claim6: Consequently, the proposed methods, QFeM and QFeP, substantially enhance the performance of the primitive quantization method, the round-to-nearest (RTN) method."", ""Claim7: Furthermore, we observe that current outlier alleviation methods [50, 51] are exposed to the activation spikes and benefit from our proposed methods."", ""Claim8: Compared to the strong baseline of fine-grained activation quantization [55], our methods show competitive performance, achieving reduced latency and memory footprint."", ""Claim9: Our work provides a module-level investigation where quantization is applied practically, focusing on different LLM architectures."", ""Claim10: Along this line of research, we propose to enhance the activation quantization based on our observations."", ""Claim11: Implemented LLMs exhibit activation spikes at specific layers."", ""Claim12: Within the early and late layers, the down modules in the FFN show noticeable magnitudes of input activations."", ""Claim13: The GLU variants generate activation spikes at the specific layers."", ""Claim14: We notice a high correlation between the emergence of activation spikes and intermediate hidden states of large scale."", ""Claim15: Non GLU-implemented LLMs show modest scale distribution."", ""Claim16: We reveal that GLU variants are associated with the hidden states and generate activation spikes."", ""Claim17: This clarifies the quantization challenge of the GLU-implemented LLMs concentrated in the early and late layers."", ""Claim18: As shown in Table 1, quantization on the top-4 rated modules solely degrades the LLM performance by significant margins, while the other cases exhibit negligible performance changes."", ""Claim19: We consider these quantization-sensitive input activations (inter alia activation spikes) to be the quantization bottleneck, which, in this paper, refers to the quantization error caused by outliers."", ""Claim20: Altogether, such dynamic bottlenecks must be handled with caution to enhance the quantization performance of LLMs."", ""Claim21: Orthogonal to the QFeM, we propose Quantization-free Prefix (QFeP) that mitigates the quantization errors by precomputing the prefix (or short prompt) corresponding to activation spikes."", ""Claim22: This approach provides a bypass of the quantization including activation spikes, while preserving the context of prefix through the KV cache. The KV cache for the prefix is precomputed once through the offline inference of LLM without quantization. Then, this KV cache is exploited in the quantization phases, such as calibration or dynamic quantization, even for quantized inference."", ""Claim23: The results demonstrate that our proposed methods resolve the activation spikes and, surprisingly, restore the performance of the W8A8 close to that of FP16."", ""Claim24: For example, the LLaMA-2 7B model achieves less than a 1% performance drop from FP16."", ""Claim25: It is worth noting that the proposed QFeM and QFeP improve at comparable levels."", ""Claim26: This indicates that the activation spikes present a direct cause of the significant decrease in quantization performance."", ""Claim27: Noticeably, the Mixtral model demonstrates robustness towards the performance degradation."", ""Claim28: Meanwhile, addressing the activation spikes is not a sufficient complement for the Gemma model compared to other models."", ""Claim29: We attribute this to the choice of activation function among GLU variants; specifically, Gemma uses GeGLU, while other models employ SwiGLU."", ""Claim30: We find that there are cases where the alleviation method fails to recover the performance when quantizing the activations with per-tensor scheme."", ""Claim31: This indicates that alleviating the outlier scales, including the activation spikes, is challenging."", ""Claim32: With the QFeM, the activation spikes are excluded, and the accurate alleviation is enabled."", ""Claim33: In addition, the QFeP also benefits from the SQ method, as seen in the case of LLaMA-2 70B."", ""Claim34: Exceptionally, the OSP successfully addresses the activation spikes in the 13B and 70B cases."", ""Claim35: our QFeP consistently shows significant improvement."", ""Claim36: our KV design for QFeP shows improvements by large margins across all models."", ""Claim37: the coarse-grained schemes achieve a competitive performance gain."", ""Claim38: the combination of AQ2 and QFeM demonstrates the performance close to the AQ1 but with faster latency."", ""Claim39: Specifically, the naive static quantization (AQ3), the fastest scheme, exhibits a significant decline."", ""Claim40: this memory overhead is much smaller than that used in the fine-grained quantization (AQ1)"", ""Claim41: the 7B model with QFeM exhibits memory usage similar to AQ2, while the 70B model with QFeM incur additional consumption for a sequence length of 1K."", ""Claim42: We explore the quantization challenge of GLU activations for modern LLMs."", ""Claim43: we find that the GLU variants generates excessive activation scales, which cause significant quantization bottlenecks at the specific layers."", ""Claim44: we propose methods that address the spikes in a layer-wise (QFeM) and token-wise manner (QFeP)."", ""Claim45: In the experiments, we confirm that the proposed methods effectively resolve the quantization bottlenecks and result in a large performance gain."", ""Claim46: Regardless of the BMM quantization, the QFeM method consistently improves the quantization bottleneck."", ""Claim47: For example, the 13B and 70B models maintain their performance, while the 7B model shows a slight decrease."", ""Claim48: As a result, we confirm that our QFeM method effectively improves the overall performance even in the BMM quantization scenario."", ""Claim49: Compared to the results obtained with per-channel weight quantization in Table 4, these results elucidate the negative impact of activation spikes on the performance of outlier alleviation methods."", ""Claim50: Nevertheless, the proposed methods, QFeM and QFeP, consistently improve the effectiveness of outlier alleviation methods by mitigating the impact of activation spikes.""], ""review"": ""Review: Weakness: 1) The perplexity/accuracy results of the baseline methods deviate from the results reported in previous papers.\n\n2) The paper does not compare its method with the state-of-the-art LLM quantization method [1], which enables W4A4 quantization (partially using 8-bit operations) with a PTQ approach.\n\n[1] Zhao, Yilong, et al. \""Atom: Low-bit quantization for efficient and accurate llm serving.\"" Proceedings of Machine Learning and Systems 6 (2024): 196-209.\nQuestions: 1) The perplexity and accuracy results of the baseline methods (SQ [1] and OSP [2]) in Table 4 are worse than the figures reported in the original SQ and OSP papers. For instance, the SQ paper reported successful preservation of LLM perplexity after W8A8 quantization (Table 7 of [1]), but Table 4 of this paper shows poor perplexity and accuracy results for SQ. Additionally, the OSP paper claimed successful preservation of LLM perplexity even after INT6 quantization and reported better perplexity results compared to SQ (Table 2 of [2]). Although OSP only evaluated LLaMA-1 and there is no LLaMA-2 data, we can reasonably expect similar trends for LLaMA-2 given that both use GLU-based activation functions. However, Table 4 of this paper shows poor perplexity and accuracy results for OSP, particularly for LLaMA-2-7B. Why are the evaluation results for the previous methods so different?\n\n2) Since the prefix token is retrieved from the calibration set and the threshold alpha is determined from it, will these parameters remain consistent when inferring on a new dataset using the same model?\n\n3) What are the advantages of the proposed method compared to Atom [3] in terms of perplexity/accuracy, latency, or other aspects?\n\n[1] Xiao, Guangxuan, et al. \""Smoothquant: Accurate and efficient post-training quantization for large language models.\"" International Conference on Machine Learning. 2023.\n\n[2] Wei, Xiuying, et al. \""Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling.\"" arXiv preprint arXiv:2304.09145 (2023).\n\n[3] Zhao, Yilong, et al. \""Atom: Low-bit quantization for efficient and accurate llm serving.\"" Proceedings of Machine Learning and Systems 6 (2024): 196-209.\n\n""}, ""review"": ""Title: Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs\nAbstract: Modern large language models (LLMs) have established state-of-the-art performance through architectural improvements, but still require significant computational cost for inference. In an effort to reduce the inference cost, post-training quantization (PTQ) has become a popular approach, quantizing weights and activations to lower precision, such as INT8. In this paper, we reveal the challenges of activation quantization in GLU variants, which are widely used in feed-forward network (FFN) of modern LLMs, such as LLaMA family.\nThe problem is that severe local quantization errors, caused by excessive magnitudes of activation in GLU variants, significantly degrade the performance of the quantized LLM. We denote these activations as activation spikes. Our further observations provide a systematic pattern of activation spikes: 1) The activation spikes occur in the FFN of specific layers, particularly in the early and late layers, 2) The activation spikes are dedicated to a couple of tokens, rather than being shared across a sequence. Based on our observations, we propose two empirical methods, Quantization-free Module (QFeM) and Quantization-free Prefix (QFeP), to isolate the activation spikes during quantization. Our extensive experiments validate the effectiveness of the proposed methods for the activation quantization, especially with coarse-grained scheme, of latest LLMs with GLU variants, including LLaMA-2/3, Mistral, Mixtral, SOLAR, and Gemma.\nIn particular, our methods enhance the current alleviation techniques (e.g., SmoothQuant) that fail to control the activation spikes.\n\nClaim1: Our extensive experiments validate the effectiveness of the proposed methods for the activation quantization, especially with coarse-grained scheme, of latest LLMs with GLU variants, including LLaMA-2/3, Mistral, Mixtral, SOLAR, and Gemma.\nClaim2: In particular, our methods enhance the current alleviation techniques (e.g., SmoothQuant) that fail to control the activation spikes.\nClaim3: In this paper, we present our discovery that the GLU architecture in the feed-forward network (FFN) generates excessively large activation values, which are responsible for significant local quantization errors.\nClaim4: In light of our observations, we propose two empirical methods to mitigate the impact of activation spikes on quantization: Quantization-free Module (QFeM) and Quantization-free Prefix (QFeP).\nClaim5: In our comprehensive experiments, we demonstrate that recently released LLMs incorporating GLU variants struggle with activation spikes when applying activation quantization.\nClaim6: Consequently, the proposed methods, QFeM and QFeP, substantially enhance the performance of the primitive quantization method, the round-to-nearest (RTN) method.\nClaim7: Furthermore, we observe that current outlier alleviation methods [50, 51] are exposed to the activation spikes and benefit from our proposed methods.\nClaim8: Compared to the strong baseline of fine-grained activation quantization [55], our methods show competitive performance, achieving reduced latency and memory footprint.\nClaim9: Our work provides a module-level investigation where quantization is applied practically, focusing on different LLM architectures.\nClaim10: Along this line of research, we propose to enhance the activation quantization based on our observations.\nClaim11: Implemented LLMs exhibit activation spikes at specific layers.\nClaim12: Within the early and late layers, the down modules in the FFN show noticeable magnitudes of input activations.\nClaim13: The GLU variants generate activation spikes at the specific layers.\nClaim14: We notice a high correlation between the emergence of activation spikes and intermediate hidden states of large scale.\nClaim15: Non GLU-implemented LLMs show modest scale distribution.\nClaim16: We reveal that GLU variants are associated with the hidden states and generate activation spikes.\nClaim17: This clarifies the quantization challenge of the GLU-implemented LLMs concentrated in the early and late layers.\nClaim18: As shown in Table 1, quantization on the top-4 rated modules solely degrades the LLM performance by significant margins, while the other cases exhibit negligible performance changes.\nClaim19: We consider these quantization-sensitive input activations (inter alia activation spikes) to be the quantization bottleneck, which, in this paper, refers to the quantization error caused by outliers.\nClaim20: Altogether, such dynamic bottlenecks must be handled with caution to enhance the quantization performance of LLMs.\nClaim21: Orthogonal to the QFeM, we propose Quantization-free Prefix (QFeP) that mitigates the quantization errors by precomputing the prefix (or short prompt) corresponding to activation spikes.\nClaim22: This approach provides a bypass of the quantization including activation spikes, while preserving the context of prefix through the KV cache. The KV cache for the prefix is precomputed once through the offline inference of LLM without quantization. Then, this KV cache is exploited in the quantization phases, such as calibration or dynamic quantization, even for quantized inference.\nClaim23: The results demonstrate that our proposed methods resolve the activation spikes and, surprisingly, restore the performance of the W8A8 close to that of FP16.\nClaim24: For example, the LLaMA-2 7B model achieves less than a 1% performance drop from FP16.\nClaim25: It is worth noting that the proposed QFeM and QFeP improve at comparable levels.\nClaim26: This indicates that the activation spikes present a direct cause of the significant decrease in quantization performance.\nClaim27: Noticeably, the Mixtral model demonstrates robustness towards the performance degradation.\nClaim28: Meanwhile, addressing the activation spikes is not a sufficient complement for the Gemma model compared to other models.\nClaim29: We attribute this to the choice of activation function among GLU variants; specifically, Gemma uses GeGLU, while other models employ SwiGLU.\nClaim30: We find that there are cases where the alleviation method fails to recover the performance when quantizing the activations with per-tensor scheme.\nClaim31: This indicates that alleviating the outlier scales, including the activation spikes, is challenging.\nClaim32: With the QFeM, the activation spikes are excluded, and the accurate alleviation is enabled.\nClaim33: In addition, the QFeP also benefits from the SQ method, as seen in the case of LLaMA-2 70B.\nClaim34: Exceptionally, the OSP successfully addresses the activation spikes in the 13B and 70B cases.\nClaim35: our QFeP consistently shows significant improvement.\nClaim36: our KV design for QFeP shows improvements by large margins across all models.\nClaim37: the coarse-grained schemes achieve a competitive performance gain.\nClaim38: the combination of AQ2 and QFeM demonstrates the performance close to the AQ1 but with faster latency.\nClaim39: Specifically, the naive static quantization (AQ3), the fastest scheme, exhibits a significant decline.\nClaim40: this memory overhead is much smaller than that used in the fine-grained quantization (AQ1)\nClaim41: the 7B model with QFeM exhibits memory usage similar to AQ2, while the 70B model with QFeM incur additional consumption for a sequence length of 1K.\nClaim42: We explore the quantization challenge of GLU activations for modern LLMs.\nClaim43: we find that the GLU variants generates excessive activation scales, which cause significant quantization bottlenecks at the specific layers.\nClaim44: we propose methods that address the spikes in a layer-wise (QFeM) and token-wise manner (QFeP).\nClaim45: In the experiments, we confirm that the proposed methods effectively resolve the quantization bottlenecks and result in a large performance gain.\nClaim46: Regardless of the BMM quantization, the QFeM method consistently improves the quantization bottleneck.\nClaim47: For example, the 13B and 70B models maintain their performance, while the 7B model shows a slight decrease.\nClaim48: As a result, we confirm that our QFeM method effectively improves the overall performance even in the BMM quantization scenario.\nClaim49: Compared to the results obtained with per-channel weight quantization in Table 4, these results elucidate the negative impact of activation spikes on the performance of outlier alleviation methods.\nClaim50: Nevertheless, the proposed methods, QFeM and QFeP, consistently improve the effectiveness of outlier alleviation methods by mitigating the impact of activation spikes.\n\nReview: Weakness: 1) The perplexity/accuracy results of the baseline methods deviate from the results reported in previous papers.\n\n2) The paper does not compare its method with the state-of-the-art LLM quantization method [1], which enables W4A4 quantization (partially using 8-bit operations) with a PTQ approach.\n\n[1] Zhao, Yilong, et al. \""Atom: Low-bit quantization for efficient and accurate llm serving.\"" Proceedings of Machine Learning and Systems 6 (2024): 196-209.\nQuestions: 1) The perplexity and accuracy results of the baseline methods (SQ [1] and OSP [2]) in Table 4 are worse than the figures reported in the original SQ and OSP papers. For instance, the SQ paper reported successful preservation of LLM perplexity after W8A8 quantization (Table 7 of [1]), but Table 4 of this paper shows poor perplexity and accuracy results for SQ. Additionally, the OSP paper claimed successful preservation of LLM perplexity even after INT6 quantization and reported better perplexity results compared to SQ (Table 2 of [2]). Although OSP only evaluated LLaMA-1 and there is no LLaMA-2 data, we can reasonably expect similar trends for LLaMA-2 given that both use GLU-based activation functions. However, Table 4 of this paper shows poor perplexity and accuracy results for OSP, particularly for LLaMA-2-7B. Why are the evaluation results for the previous methods so different?\n\n2) Since the prefix token is retrieved from the calibration set and the threshold alpha is determined from it, will these parameters remain consistent when inferring on a new dataset using the same model?\n\n3) What are the advantages of the proposed method compared to Atom [3] in terms of perplexity/accuracy, latency, or other aspects?\n\n[1] Xiao, Guangxuan, et al. \""Smoothquant: Accurate and efficient post-training quantization for large language models.\"" International Conference on Machine Learning. 2023.\n\n[2] Wei, Xiuying, et al. \""Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling.\"" arXiv preprint arXiv:2304.09145 (2023).\n\n[3] Zhao, Yilong, et al. \""Atom: Low-bit quantization for efficient and accurate llm serving.\"" Proceedings of Machine Learning and Systems 6 (2024): 196-209.\n\n\n\n"", ""id"": ""t8ch1OCvHh0"", ""pdf"": ""openreview.net/pdf?id=t8ch1OCvHh""}","32"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""- Some claims may be misleading"", ""Target claims"": [], ""Reasoning"": ""The review mentions that some claims in the paper might be misleading, but it does not specify which claims these are. Therefore, there's no direct target claim indicated for this weakness. While this statement flags a potential issue with the claims, it does not provide explicit mention or argumentation related to any specific claim from the list.""}, {""Weakness span"": ""- RetNet's performance lags behind Transformers at smaller model scales, suggesting it might be more demanding in terms of capacity and compute resources for optimal performance. This trade-off should be carefully considered and analyzed."", ""Target claims"": [8, 19, 20], ""Reasoning"": ""This weakness points out that RetNet's performance is not as good as Transformers at smaller model scales, questioning the claim that RetNet is a potential candidate to replace Transformers for large language models (Claim 8), that it is a strong competitor to Transformer (Claim 19), and that it starts outperforming Transformers for models larger than 2B, implying issues with smaller scales (Claim 20). This contradicts the implication of broad competitiveness with Transformers regardless of scale, hence targeting these claims.""}, {""Weakness span"": ""The claim that 'None of the previous work can achieve strong performance and efficient inference at the same time compared with Transformers' is overly strong and potentially misleading. Recent advancements in efficient modeling, such as Mamba, have demonstrated better scaling properties than Transformers."", ""Target claims"": [1, 11], ""Reasoning"": ""This weakness directly targets the notion that the claims in the paper might overlook the performance of existing models like Mamba, thereby questioning general statements about RetNet's efficiency or unique standing. This targets Claim 1, which is about RetNet\u2019s efficiency and performance, and Claim 11 which involves comparisons with efficient Transformer variants, like Mamba, which is cited here as having demonstrated better scaling properties.""}]}, ""meta"": {""id"": ""sxZlp9ZoHD"", ""review_idx"": 0, ""title"": ""Title: Retentive Network\nAbstract: In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost \n inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference.\n\nTLDR:RetNet is a foundation architecture, simultaneously achieving training parallelism, low-cost inference, and good performance.\n\nKeywords:Retentive Network, Model Architecture"", ""claims"": [""Claim1: In this work, we propose retentive networks (RetNet), achieving low-cost inference, efficient long-sequence modeling, Transformer-comparable performance, and parallel model training simultaneously."", ""Claim2: We compare RetNet with Transformer and its variants."", ""Claim3: Experimental results on language modeling show that RetNet is consistently competitive in terms of both scaling curves and in-context learning."", ""Claim4: Moreover, the inference cost of RetNet is length-invariant."", ""Claim5: For a 7B model and 8k sequence length, RetNet decodes 8.4 \u00d7 faster and saves 70% of memory than Transformers with key-value caches."", ""Claim6: During training, RetNet also achieves 3 \u00d7 acceleration than standard Transformer with highly-optimized FlashAttention-2 [10]."", ""Claim7: Besides, RetNet\u2019s inference latency is insensitive to batch size, allowing enormous throughput."", ""Claim8: The intriguing properties make RetNet a potential candidate to replace Transformer for large language models."", ""Claim9: Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference."", ""Claim10: We perform language modeling experiments to evaluate RetNet."", ""Claim11: We compare RetNet with various efficient Transformer variants, including RWKV [36], H3 [11], Hyena [38], and Mamba [19]."", ""Claim12: For RWKV, we use the TimeMix module to substitute self-attention layers while keeping FFN layers consistent with other models for fair comparisons."", ""Claim13: As shown in Table 1, we first report the language modeling perplexity of validation sets."", ""Claim14: RetNet outperforms previous methods on both \u201cAR-Hit\u201d and \u201cFirst Occur\u201d splits, which is important for real-world use cases."", ""Claim15: RetNet achieves competitive results among the architectures."", ""Claim16: RetNet achieves comparable results with Transformers."", ""Claim17: More importantly, the results indicate that RetNet is favorable in terms of size scaling."", ""Claim18: In addition to performance, RetNet training is quite stable in our experiments."", ""Claim19: Experimental results show that RetNet is a strong competitor to Transformer for large language models."", ""Claim20: Empirically, we find that RetNet starts to outperform Transformer when the model size is larger than 2B."", ""Claim21: Moreover, RetNet+ obtains competitive results compared with Transformer for long-context modeling."", ""Claim22: Meanwhile, retention has better training and inference efficiency."", ""Claim23: Figure 5 shows that RetNet outperforms Transformer in terms of inference cost."", ""Claim24: As shown in Figure 5a, the memory cost of Transformer increases linearly due to KV caches."", ""Claim25: In contrast, the memory consumption of RetNet remains consistent even for long sequences, requiring much less GPU memory to host RetNet."", ""Claim26: The additional memory consumption of RetNet is almost negligible (i.e., about 3%) while the model weights occupy 97%."", ""Claim27: As presented in Figure 5b, the throughput of Transformer drops along with the decoding length increases."", ""Claim28: In comparison, RetNet has higher and length-invariant throughput during decoding, by utilizing the recurrent representation of retention."", ""Claim29: Experimental results show that increasing batch size renders the Transformer\u2019s latency larger."", ""Claim30: Moreover, the latency of Transformers grows faster with longer input."", ""Claim31: By contrast, RetNet\u2019s decoding latency outperforms Transformers and stays almost the same across different batch sizes and input lengths."", ""Claim32: Experimental results show that RetNet has higher training throughput than Transformers."", ""Claim33: The acceleration ratio increases as the sequence length is longer."", ""Claim34: When the training length is 64k, RetNet\u2019s throughput is about 3 times than Transformer\u2019s."", ""Claim35: RetNet achieves comparable performance with Transformer on zero-shot and in-context learning settings."", ""Claim36: Table 3 indicates that both the decay mechanism and using multiple decay rates can improve the language modeling performance."", ""Claim37: Table 3 shows that the larger head dimension achieves better performance."", ""Claim38: Table 4 shows the results across various vision tasks."", ""Claim39: RetNet is competitive compared with DeiT."", ""Claim40: For classification and segmentation, RetNet is slightly better than DeiT, where RetNet achieves 0.81% accuracy improvement on ImageNet and 0.61% mIoU improvement on ADE20K."", ""Claim41: For object detection, the results are comparable."", ""Claim42: Retention preserves high-dimensional states to encode sequence information, which contributes to expressive ability and better performance."", ""Claim43: RetNet achieves significantly better inference efficiency (in terms of memory, speed, and latency), favorable training parallelization, and competitive performance compared with Transformers."", ""Claim44: RetNet+ achieves a performance comparable to Transformer Repro on language tasks."", ""Claim45: Experimental results show that RetNet performs comparably with Transformer in different context lengths."", ""Claim46: As shown in Table 10, we compare the inference cost with grouped-query attention and apply the method for RetNet."", ""Claim47: When equipped with grouped-query retention, RetNet-70B achieves 38% acceleration and saves 30% memory."", ""Claim48: Table 10 shows that the inference cost of Transformers increases with the sequence length. In contrast, RetNet is length-invariant."", ""Claim49: Moreover, RetNet-70B-GQ2 achieves better latency, throughput, and GPU memory than LLaMA2-70B-2k/8k equipped with grouped-query attention.""], ""review"": ""Review: Weakness: - Some experiments could be improved\n- Some claims may be misleading\n- RetNet's performance lags behind Transformers at smaller model scales, suggesting it might be more demanding in terms of capacity and compute resources for optimal performance. This trade-off should be carefully considered and analyzed.\nQuestions: 1. The inference results in Figure 5 start with 2048. What\u2019s the inference speed for shorter sequences?\n2. The claim that \""None of the previous work can achieve strong performance and efficient inference at the same time compared with Transformers\"" is overly strong and potentially misleading. Recent advancements in efficient modeling, such as Mamba, have demonstrated better scaling properties than Transformers.\n3. It would be great to include the training loss curve for both Transformer and RetNet.\n\n""}, ""review"": ""Title: Retentive Network\nAbstract: In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost \n inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference.\n\nClaim1: In this work, we propose retentive networks (RetNet), achieving low-cost inference, efficient long-sequence modeling, Transformer-comparable performance, and parallel model training simultaneously.\nClaim2: We compare RetNet with Transformer and its variants.\nClaim3: Experimental results on language modeling show that RetNet is consistently competitive in terms of both scaling curves and in-context learning.\nClaim4: Moreover, the inference cost of RetNet is length-invariant.\nClaim5: For a 7B model and 8k sequence length, RetNet decodes 8.4 \u00d7 faster and saves 70% of memory than Transformers with key-value caches.\nClaim6: During training, RetNet also achieves 3 \u00d7 acceleration than standard Transformer with highly-optimized FlashAttention-2 [10].\nClaim7: Besides, RetNet\u2019s inference latency is insensitive to batch size, allowing enormous throughput.\nClaim8: The intriguing properties make RetNet a potential candidate to replace Transformer for large language models.\nClaim9: Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference.\nClaim10: We perform language modeling experiments to evaluate RetNet.\nClaim11: We compare RetNet with various efficient Transformer variants, including RWKV [36], H3 [11], Hyena [38], and Mamba [19].\nClaim12: For RWKV, we use the TimeMix module to substitute self-attention layers while keeping FFN layers consistent with other models for fair comparisons.\nClaim13: As shown in Table 1, we first report the language modeling perplexity of validation sets.\nClaim14: RetNet outperforms previous methods on both \u201cAR-Hit\u201d and \u201cFirst Occur\u201d splits, which is important for real-world use cases.\nClaim15: RetNet achieves competitive results among the architectures.\nClaim16: RetNet achieves comparable results with Transformers.\nClaim17: More importantly, the results indicate that RetNet is favorable in terms of size scaling.\nClaim18: In addition to performance, RetNet training is quite stable in our experiments.\nClaim19: Experimental results show that RetNet is a strong competitor to Transformer for large language models.\nClaim20: Empirically, we find that RetNet starts to outperform Transformer when the model size is larger than 2B.\nClaim21: Moreover, RetNet+ obtains competitive results compared with Transformer for long-context modeling.\nClaim22: Meanwhile, retention has better training and inference efficiency.\nClaim23: Figure 5 shows that RetNet outperforms Transformer in terms of inference cost.\nClaim24: As shown in Figure 5a, the memory cost of Transformer increases linearly due to KV caches.\nClaim25: In contrast, the memory consumption of RetNet remains consistent even for long sequences, requiring much less GPU memory to host RetNet.\nClaim26: The additional memory consumption of RetNet is almost negligible (i.e., about 3%) while the model weights occupy 97%.\nClaim27: As presented in Figure 5b, the throughput of Transformer drops along with the decoding length increases.\nClaim28: In comparison, RetNet has higher and length-invariant throughput during decoding, by utilizing the recurrent representation of retention.\nClaim29: Experimental results show that increasing batch size renders the Transformer\u2019s latency larger.\nClaim30: Moreover, the latency of Transformers grows faster with longer input.\nClaim31: By contrast, RetNet\u2019s decoding latency outperforms Transformers and stays almost the same across different batch sizes and input lengths.\nClaim32: Experimental results show that RetNet has higher training throughput than Transformers.\nClaim33: The acceleration ratio increases as the sequence length is longer.\nClaim34: When the training length is 64k, RetNet\u2019s throughput is about 3 times than Transformer\u2019s.\nClaim35: RetNet achieves comparable performance with Transformer on zero-shot and in-context learning settings.\nClaim36: Table 3 indicates that both the decay mechanism and using multiple decay rates can improve the language modeling performance.\nClaim37: Table 3 shows that the larger head dimension achieves better performance.\nClaim38: Table 4 shows the results across various vision tasks.\nClaim39: RetNet is competitive compared with DeiT.\nClaim40: For classification and segmentation, RetNet is slightly better than DeiT, where RetNet achieves 0.81% accuracy improvement on ImageNet and 0.61% mIoU improvement on ADE20K.\nClaim41: For object detection, the results are comparable.\nClaim42: Retention preserves high-dimensional states to encode sequence information, which contributes to expressive ability and better performance.\nClaim43: RetNet achieves significantly better inference efficiency (in terms of memory, speed, and latency), favorable training parallelization, and competitive performance compared with Transformers.\nClaim44: RetNet+ achieves a performance comparable to Transformer Repro on language tasks.\nClaim45: Experimental results show that RetNet performs comparably with Transformer in different context lengths.\nClaim46: As shown in Table 10, we compare the inference cost with grouped-query attention and apply the method for RetNet.\nClaim47: When equipped with grouped-query retention, RetNet-70B achieves 38% acceleration and saves 30% memory.\nClaim48: Table 10 shows that the inference cost of Transformers increases with the sequence length. In contrast, RetNet is length-invariant.\nClaim49: Moreover, RetNet-70B-GQ2 achieves better latency, throughput, and GPU memory than LLaMA2-70B-2k/8k equipped with grouped-query attention.\n\nReview: Weakness: - Some experiments could be improved\n- Some claims may be misleading\n- RetNet's performance lags behind Transformers at smaller model scales, suggesting it might be more demanding in terms of capacity and compute resources for optimal performance. This trade-off should be carefully considered and analyzed.\nQuestions: 1. The inference results in Figure 5 start with 2048. What\u2019s the inference speed for shorter sequences?\n2. The claim that \""None of the previous work can achieve strong performance and efficient inference at the same time compared with Transformers\"" is overly strong and potentially misleading. Recent advancements in efficient modeling, such as Mamba, have demonstrated better scaling properties than Transformers.\n3. It would be great to include the training loss curve for both Transformer and RetNet.\n\n\n\n"", ""id"": ""sxZlp9ZoHD0"", ""pdf"": ""openreview.net/pdf?id=sxZlp9ZoHD""}","33"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""The paper seems to make claims that are unsubstantiated by experiments. In the abstract and introduction, the paper claims speedup without sacrificing model quality. However, there is no experiment evaluating model quality in the experiments. This is a critical flaw."", ""Target claims"": [6], ""Reasoning"": ""The reviewer notes that the paper claims speedup without sacrificing model quality, specifically mentioned in Claim 6, which states that FlashMask enhances training speed without sacrificing model accuracy. The reviewer argues this is unsubstantiated as there is no experimental evaluation of model quality provided to support this claim. Thus, Claim 6 is directly questioned here.""}, {""Weakness span"": ""Further, the contribution of the paper is unclear. Block-sparsity is already supported in FlashAttention (see section 3.3 of FlashAttention). It is unclear how this paper is different. There are also more recent works such as \""Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention\"" (NeurIPS 2023), which seem to be strictly more expressive in features than this paper."", ""Target claims"": [1, 9, 28, 39], ""Reasoning"": ""The reviewer questions the novelty and contributions claimed in the paper, which involve the introduction of FlashMask designed to reduce computational complexity (Claim 1), deliver exact computations (Claim 9), mitigate limitations of existing methods (Claim 28), and enhance processing speeds (Claim 39). The reviewer suggests that similar features like block-sparsity are already supported in existing methods like FlashAttention and refers to more recent works being more expressive. Hence, these claims are directly under question due to lack of differentiation and potential novelty.""}]}, ""meta"": {""id"": ""rog0J435OO"", ""review_idx"": 0, ""title"": ""Title: FlashMask: Reducing the Complexity of Attention Computation through Sparse Mask Representation\nAbstract: Recent advancements in Larger-Scale Transformers have significantly benefited from sophisticated attention mechanisms, which are critical for modeling long-context sequences. However, the computational and memory demands of conventional attention mask computations, typically scaling with an $\\mathcal{O}(N^2)$ complexity where $N$ is the sequence length, pose significant challenges. This paper introduces FlashMask, a simple yet effective \\emph{Exact} attention algorithm designed to substantially reduce both the computational complexity and memory requirements of attention computations. By adopting a novel column-wise sparse representation of attention masks, FlashMask achieves a linear memory complexity of $\\mathcal{O}(N)$ and computational complexity of $\\mathcal{O}(N)\\sim\\mathcal{O}(N^2)$. We assess the performance of FlashMask in a variety of masking scenarios, including causal and customized attention masks, demonstrating its versatility and robustness across a wide range of attention patterns and models. Our empirical analysis encompasses a variety of downstream training modalities, including Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reward Model (RM). We compare FlashMask against state-of-the-art techniques, including notably FlashAttention. In kernel-level assessments, FlashMask achieves substantial computational speedups, up to 6.7x (SFT), 6.9x (DPO), and 8.3x (RM). Furthermore, in end-to-end training, FlashMask consistently enhances training speed significantly, with accelerations up to 2.4x (SFT), 4.2x (LoRA), 2.5x (DPO), and 2.6x (RM) across these varied scenarios without sacrificing model accuracy. Additionally, when implemented in the LoRA scenario, FlashMask enables the LLaMA2-7B to process sequence lengths of up to 544k, significantly enhancing its capability for long-context input.\n\nTLDR:\n\nKeywords:FlashMask, Efficient Attention Computation, Sparse Mask Representation, Linear Memory Complexity, Low Computational Complexity"", ""claims"": [""Claim1: This paper introduces FlashMask, a simple yet effective \\emph{Exact} attention algorithm designed to substantially reduce both the computational complexity and memory requirements of attention computations."", ""Claim2: By adopting a novel column-wise sparse representation of attention masks, FlashMask achieves a linear memory complexity of \\(\\mathcal{O}(N)\\) and computational complexity of \\(\\mathcal{O}(N)\\sim\\mathcal{O}(N^2)\\)."", ""Claim3: We assess the performance of FlashMask in a variety of masking scenarios, including causal and customized attention masks, demonstrating its versatility and robustness across a wide range of attention patterns and models."", ""Claim4: Our empirical analysis encompasses a variety of downstream training modalities, including Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reward Model (RM)."", ""Claim5: In kernel-level assessments, FlashMask achieves substantial computational speedups, up to 6.7x (SFT), 6.9x (DPO), and 8.3x (RM)."", ""Claim6: Furthermore, in end-to-end training, FlashMask consistently enhances training speed significantly, with accelerations up to 2.4x (SFT), 4.2x (LoRA), 2.5x (DPO), and 2.6x (RM) across these varied scenarios without sacrificing model accuracy."", ""Claim7: Additionally, when implemented in the LoRA scenario, FlashMask enables the LLaMA2-7B to process sequence lengths of up to 544k, significantly enhancing its capability for long-context input."", ""Claim8: This paper introduces FlashMask, a novel approach utilizing a sparse mask representation to accelerate attention computations in transformers, effectively addressing both computational and memory scala- bility issues."", ""Claim9: Conversely, our proposed FlashMask is capable of delivering an exact computations."", ""Claim10: It is important to note that unlike various approximate attention algorithms, our method ensures that each effective element of the attention score matrix is computed identically to FlashAttention-2, with masked elements explicitly set to \u2212\u221e, thus maintaining the accuracy of the algorithm\u2019s results."", ""Claim11: Futhermore, FlashMask is easily extendable to bidirectional attention computations."", ""Claim12: The total space complexity for FlashMask is O( N ), significantly reducing memory usage and supporting training on longer sequences."", ""Claim13: Memory access complexity. The dense mask accesses the entire D \u2208 R N \u00d7 N matrix in line 20 of Algorithm 1, totaling N 2 memory accesses on HBM."", ""Claim14: Computational complexity. The attention computation process normally iterates over the entire attention score matrix, with a computational complexity of O( N 2 ). By skipping entirely masked blocks, FlashMask leverages block sparsity to reduce computational complexity to O(( 1 \u2212 \u03b1 ) N 2 )."", ""Claim15: Experiments were conducted using GPU A800-SXM 80G, Intel(R) Xeon(R) Platinum 8350C CPUs, CUDA 12.0, and driver version 525.125.06."", ""Claim16: We evaluated FlashMask against various methods including Vanilla Attention, FlashAttention with dense mask (FA-DenseMask), variable length (FA214Varlen), and sliding window (FA-Window) across different scenarios and sequence lengths."", ""Claim17: Both kernel-level and end-to-end performance demonstrated the effectiveness of our method."", ""Claim18: For comprehensive testing of FlashMask, we constructed synthetic data to simulate long-sequence training."", ""Claim19: FlashMask demonstrated significant latency advantages across all lengths, up to 8.3-fold time saving compared to FA-DenseMask."", ""Claim20: Performance benefits from varying sparsity levels were also quantified, with FlashMask showing linear negative correlation with increasing sparsity, demonstrating efficient utilization of sample sparsity for acceleration."", ""Claim21: The end-to-end performance 1 of the model was tested using synthetic datasets across three scales of the LLaMA2 model and four downstream scenarios (SFT, LoRA, DPO, RM) at various sequence lengths, measuring throughput in average Tokens/Sec/GPU."", ""Claim22: FlashMask consistently showed significant advantages."", ""Claim23: Notably, in the LoRA scenario at the LLaMA2-7B, FlashMask achieved a 4.16x throughput improvement over FA-DenseMask, supporting sequence lengths up to 544K."", ""Claim24: Additional experiments were conducted on the open-source dataset LongBench [45], comparing the end-to-end performance of FA-DenseMask, FA-Varlen, and FlashMask at sequence lengths of 16K, 32K, and 64K."", ""Claim25: The performance improvements were consistent with those observed in the synthetic dataset."", ""Claim26: Memory usage during the experiments was also recorded, showing significant reductions for FlashMask compared to FA-DenseMask."", ""Claim27: The substantial advancement rendered by FlashMask in improving attention mask computation is a significant evolution over the current FlashAttention framework."", ""Claim28: Notably, FlashMask addresses and significantly mitigates the limitations observed with FlashAttention in handling conventional and custom mask computations."", ""Claim29: This enhancement not only broadens the applicative reach of FlashAttention but also signifies a key shift in efficiency metrics critical for Transformer architectures."", ""Claim30: More importantly, the flexibility of FlashMask extends beyond the proprietary boundaries of FlashAttention, offering potential benefits to a wider range of Transformer-based models."", ""Claim31: By facilitating more efficient computation of the attention mechanism, FlashMask enables innovations in processing vast datasets and complex models, thereby improving performances across varied applications in the LLM field."", ""Claim32: This cross-model adaptability confirms the robustness and utility of FlashMask as a universally applicable enhancement tool within and potentially outside the Transformer architecture spectrum, promising substantial gains in computational efficiency and model scalability."", ""Claim33: FlashMask transcends conventional computational bounds, casting itself not just as a sparse attention methodology, but as a versatile computational paradigm."", ""Claim34: Its adaptability across various custom masking tasks and ability to effectively manage diverse types of mask combinations underscores its potential to greatly enhance the efficiency of attention computations."", ""Claim35: Moreover, the inherent sparsity of the attention mask during inference provides a robust justification for employing FlashMask, indicating its utility and effectiveness in practical applications."", ""Claim36: FlashMask demonstrates impressive performance in handling long-context sequences."", ""Claim37: Exploring synergistic solutions that combine the strengths of both algorithmic innovation (like FlashMask) and distributed system designs stands as a promising venture."", ""Claim38: This approach is anticipated to address scalability challenges and could set the stage for breakthroughs in handling unprecedentedly large data sets and complex model architectures."", ""Claim39: By implementing a novel column-wise sparse representation of attention masks, FlashMask substantially reduces the memory and computational complexity from quadratic to linear with the sequence length, thereby enhancing processing speeds and efficiency."", ""Claim40: Extensive empirical analysis confirms that FlashMask accelerates computational speed significantly, achieving up to 8.3x speedup in common modalities comparable to state-of-the-art methods like FlashAttention."", ""Claim41: This advancement marks a significant leap forward in the design of attention computation, offering the potential for broader applications and setting a new benchmark in the efficiency of processing long-context sequences."", ""Claim42: All end-to-end training and testing in this paper were conducted on 4 servers, each equipped with 32 NVIDIA A800-SXM 80G GPUs. We comprehensively evaluated the performance of the LLaMA2 13 model across three different parameter scales, four downstream task scenarios, and various sequence lengths."", ""Claim43: The primary goal of the experiments is not to achieve optimal end-to-end training performance but to demonstrate the effectiveness of the FlashMask method."", ""Claim44: Results indicate that the sparsity distributions of LongBench dataset and synthetic dataset are similar."", ""Claim45: Kernel experiments are also conducted on batch sizes 4, and 8. FA-Varlen is excluded by default. Results are presented in Figure 8 and 9. The trends are identical to Figure 3 in Section 4.3, except memory exhaustion occurred with less sequence length, especially for FA-DenseMask and Vanilla Attention which require \ud835\udc42 ( \ud835\udc41 2 ) memory to launch."", ""Claim46: We evaluate the effectiveness of FlashMask on the open-source dataset LongBench. The throughput of LoRA fine-tuning for LLaMA2-7B are shown in Figure 10. FlashMask performed close to FA-Varlen, showcasing 4.12x faster than FA-DenseMask, proving that FlashMask can deliver significant training accelerations in generalized real-world scenarios."", ""Claim47: FlashMask showed linear memory consumption with increasing sequence length, far less than FA-DenseMask. Therefore, FlashMask supports training with much longer sequences in memory limits of 80G.""], ""review"": ""Review: Weakness: The paper seems to make claims that are unsubstantiated by experiments. In the abstract and introduction, the paper claims speedup without sacrificing model quality. However, there is no experiment evaluating model quality in the experiments. This is a critical flaw.\n\nFurther, the contribution of the paper is unclear. Block-sparsity is already supported in FlashAttention (see section 3.3 of FlashAttention). It is unclear how this paper is different. There are also more recent works such as \""Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention\"" (NeurIPS 2023), which seem to be strictly more expressive in features than this paper.\nQuestions: See weaknesses\n\n""}, ""review"": ""Title: FlashMask: Reducing the Complexity of Attention Computation through Sparse Mask Representation\nAbstract: Recent advancements in Larger-Scale Transformers have significantly benefited from sophisticated attention mechanisms, which are critical for modeling long-context sequences. However, the computational and memory demands of conventional attention mask computations, typically scaling with an $\\mathcal{O}(N^2)$ complexity where $N$ is the sequence length, pose significant challenges. This paper introduces FlashMask, a simple yet effective \\emph{Exact} attention algorithm designed to substantially reduce both the computational complexity and memory requirements of attention computations. By adopting a novel column-wise sparse representation of attention masks, FlashMask achieves a linear memory complexity of $\\mathcal{O}(N)$ and computational complexity of $\\mathcal{O}(N)\\sim\\mathcal{O}(N^2)$. We assess the performance of FlashMask in a variety of masking scenarios, including causal and customized attention masks, demonstrating its versatility and robustness across a wide range of attention patterns and models. Our empirical analysis encompasses a variety of downstream training modalities, including Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reward Model (RM). We compare FlashMask against state-of-the-art techniques, including notably FlashAttention. In kernel-level assessments, FlashMask achieves substantial computational speedups, up to 6.7x (SFT), 6.9x (DPO), and 8.3x (RM). Furthermore, in end-to-end training, FlashMask consistently enhances training speed significantly, with accelerations up to 2.4x (SFT), 4.2x (LoRA), 2.5x (DPO), and 2.6x (RM) across these varied scenarios without sacrificing model accuracy. Additionally, when implemented in the LoRA scenario, FlashMask enables the LLaMA2-7B to process sequence lengths of up to 544k, significantly enhancing its capability for long-context input.\n\nClaim1: This paper introduces FlashMask, a simple yet effective \\emph{Exact} attention algorithm designed to substantially reduce both the computational complexity and memory requirements of attention computations.\nClaim2: By adopting a novel column-wise sparse representation of attention masks, FlashMask achieves a linear memory complexity of \\(\\mathcal{O}(N)\\) and computational complexity of \\(\\mathcal{O}(N)\\sim\\mathcal{O}(N^2)\\).\nClaim3: We assess the performance of FlashMask in a variety of masking scenarios, including causal and customized attention masks, demonstrating its versatility and robustness across a wide range of attention patterns and models.\nClaim4: Our empirical analysis encompasses a variety of downstream training modalities, including Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reward Model (RM).\nClaim5: In kernel-level assessments, FlashMask achieves substantial computational speedups, up to 6.7x (SFT), 6.9x (DPO), and 8.3x (RM).\nClaim6: Furthermore, in end-to-end training, FlashMask consistently enhances training speed significantly, with accelerations up to 2.4x (SFT), 4.2x (LoRA), 2.5x (DPO), and 2.6x (RM) across these varied scenarios without sacrificing model accuracy.\nClaim7: Additionally, when implemented in the LoRA scenario, FlashMask enables the LLaMA2-7B to process sequence lengths of up to 544k, significantly enhancing its capability for long-context input.\nClaim8: This paper introduces FlashMask, a novel approach utilizing a sparse mask representation to accelerate attention computations in transformers, effectively addressing both computational and memory scala- bility issues.\nClaim9: Conversely, our proposed FlashMask is capable of delivering an exact computations.\nClaim10: It is important to note that unlike various approximate attention algorithms, our method ensures that each effective element of the attention score matrix is computed identically to FlashAttention-2, with masked elements explicitly set to \u2212\u221e, thus maintaining the accuracy of the algorithm\u2019s results.\nClaim11: Futhermore, FlashMask is easily extendable to bidirectional attention computations.\nClaim12: The total space complexity for FlashMask is O( N ), significantly reducing memory usage and supporting training on longer sequences.\nClaim13: Memory access complexity. The dense mask accesses the entire D \u2208 R N \u00d7 N matrix in line 20 of Algorithm 1, totaling N 2 memory accesses on HBM.\nClaim14: Computational complexity. The attention computation process normally iterates over the entire attention score matrix, with a computational complexity of O( N 2 ). By skipping entirely masked blocks, FlashMask leverages block sparsity to reduce computational complexity to O(( 1 \u2212 \u03b1 ) N 2 ).\nClaim15: Experiments were conducted using GPU A800-SXM 80G, Intel(R) Xeon(R) Platinum 8350C CPUs, CUDA 12.0, and driver version 525.125.06.\nClaim16: We evaluated FlashMask against various methods including Vanilla Attention, FlashAttention with dense mask (FA-DenseMask), variable length (FA214Varlen), and sliding window (FA-Window) across different scenarios and sequence lengths.\nClaim17: Both kernel-level and end-to-end performance demonstrated the effectiveness of our method.\nClaim18: For comprehensive testing of FlashMask, we constructed synthetic data to simulate long-sequence training.\nClaim19: FlashMask demonstrated significant latency advantages across all lengths, up to 8.3-fold time saving compared to FA-DenseMask.\nClaim20: Performance benefits from varying sparsity levels were also quantified, with FlashMask showing linear negative correlation with increasing sparsity, demonstrating efficient utilization of sample sparsity for acceleration.\nClaim21: The end-to-end performance 1 of the model was tested using synthetic datasets across three scales of the LLaMA2 model and four downstream scenarios (SFT, LoRA, DPO, RM) at various sequence lengths, measuring throughput in average Tokens/Sec/GPU.\nClaim22: FlashMask consistently showed significant advantages.\nClaim23: Notably, in the LoRA scenario at the LLaMA2-7B, FlashMask achieved a 4.16x throughput improvement over FA-DenseMask, supporting sequence lengths up to 544K.\nClaim24: Additional experiments were conducted on the open-source dataset LongBench [45], comparing the end-to-end performance of FA-DenseMask, FA-Varlen, and FlashMask at sequence lengths of 16K, 32K, and 64K.\nClaim25: The performance improvements were consistent with those observed in the synthetic dataset.\nClaim26: Memory usage during the experiments was also recorded, showing significant reductions for FlashMask compared to FA-DenseMask.\nClaim27: The substantial advancement rendered by FlashMask in improving attention mask computation is a significant evolution over the current FlashAttention framework.\nClaim28: Notably, FlashMask addresses and significantly mitigates the limitations observed with FlashAttention in handling conventional and custom mask computations.\nClaim29: This enhancement not only broadens the applicative reach of FlashAttention but also signifies a key shift in efficiency metrics critical for Transformer architectures.\nClaim30: More importantly, the flexibility of FlashMask extends beyond the proprietary boundaries of FlashAttention, offering potential benefits to a wider range of Transformer-based models.\nClaim31: By facilitating more efficient computation of the attention mechanism, FlashMask enables innovations in processing vast datasets and complex models, thereby improving performances across varied applications in the LLM field.\nClaim32: This cross-model adaptability confirms the robustness and utility of FlashMask as a universally applicable enhancement tool within and potentially outside the Transformer architecture spectrum, promising substantial gains in computational efficiency and model scalability.\nClaim33: FlashMask transcends conventional computational bounds, casting itself not just as a sparse attention methodology, but as a versatile computational paradigm.\nClaim34: Its adaptability across various custom masking tasks and ability to effectively manage diverse types of mask combinations underscores its potential to greatly enhance the efficiency of attention computations.\nClaim35: Moreover, the inherent sparsity of the attention mask during inference provides a robust justification for employing FlashMask, indicating its utility and effectiveness in practical applications.\nClaim36: FlashMask demonstrates impressive performance in handling long-context sequences.\nClaim37: Exploring synergistic solutions that combine the strengths of both algorithmic innovation (like FlashMask) and distributed system designs stands as a promising venture.\nClaim38: This approach is anticipated to address scalability challenges and could set the stage for breakthroughs in handling unprecedentedly large data sets and complex model architectures.\nClaim39: By implementing a novel column-wise sparse representation of attention masks, FlashMask substantially reduces the memory and computational complexity from quadratic to linear with the sequence length, thereby enhancing processing speeds and efficiency.\nClaim40: Extensive empirical analysis confirms that FlashMask accelerates computational speed significantly, achieving up to 8.3x speedup in common modalities comparable to state-of-the-art methods like FlashAttention.\nClaim41: This advancement marks a significant leap forward in the design of attention computation, offering the potential for broader applications and setting a new benchmark in the efficiency of processing long-context sequences.\nClaim42: All end-to-end training and testing in this paper were conducted on 4 servers, each equipped with 32 NVIDIA A800-SXM 80G GPUs. We comprehensively evaluated the performance of the LLaMA2 13 model across three different parameter scales, four downstream task scenarios, and various sequence lengths.\nClaim43: The primary goal of the experiments is not to achieve optimal end-to-end training performance but to demonstrate the effectiveness of the FlashMask method.\nClaim44: Results indicate that the sparsity distributions of LongBench dataset and synthetic dataset are similar.\nClaim45: Kernel experiments are also conducted on batch sizes 4, and 8. FA-Varlen is excluded by default. Results are presented in Figure 8 and 9. The trends are identical to Figure 3 in Section 4.3, except memory exhaustion occurred with less sequence length, especially for FA-DenseMask and Vanilla Attention which require \ud835\udc42 ( \ud835\udc41 2 ) memory to launch.\nClaim46: We evaluate the effectiveness of FlashMask on the open-source dataset LongBench. The throughput of LoRA fine-tuning for LLaMA2-7B are shown in Figure 10. FlashMask performed close to FA-Varlen, showcasing 4.12x faster than FA-DenseMask, proving that FlashMask can deliver significant training accelerations in generalized real-world scenarios.\nClaim47: FlashMask showed linear memory consumption with increasing sequence length, far less than FA-DenseMask. Therefore, FlashMask supports training with much longer sequences in memory limits of 80G.\n\nReview: Weakness: The paper seems to make claims that are unsubstantiated by experiments. In the abstract and introduction, the paper claims speedup without sacrificing model quality. However, there is no experiment evaluating model quality in the experiments. This is a critical flaw.\n\nFurther, the contribution of the paper is unclear. Block-sparsity is already supported in FlashAttention (see section 3.3 of FlashAttention). It is unclear how this paper is different. There are also more recent works such as \""Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention\"" (NeurIPS 2023), which seem to be strictly more expressive in features than this paper.\nQuestions: See weaknesses\n\n\n\n"", ""id"": ""rog0J435OO0"", ""pdf"": ""openreview.net/pdf?id=rog0J435OO""}","34"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""- While the paper claims scalability, there is limited discussion on the computational resources required for generating the synthetic data at scale. Practical constraints related to computational costs and time could be a potential weakness. It was mentioned in the checklist that it is very computationally expensive to repeat experiments."", ""Target claims"": [4, 23, 37], ""Reasoning"": ""The review points out a potential weakness in the paper's claim of scalability. Although scalability is claimed in several places (Claim4, Claim23, and Claim37), the review mentions that there's limited discussion of the computational resources and practical constraints related to this scalability, which could challenge these claims.""}, {""Weakness span"": ""Questions: - Line 269: Why do you say that the reason why errors coming from humanities and social science questions is due to CoT? Could it be due to the lack of knowledge, because it was not trained on that knowledge?"", ""Target claims"": [19, 20], ""Reasoning"": ""The review questions the reasoning behind the claim that errors in humanities and social science questions are due to the default responses with Chain-of-Thoughts (CoT) reasoning. This calls into question Claims 19 and 20, which suggest an analysis of performance discrepancies based on CoT reasoning rather than a lack of training or knowledge in these areas.""}]}, ""meta"": {""id"": ""rcch4UsMBi"", ""review_idx"": 0, ""title"": ""Title: Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models\nAbstract: We introduce Generalized Instruction Tuning (called GLAN), a general and scalable method for instruction tuning of Large Language Models (LLMs). \nUnlike prior work that relies on seed examples or existing datasets to construct instruction tuning data, \nGLAN exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale \nsynthetic instruction data across all disciplines.\nSpecifically, inspired by the systematic structure in human education system, we build the taxonomy by decomposing human knowledge and capabilities to various fields, sub-fields and ultimately, distinct disciplines semi-automatically, facilitated by LLMs. \nSubsequently, we generate a comprehensive list of subjects for every discipline and proceed to design a syllabus tailored to each subject, again utilizing LLMs.\nWith the fine-grained key concepts detailed in every class session of the syllabus, we are able to generate diverse instructions with a broad coverage across the entire spectrum of human knowledge and skills. \nExtensive experiments on large language models (e.g., Mistral) demonstrate that GLAN excels in multiple dimensions from mathematical reasoning, coding, academic exams, logical reasoning to general instruction following without using task-specific training data of these tasks. In addition, GLAN allows for easy customization and new fields or skills can be added by simply incorporating a new node into our taxonomy.\n\nTLDR:\n\nKeywords:Synthetic data; large language models"", ""claims"": [""Claim1: Extensive experiments on large language models (e.g., Mistral) demonstrate that GLAN excels in multiple dimensions from mathematical reasoning, coding, academic exams, logical reasoning to general instruction following without using task-specific training data of these tasks."", ""Claim2: GLAN is general, scalable and customizable."", ""Claim3: GLAN is a general method, which is task-agnostic and is capable of covering a wide range of domains."", ""Claim4: GLAN is scalable."", ""Claim5: Moreover, the input of GLAN is a taxonomy, which is generated by prompting an LLM and human verification, requiring minimal human effort."", ""Claim6: GLAN allows for easy customization."", ""Claim7: Extensive experiments on large language models (e.g., Mistral) demonstrate that GLAN excels in multiple dimensions from mathematical reasoning, coding, academic exams, and logical reasoning to general instruction following without using task-specific training data of these tasks."", ""Claim8: Extensive experiments on large language models (e.g., Mistral) demonstrate that GLAN excels in multiple dimensions from mathematical reasoning, coding, academic exams, logical reasoning to general instruction following without using task-specific training data of these tasks."", ""Claim9: GLAN allows for easy customization and new fields or skills can be added by simply incorporating a new node into our taxonomy."", ""Claim10: The instruction data GLAN generated spans a wide range of subjects."", ""Claim11: Mathematical Reasoning: Mathematics is a common subject in many different disciplines. Hence, it is necessary to test the math reasoning ability of GLAN. We choose the two popular benchmarks for evaluation (i.e., GSM8K and MATH). GSM8K is a high-quality math problem dataset that measures the basic multi-step mathematical reasoning ability. It contains around 7k problems for training and 1K problems for test. MATH is a challenging math dataset that contains mathematics competition-level problems from AMC, AIME, etc. The 7.5k training and 5K test problems cover seven math subjects, i.e., Prealgebra, Precalculus, Algebra, Intermediate Algebra, Number Theory, Counting and Probability, and Geometry. Note that GLAN does not use any examples in the training set of GSM8K or MATH."", ""Claim12: Coding: To evaluate the coding capability of GLAN, we opt for two coding benchmarks HumanEval and MBPP. We employ 0-shot setting for HumanEval and 3-shot setting for MBPP following prior art."", ""Claim13: BBH: The instruction dataset we generated covers many disciplines, which can potentially enhance the reasoning ability of GLAN. Therefore, we evaluate GLAN on the BIG-Bench Hard dataset (BBH), which contains 23 challenging tasks from Big-Bench."", ""Claim14: Academic Exams: We also evaluate GLAN on different academic benchmarks to verify whether GLAN is capable of solving exam questions. We choose two benchmarks (i.e., ARC and MMLU). Both benchmarks are composed of multi-choice questions. AI2 Reasoning Challenge (ARC) contains grade-school level, multi-choice science questions. It contains two sub-sets, which are ARC-Challenge (ARC-C) and ARC-Easy (ARC-E). Massive Multitask Language Understanding (MMLU) consists of a set of multiple-choice questions about 57 subjects ranging in difficulty from elementary levels to professional levels. It covers various of domains of knowledge, including humanities, STEM and social sciences. Note that there is a training set for ARC. However, we have excluded it from our evaluation to verify the generalization capability of our synthetic data."", ""Claim15: We observe that after training on 10 million instructions, GLAN is able to generate its predicted options and analysis of multi-choice questions in plain text as GPT-3.5 does."", ""Claim16: GLAN either obtains the best results or results close to the best across all benchmarks."", ""Claim17: After instruction tuning, GLAN excels on multiple dimensions from mathematical reasoning, coding, reasoning, and academic exams with a systematical data generation approach."", ""Claim18: Also note that our method does not use any task-specific training data such as training sets of GSM8K, MATH, or ARC as in Orca 2, MetaMath, and WizardMath, which indicates the general applicability of GLAN."", ""Claim19: By grouping the 57 subjects in MMLU into four categories (i.e., STEM, Humanities, Social Sciences, and Other (business, health, misc.)), we observe GLAN wildly improves on STEM in MMLU while not in other categories."", ""Claim20: GLAN is good at STEM subjects may be because responses of our dataset are from GPT-3.5-turbo, which by default generates responses with Chain-of-Thoughts (CoT) reasoning."", ""Claim21: We investigate the scaling property of GLAN by training Mistral on different numbers of examples (i.e., 50K, 200K, 500K, 1M, and 10M) we generated."", ""Claim22: It can be observed that overall task performance tends to increase as we increase the data size."", ""Claim23: GLAN is a generalized method to create synthetic data for instruction tuning."", ""Claim24: We compute the training loss L train and test loss L test on ARC Challenge (ARC-C), GSM8K and MATH for GLAN and other models in comparison."", ""Claim25: Table 3 shows the losses of the training and test splits for GLAN are nearly identical (or \u2206 is negative)."", ""Claim26: This suggests that GLAN has not been exposed to in-domain data during training and tuning procedures."", ""Claim27: Additionally, as shown in Table 8, we observe that GLAN obtains higher losses on both test and training splits on GSM8K, MATH, and ARC compared to other models, while performances of GLAN on these datasets are high."", ""Claim28: This might imply that synthetic data generated by GLAN is diverse and our resulting model avoids convergence to any specific domain or style present in existing benchmarks."", ""Claim29: As shown in Table 4, GLAN demonstrates superior instruction-following capabilities in both prompt-level and instruction-level evaluations."", ""Claim30: However, there is still a considerable gap compared to GPT-3.5-turbo and GPT-4."", ""Claim31: GLAN showcases superior performance compared to LLaMA-2, Orca 2, Mistral Instruct, and even WizardLM-13B (note that GLAN contains only 7B parameters) on most difficulty levels and overall scores."", ""Claim32: GLAN demonstrates improved ability to process diverse instructions, regardless of their difficulty or complexity."", ""Claim33: GLAN demonstrates strong performance on most skills, especially in Math, Coding, and Reasoning."", ""Claim34: We also created GLAN-Test, similar to the Evol-Instruct Test but much larger in size, where GLAN outperforms other models as well (see Appendix A.8)."", ""Claim35: Experiments show that GLAN can help large language models improve their capabilities in multiple dimensions, from mathematical reasoning, coding, academic exams, and logical reasoning to general instruction following."", ""Claim36: Data synthesizing is crucial for the continual scaling of large language models, especially as we exhaust available human data."", ""Claim37: GLAN demonstrates the potential to generate vast amounts of synthetic data from scratch, paving the way for even larger-scale data synthesis efforts."", ""Claim38: We believe that the extensive domain coverage of GLAN-Test renders it an effective test bed for the assessment of generalization capabilities in LLMs."", ""Claim39: GLAN obtains higher GPT-4 scores than Orca2-7B, Mistral-7B Instruct and WizardLM-13B, despite using only 7B parameters."", ""Claim40: GLAN demonstrates its effectiveness on multiple domains (or disciplines) such as Mathematics, Physics, Chemistry, Computer science, Electrical, Mechanical, etc., indicating that smaller models may yield general improvements on various domains through strategic fine-tuning."", ""Claim41: GLAN demonstrates less-than-ideal performance across distinct disciplines such as American history, Divinity, or Radiology. This observation underscores the potential for further refinement and development of our methodology within these domains.""], ""review"": ""Review: Weakness: ## Quality\n- While the paper claims scalability, there is limited discussion on the computational resources required for generating the synthetic data at scale. Practical constraints related to computational costs and time could be a potential weakness. It was mentioned in the checklist that it is very computationally expensive to repeat experiments.\nQuestions: - Line 269: Why do you say that the reason why errors coming from humanities and social science questions is due to CoT? Could it be due to the lack of knowledge, because it was not trained on that knowledge?\n- Figure 2: Any reason why for HumanEval and BBH datasets, the scores dropped even though the GLAN data size increase?\n\n""}, ""review"": ""Title: Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models\nAbstract: We introduce Generalized Instruction Tuning (called GLAN), a general and scalable method for instruction tuning of Large Language Models (LLMs). \nUnlike prior work that relies on seed examples or existing datasets to construct instruction tuning data, \nGLAN exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale \nsynthetic instruction data across all disciplines.\nSpecifically, inspired by the systematic structure in human education system, we build the taxonomy by decomposing human knowledge and capabilities to various fields, sub-fields and ultimately, distinct disciplines semi-automatically, facilitated by LLMs. \nSubsequently, we generate a comprehensive list of subjects for every discipline and proceed to design a syllabus tailored to each subject, again utilizing LLMs.\nWith the fine-grained key concepts detailed in every class session of the syllabus, we are able to generate diverse instructions with a broad coverage across the entire spectrum of human knowledge and skills. \nExtensive experiments on large language models (e.g., Mistral) demonstrate that GLAN excels in multiple dimensions from mathematical reasoning, coding, academic exams, logical reasoning to general instruction following without using task-specific training data of these tasks. In addition, GLAN allows for easy customization and new fields or skills can be added by simply incorporating a new node into our taxonomy.\n\nClaim1: Extensive experiments on large language models (e.g., Mistral) demonstrate that GLAN excels in multiple dimensions from mathematical reasoning, coding, academic exams, logical reasoning to general instruction following without using task-specific training data of these tasks.\nClaim2: GLAN is general, scalable and customizable.\nClaim3: GLAN is a general method, which is task-agnostic and is capable of covering a wide range of domains.\nClaim4: GLAN is scalable.\nClaim5: Moreover, the input of GLAN is a taxonomy, which is generated by prompting an LLM and human verification, requiring minimal human effort.\nClaim6: GLAN allows for easy customization.\nClaim7: Extensive experiments on large language models (e.g., Mistral) demonstrate that GLAN excels in multiple dimensions from mathematical reasoning, coding, academic exams, and logical reasoning to general instruction following without using task-specific training data of these tasks.\nClaim8: Extensive experiments on large language models (e.g., Mistral) demonstrate that GLAN excels in multiple dimensions from mathematical reasoning, coding, academic exams, logical reasoning to general instruction following without using task-specific training data of these tasks.\nClaim9: GLAN allows for easy customization and new fields or skills can be added by simply incorporating a new node into our taxonomy.\nClaim10: The instruction data GLAN generated spans a wide range of subjects.\nClaim11: Mathematical Reasoning: Mathematics is a common subject in many different disciplines. Hence, it is necessary to test the math reasoning ability of GLAN. We choose the two popular benchmarks for evaluation (i.e., GSM8K and MATH). GSM8K is a high-quality math problem dataset that measures the basic multi-step mathematical reasoning ability. It contains around 7k problems for training and 1K problems for test. MATH is a challenging math dataset that contains mathematics competition-level problems from AMC, AIME, etc. The 7.5k training and 5K test problems cover seven math subjects, i.e., Prealgebra, Precalculus, Algebra, Intermediate Algebra, Number Theory, Counting and Probability, and Geometry. Note that GLAN does not use any examples in the training set of GSM8K or MATH.\nClaim12: Coding: To evaluate the coding capability of GLAN, we opt for two coding benchmarks HumanEval and MBPP. We employ 0-shot setting for HumanEval and 3-shot setting for MBPP following prior art.\nClaim13: BBH: The instruction dataset we generated covers many disciplines, which can potentially enhance the reasoning ability of GLAN. Therefore, we evaluate GLAN on the BIG-Bench Hard dataset (BBH), which contains 23 challenging tasks from Big-Bench.\nClaim14: Academic Exams: We also evaluate GLAN on different academic benchmarks to verify whether GLAN is capable of solving exam questions. We choose two benchmarks (i.e., ARC and MMLU). Both benchmarks are composed of multi-choice questions. AI2 Reasoning Challenge (ARC) contains grade-school level, multi-choice science questions. It contains two sub-sets, which are ARC-Challenge (ARC-C) and ARC-Easy (ARC-E). Massive Multitask Language Understanding (MMLU) consists of a set of multiple-choice questions about 57 subjects ranging in difficulty from elementary levels to professional levels. It covers various of domains of knowledge, including humanities, STEM and social sciences. Note that there is a training set for ARC. However, we have excluded it from our evaluation to verify the generalization capability of our synthetic data.\nClaim15: We observe that after training on 10 million instructions, GLAN is able to generate its predicted options and analysis of multi-choice questions in plain text as GPT-3.5 does.\nClaim16: GLAN either obtains the best results or results close to the best across all benchmarks.\nClaim17: After instruction tuning, GLAN excels on multiple dimensions from mathematical reasoning, coding, reasoning, and academic exams with a systematical data generation approach.\nClaim18: Also note that our method does not use any task-specific training data such as training sets of GSM8K, MATH, or ARC as in Orca 2, MetaMath, and WizardMath, which indicates the general applicability of GLAN.\nClaim19: By grouping the 57 subjects in MMLU into four categories (i.e., STEM, Humanities, Social Sciences, and Other (business, health, misc.)), we observe GLAN wildly improves on STEM in MMLU while not in other categories.\nClaim20: GLAN is good at STEM subjects may be because responses of our dataset are from GPT-3.5-turbo, which by default generates responses with Chain-of-Thoughts (CoT) reasoning.\nClaim21: We investigate the scaling property of GLAN by training Mistral on different numbers of examples (i.e., 50K, 200K, 500K, 1M, and 10M) we generated.\nClaim22: It can be observed that overall task performance tends to increase as we increase the data size.\nClaim23: GLAN is a generalized method to create synthetic data for instruction tuning.\nClaim24: We compute the training loss L train and test loss L test on ARC Challenge (ARC-C), GSM8K and MATH for GLAN and other models in comparison.\nClaim25: Table 3 shows the losses of the training and test splits for GLAN are nearly identical (or \u2206 is negative).\nClaim26: This suggests that GLAN has not been exposed to in-domain data during training and tuning procedures.\nClaim27: Additionally, as shown in Table 8, we observe that GLAN obtains higher losses on both test and training splits on GSM8K, MATH, and ARC compared to other models, while performances of GLAN on these datasets are high.\nClaim28: This might imply that synthetic data generated by GLAN is diverse and our resulting model avoids convergence to any specific domain or style present in existing benchmarks.\nClaim29: As shown in Table 4, GLAN demonstrates superior instruction-following capabilities in both prompt-level and instruction-level evaluations.\nClaim30: However, there is still a considerable gap compared to GPT-3.5-turbo and GPT-4.\nClaim31: GLAN showcases superior performance compared to LLaMA-2, Orca 2, Mistral Instruct, and even WizardLM-13B (note that GLAN contains only 7B parameters) on most difficulty levels and overall scores.\nClaim32: GLAN demonstrates improved ability to process diverse instructions, regardless of their difficulty or complexity.\nClaim33: GLAN demonstrates strong performance on most skills, especially in Math, Coding, and Reasoning.\nClaim34: We also created GLAN-Test, similar to the Evol-Instruct Test but much larger in size, where GLAN outperforms other models as well (see Appendix A.8).\nClaim35: Experiments show that GLAN can help large language models improve their capabilities in multiple dimensions, from mathematical reasoning, coding, academic exams, and logical reasoning to general instruction following.\nClaim36: Data synthesizing is crucial for the continual scaling of large language models, especially as we exhaust available human data.\nClaim37: GLAN demonstrates the potential to generate vast amounts of synthetic data from scratch, paving the way for even larger-scale data synthesis efforts.\nClaim38: We believe that the extensive domain coverage of GLAN-Test renders it an effective test bed for the assessment of generalization capabilities in LLMs.\nClaim39: GLAN obtains higher GPT-4 scores than Orca2-7B, Mistral-7B Instruct and WizardLM-13B, despite using only 7B parameters.\nClaim40: GLAN demonstrates its effectiveness on multiple domains (or disciplines) such as Mathematics, Physics, Chemistry, Computer science, Electrical, Mechanical, etc., indicating that smaller models may yield general improvements on various domains through strategic fine-tuning.\nClaim41: GLAN demonstrates less-than-ideal performance across distinct disciplines such as American history, Divinity, or Radiology. This observation underscores the potential for further refinement and development of our methodology within these domains.\n\nReview: Weakness: ## Quality\n- While the paper claims scalability, there is limited discussion on the computational resources required for generating the synthetic data at scale. Practical constraints related to computational costs and time could be a potential weakness. It was mentioned in the checklist that it is very computationally expensive to repeat experiments.\nQuestions: - Line 269: Why do you say that the reason why errors coming from humanities and social science questions is due to CoT? Could it be due to the lack of knowledge, because it was not trained on that knowledge?\n- Figure 2: Any reason why for HumanEval and BBH datasets, the scores dropped even though the GLAN data size increase?\n\n\n\n"", ""id"": ""rcch4UsMBi0"", ""pdf"": ""openreview.net/pdf?id=rcch4UsMBi""}","35"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""There are some issues regarding the comparisons to existing models, making it unclear how much of the observed improvement is really due to the novel method proposed. LayTexLLM is implemented with Llama-2-7b, but it seems that many models compared to (e.g. the strong-performing LayoutLLM) may use other LLM backbones, making it unclear whether the superior performance of LayTexLLM in many settings is due to the proposed novel method or the LLM backbone. The results will be more convincing with a comparison of different methods with the same LLM backbone (or at least an analysis of the number of parameters in each model)."", ""Target claims"": [2, 20, 21, 23], ""Reasoning"": ""This span questions the validity of the performance improvements claimed in the paper by pointing out that the comparisons to existing models might not be fair due to different LLM backbones used, leaving the reason for the improvement (novel method or backbone) ambiguous. This directly targets Claim2, which speaks about performance improvements over SOTA methods, Claim20, which mentions significant improvements over SOTA OCR-based methods, Claim21, which discusses superior performance compared to DocLLM, and Claim23, which shows discrepant performance against LayoutLLM.""}, {""Weakness span"": ""It is not clear what OCR engine is used, raising the concern that different OCR engines could explain some of the gaps in performance between models being compared."", ""Target claims"": [2, 18, 20], ""Reasoning"": ""This weakness highlights the uncertainty surrounding the performance comparisons due to unreported differences in OCR engines, bringing into question the legitimacy of the performance claims (such as those in Claim2 about improvements in KIE and VQA tasks, Claim18 about surpassing SOTA OCR-free methods, and Claim20 about outperforming OCR-based methods).""}, {""Weakness span"": ""The statements about large improvements over SOTA MLMMs (L13-14, L83-84) seem slightly misleading since LayTextLLM uses OCR detections and thus is more comparable to other OCR-based methods."", ""Target claims"": [2, 24], ""Reasoning"": ""This span criticizes claims of improvement by pointing out potentially misleading comparisons, questioning the legitimacy of reported performance enhancements over SOTA methods. It directly targets Claim2 regarding enhancements in KIE and VQA tasks over SOTA document understanding MLLMs, and Claim24 about outperforming LayoutLLM in zero-shot scenarios.""}, {""Weakness span"": ""LNTP (Sec. 3.2.1) is presented as a novelty but seems to just be the regular language modeling objective."", ""Target claims"": [9], ""Reasoning"": ""This comment questions the novelty of the LNTP, which is one of the paper's contributions as per Claim9. The reviewer suggests that LNTP may not be a novel concept but rather a reiteration of existing objectives, putting into question the claim of it being a new methodology.""}]}, ""meta"": {""id"": ""rSSpEmrN0C"", ""review_idx"": 0, ""title"": ""Title: A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding\nAbstract: Recently, many studies have demonstrated that exclusively incorporating OCR-derived text and spatial layouts with large language models (LLMs) can be highly effective for document understanding tasks. However, existing methods that integrate spatial layouts with text have limitations, such as producing overly long text sequences or failing to fully leverage the autoregressive traits of LLMs. In this work, we introduce nterleaving Layout and Text in a Large Language Model (LayTextLLM)} for document understanding. In particular, LayTextLLM projects each bounding box to a single embedding and interleaves it with text, efficiently avoiding long sequence issues while leveraging autoregressive traits of LLMs. LayTextLLM not only streamlines the interaction of layout and textual data but also shows enhanced performance in Key Information Extraction (KIE) and Visual Question Answering (VQA). Comprehensive benchmark evaluations reveal significant improvements, with a 27.0\\% increase on KIE tasks and 24.1\\% on VQA tasks compared to previous state-of-the-art document understanding MLLMs, as well as a 15.5\\% improvement over other SOTA OCR-based LLMs on KIE tasks.\n\nTLDR:\n\nKeywords:LLM, DocAI, Visually Rich Document Understanding, KIE"", ""claims"": [""Claim1: LayTextLLM not only streamlines the interaction of layout and textual data but also shows enhanced performance in Key Information Extraction (KIE) and Visual Question Answering (VQA)."", ""Claim2: Comprehensive benchmark evaluations reveal significant improvements, with a 27.0% increase on KIE tasks and 24.1% on VQA tasks compared to previous state-of-the-art document understanding MLLMs, as well as a 15.5% improvement over other SOTA OCR-based LLMs on KIE tasks."", ""Claim3: Finally, through the proposed Shuffled-OCR Supervised Fine-tuning, LayTextLLM significantly improves performance on downstream document-related VQA and KIE tasks."", ""Claim4: LayTextLLM significantly outperforms the 175B models, while only slightly increasing or even reducing the sequence length compared to DocLLM."", ""Claim5: This enhancement enables the model to process both spatial layouts and textual inputs simultaneously."", ""Claim6: Compared to the coordinate-as-tokens scheme, the SLP represents each bounding box with a single token."", ""Claim7: This approach significantly reduces the number of input tokens and adheres to the practice of interleaving any modality with text, effectively integrating layout and textual information into a unified sequence."", ""Claim8: This allows the model to process both modalities simultaneously and coherently, fully leveraging the autoregressive traits of LLMs."", ""Claim9: It is important to note that the proposed Layout-aware Next Token Prediction is a completely self-supervised pre-training procedure, unlike previous works that require human annotations of document structure data or synthetic data generated by larger LLMs such as GPT-4."", ""Claim10: Thus, LNTP facilitates the creation of large-scale, high-fidelity pre-training datasets at minimal cost."", ""Claim11: Experimental results in Section 4.6 demonstrate that utilizing SSFT can further enhance model performance, making it more robust to disruptions in input token order."", ""Claim12: The experimental results shown in Tab. 1 demonstrate the outstanding performance of the LayTextLLM series across various tasks."", ""Claim13: LayTextLLM zero significantly outperforms previous SOTA OCR-free methods, such as TextMonkey [12], in zero-shot capabilities, even when these methods use the training set of the dataset."", ""Claim14: For example, in the DocVQA and InfoVQA datasets, LayTextLLM zero achieves accuracies of 71.8% and 33.8%, respectively, which are markedly higher than existing OCR-free methods such as TextMonkey and InternLM-XComposer2."", ""Claim15: When fine-tuned with corresponding datasets, LayTextLLM shows even greater performance improvements, particularly in document-oriented VQA datasets."", ""Claim16: Specifically, its accuracies on DocVQA and InfoVQA increase to 77.4% and 66.1%, respectively, demonstrating the model\u2019s strong ability to leverage task-specific data."", ""Claim17: LayTextLLM zero excels in KIE datasets, particularly on the SROIE and POIE datasets, achieving accuracies of 86.7% and 66.1%, respectively."", ""Claim18: These results significantly surpass those of previous SOTA OCR-free model ( i.e. , TextMonkey + ) by margins of 40.5% and 34.1%, respectively."", ""Claim19: This significant performance gain is likely due to these datasets containing low-resolution images that are too blurred for current MLLMs to extract visual features, whereas LayTextLLM shows robustness in such challenging scenarios."", ""Claim20: The experimental results presented in Tab. 2 showcase significant performance improvements achieved by LayTextLLM models compared to pure OCR-based SOTA methods such as DocLLM [19]."", ""Claim21: Specifically, when comparing with DocLLM, LayTextLLM zero demonstrates notably superior performance, with even its zero-shot capabilities being competitive with supervised SFT approaches."", ""Claim22: Similarly, when contrasting with coordinate-as-tokens employed in Llama2-7B, LayTextLLM zero again outperforms significantly."", ""Claim23: In comparison to LayoutLLM [21], our approach exhibits discrepant performance in different tasks, as shown in Tab. 3."", ""Claim24: In zero-shot scenarios, we outperform LayoutLLM in most KIE datasets, validating our capability to leverage OCR-based results effectively."", ""Claim25: However, we fall short on document-oriented VQA tasks since answering some questions that are strongly related to vision information may challenge our approach."", ""Claim26: However, when fine-tuned with tailored data, LayTextLLM significantly outperforms LayoutLLM, showcasing its strong ability to utilize task-specific data."", ""Claim27: It is evident that both LNTP and SSFT significantly enhance the utility of LayTextLLM."", ""Claim28: Specifically, disabling LNTP results in an 8% decrease in performance on VQA tasks and a 5.5% decrease on KIE tasks."", ""Claim29: Disabling SSFT leads to a decrease in average accuracy by 6.7% and 2.6% for VQA and KIE tasks, respectively."", ""Claim30: Although LayTextLLM has shown significant capabilities in text-rich VQA and KIE tasks, this alone does not suffice for all real-world applications."", ""Claim31: Extensive experiments confirm the effectiveness of LayTextLLM."", ""Claim32: The results indicate that LayTextLLM is highly effective in utilizing spatial layout information to make more accurate predictions for these challenging examples."", ""Claim33: In contrast, LayTextLLM integrates layout information to accurately predict the total price, as demonstrated by the other examples, underscoring the utility of LayTextLLM."", ""Claim34: LayTextLLM consistently outperforms Llama2-7B-base across all levels of shuffling, which further underscores the significance of interleaving spatial layouts with text."", ""Claim35: Particularly at the 100% shuffle level, Llama2-7B-base demonstrates limited accuracy at only 20.3, while LayTextLLM maintains a relatively higher performance."", ""Claim36: It is also interesting to note that Llama2-7B-base generally improves as the shuffling percentage decreases, whereas LayTextLLM performs best when 20% of the examples with OCR-derived text are shuffled."", ""Claim37: This observation suggests that LayTextLLM effectively utilizes spatial layouts and is less dependent on the sequence of input tokens."", ""Claim38: Therefore, a certain proportion of shuffled examples can serve as adversarial examples to enhance the model's robustness, addressing situations such as errors in the text order from the OCR engine, which are caused by subtle differences in horizontal or vertical coordinates."", ""Claim39: However, with only text and layout information as input, the proposed LayTextLLM inevitably have difficulties in reasoning visual-related information."", ""Claim40: Thus, on the ChartQA dataset, LayTextLLM can hardly achieve better performance than previous methods that include visual inputs."", ""Claim41: Although the visual information is not used in LayTextLLM, it can still exhibit better zero-shot ability than UniDoc."", ""Claim42: After incorporating the training set of ChartQA, the performance of LayTextLLM can be boosted to 35.7%.""], ""review"": ""Review: Weakness: There are some issues regarding the comparisons to existing models, making it unclear how much of the observed improvement is really due to the novel method proposed.\n\nLayTexLLM is implemented with Llama-2-7b, but it seems that many models compared to (e.g. the strong-performing LayoutLLM) may use other LLM backbones, making it unclear whether the superior performance of LayTexLLM in many settings is due to the proposed novel method or the LLM backbone. The results will be more convincing with a comparison of different methods with the same LLM backbone (or at least an analysis of the number of parameters in each model).\n\nIt is not clear what OCR engine is used, raising the concern that different OCR engines could explain some of the gaps in performance between models being compared.\n\nThere are also issues with how the training is presented that make it difficult to interpret results. Some places (L131, L179, etc.) mention pre-training and SSFT, implying that pre-training means the LNTP training task. However, Sec 4.1 mentions \u201cpre-training\u201d and \u201cSFT\u201d, implying that pre-training refers to SSFT+LNTP and that it is followed by SFT (Supervised Fine Tuning) for particular tasks (VQA and KIE). The results also mention zero-shot and supervised results (e.g. L297), but it is unclear from the text and results tables which results are obtained zero-shot or from SFT, making it hard to understand if the comparisons are fair.\n\nThe statements about large improvements over SOTA MLMMs (L13-14, L83-84) seem slightly misleading since LayTextLLM uses OCR detections and thus is more comparable to other OCR-based methods.\n\nLNTP (Sec. 3.2.1) is presented as a novelty but seems to just be the regular language modeling objective. If I understand correctly, this could be toned down to simply say that the added SLP and P-LoRA parameters are updated with a language modeling loss.\nQuestions: I don\u2019t fully follow the claim of L54-55 about autoregressive models vs. DocLLM-style models. Why would autoregressive modeling a priori be expected to outperform spatial cross attention for document understanding?\n\nWhile the justification of SSFT (Sec 3.2.2) makes sense, it seems that the issue stems from the use of positional encodings which encode left-to-right order of tokens. Have you considered using positional encodings that directly encode (x, y)-positions of text to avoid this artifact or to give the model an inductive bias towards the layout\u2019s 2D positioning?\n\nWhy was only Llama-2-7b used? Would the proposed method work for other LLM backbones?\n\nWhat is the motivation for using P-LoRA (Sec. 3.1.2)? Is it applied to every layer?\n\nL195 states that LNTP loss is calculated for text tokens only. Does this mean that bounding box tokens are still used as inputs but just not as targets? Why is this done, and is it tested?\n\nThere are a number of minor grammatical errors throughout the paper that need revision, including missing articles (e.g. L163 \u201cto (the) visual modality\u201d, among others) and some awkward wording (e.g. L142 \u201cspecific\u201d, L160 \u201ccommunicated\u201d, L211 \u201ccons and pros\u201d => \u201cadvantages and disadvantages\u201d, L262 \u201cit\u2019s\u201d => \u201cit is\u201d, L332 \u201c(and we provide further) discussion\u201d, among others). The acronym SFT used throughout should be defined somewhere. \n\nTables 1-3: The term \u201cpolys\u201d and the exact meaning of the asterisk * are unclear.\n\n""}, ""review"": ""Title: A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding\nAbstract: Recently, many studies have demonstrated that exclusively incorporating OCR-derived text and spatial layouts with large language models (LLMs) can be highly effective for document understanding tasks. However, existing methods that integrate spatial layouts with text have limitations, such as producing overly long text sequences or failing to fully leverage the autoregressive traits of LLMs. In this work, we introduce nterleaving Layout and Text in a Large Language Model (LayTextLLM)} for document understanding. In particular, LayTextLLM projects each bounding box to a single embedding and interleaves it with text, efficiently avoiding long sequence issues while leveraging autoregressive traits of LLMs. LayTextLLM not only streamlines the interaction of layout and textual data but also shows enhanced performance in Key Information Extraction (KIE) and Visual Question Answering (VQA). Comprehensive benchmark evaluations reveal significant improvements, with a 27.0\\% increase on KIE tasks and 24.1\\% on VQA tasks compared to previous state-of-the-art document understanding MLLMs, as well as a 15.5\\% improvement over other SOTA OCR-based LLMs on KIE tasks.\n\nClaim1: LayTextLLM not only streamlines the interaction of layout and textual data but also shows enhanced performance in Key Information Extraction (KIE) and Visual Question Answering (VQA).\nClaim2: Comprehensive benchmark evaluations reveal significant improvements, with a 27.0% increase on KIE tasks and 24.1% on VQA tasks compared to previous state-of-the-art document understanding MLLMs, as well as a 15.5% improvement over other SOTA OCR-based LLMs on KIE tasks.\nClaim3: Finally, through the proposed Shuffled-OCR Supervised Fine-tuning, LayTextLLM significantly improves performance on downstream document-related VQA and KIE tasks.\nClaim4: LayTextLLM significantly outperforms the 175B models, while only slightly increasing or even reducing the sequence length compared to DocLLM.\nClaim5: This enhancement enables the model to process both spatial layouts and textual inputs simultaneously.\nClaim6: Compared to the coordinate-as-tokens scheme, the SLP represents each bounding box with a single token.\nClaim7: This approach significantly reduces the number of input tokens and adheres to the practice of interleaving any modality with text, effectively integrating layout and textual information into a unified sequence.\nClaim8: This allows the model to process both modalities simultaneously and coherently, fully leveraging the autoregressive traits of LLMs.\nClaim9: It is important to note that the proposed Layout-aware Next Token Prediction is a completely self-supervised pre-training procedure, unlike previous works that require human annotations of document structure data or synthetic data generated by larger LLMs such as GPT-4.\nClaim10: Thus, LNTP facilitates the creation of large-scale, high-fidelity pre-training datasets at minimal cost.\nClaim11: Experimental results in Section 4.6 demonstrate that utilizing SSFT can further enhance model performance, making it more robust to disruptions in input token order.\nClaim12: The experimental results shown in Tab. 1 demonstrate the outstanding performance of the LayTextLLM series across various tasks.\nClaim13: LayTextLLM zero significantly outperforms previous SOTA OCR-free methods, such as TextMonkey [12], in zero-shot capabilities, even when these methods use the training set of the dataset.\nClaim14: For example, in the DocVQA and InfoVQA datasets, LayTextLLM zero achieves accuracies of 71.8% and 33.8%, respectively, which are markedly higher than existing OCR-free methods such as TextMonkey and InternLM-XComposer2.\nClaim15: When fine-tuned with corresponding datasets, LayTextLLM shows even greater performance improvements, particularly in document-oriented VQA datasets.\nClaim16: Specifically, its accuracies on DocVQA and InfoVQA increase to 77.4% and 66.1%, respectively, demonstrating the model\u2019s strong ability to leverage task-specific data.\nClaim17: LayTextLLM zero excels in KIE datasets, particularly on the SROIE and POIE datasets, achieving accuracies of 86.7% and 66.1%, respectively.\nClaim18: These results significantly surpass those of previous SOTA OCR-free model ( i.e. , TextMonkey + ) by margins of 40.5% and 34.1%, respectively.\nClaim19: This significant performance gain is likely due to these datasets containing low-resolution images that are too blurred for current MLLMs to extract visual features, whereas LayTextLLM shows robustness in such challenging scenarios.\nClaim20: The experimental results presented in Tab. 2 showcase significant performance improvements achieved by LayTextLLM models compared to pure OCR-based SOTA methods such as DocLLM [19].\nClaim21: Specifically, when comparing with DocLLM, LayTextLLM zero demonstrates notably superior performance, with even its zero-shot capabilities being competitive with supervised SFT approaches.\nClaim22: Similarly, when contrasting with coordinate-as-tokens employed in Llama2-7B, LayTextLLM zero again outperforms significantly.\nClaim23: In comparison to LayoutLLM [21], our approach exhibits discrepant performance in different tasks, as shown in Tab. 3.\nClaim24: In zero-shot scenarios, we outperform LayoutLLM in most KIE datasets, validating our capability to leverage OCR-based results effectively.\nClaim25: However, we fall short on document-oriented VQA tasks since answering some questions that are strongly related to vision information may challenge our approach.\nClaim26: However, when fine-tuned with tailored data, LayTextLLM significantly outperforms LayoutLLM, showcasing its strong ability to utilize task-specific data.\nClaim27: It is evident that both LNTP and SSFT significantly enhance the utility of LayTextLLM.\nClaim28: Specifically, disabling LNTP results in an 8% decrease in performance on VQA tasks and a 5.5% decrease on KIE tasks.\nClaim29: Disabling SSFT leads to a decrease in average accuracy by 6.7% and 2.6% for VQA and KIE tasks, respectively.\nClaim30: Although LayTextLLM has shown significant capabilities in text-rich VQA and KIE tasks, this alone does not suffice for all real-world applications.\nClaim31: Extensive experiments confirm the effectiveness of LayTextLLM.\nClaim32: The results indicate that LayTextLLM is highly effective in utilizing spatial layout information to make more accurate predictions for these challenging examples.\nClaim33: In contrast, LayTextLLM integrates layout information to accurately predict the total price, as demonstrated by the other examples, underscoring the utility of LayTextLLM.\nClaim34: LayTextLLM consistently outperforms Llama2-7B-base across all levels of shuffling, which further underscores the significance of interleaving spatial layouts with text.\nClaim35: Particularly at the 100% shuffle level, Llama2-7B-base demonstrates limited accuracy at only 20.3, while LayTextLLM maintains a relatively higher performance.\nClaim36: It is also interesting to note that Llama2-7B-base generally improves as the shuffling percentage decreases, whereas LayTextLLM performs best when 20% of the examples with OCR-derived text are shuffled.\nClaim37: This observation suggests that LayTextLLM effectively utilizes spatial layouts and is less dependent on the sequence of input tokens.\nClaim38: Therefore, a certain proportion of shuffled examples can serve as adversarial examples to enhance the model's robustness, addressing situations such as errors in the text order from the OCR engine, which are caused by subtle differences in horizontal or vertical coordinates.\nClaim39: However, with only text and layout information as input, the proposed LayTextLLM inevitably have difficulties in reasoning visual-related information.\nClaim40: Thus, on the ChartQA dataset, LayTextLLM can hardly achieve better performance than previous methods that include visual inputs.\nClaim41: Although the visual information is not used in LayTextLLM, it can still exhibit better zero-shot ability than UniDoc.\nClaim42: After incorporating the training set of ChartQA, the performance of LayTextLLM can be boosted to 35.7%.\n\nReview: Weakness: There are some issues regarding the comparisons to existing models, making it unclear how much of the observed improvement is really due to the novel method proposed.\n\nLayTexLLM is implemented with Llama-2-7b, but it seems that many models compared to (e.g. the strong-performing LayoutLLM) may use other LLM backbones, making it unclear whether the superior performance of LayTexLLM in many settings is due to the proposed novel method or the LLM backbone. The results will be more convincing with a comparison of different methods with the same LLM backbone (or at least an analysis of the number of parameters in each model).\n\nIt is not clear what OCR engine is used, raising the concern that different OCR engines could explain some of the gaps in performance between models being compared.\n\nThere are also issues with how the training is presented that make it difficult to interpret results. Some places (L131, L179, etc.) mention pre-training and SSFT, implying that pre-training means the LNTP training task. However, Sec 4.1 mentions \u201cpre-training\u201d and \u201cSFT\u201d, implying that pre-training refers to SSFT+LNTP and that it is followed by SFT (Supervised Fine Tuning) for particular tasks (VQA and KIE). The results also mention zero-shot and supervised results (e.g. L297), but it is unclear from the text and results tables which results are obtained zero-shot or from SFT, making it hard to understand if the comparisons are fair.\n\nThe statements about large improvements over SOTA MLMMs (L13-14, L83-84) seem slightly misleading since LayTextLLM uses OCR detections and thus is more comparable to other OCR-based methods.\n\nLNTP (Sec. 3.2.1) is presented as a novelty but seems to just be the regular language modeling objective. If I understand correctly, this could be toned down to simply say that the added SLP and P-LoRA parameters are updated with a language modeling loss.\nQuestions: I don\u2019t fully follow the claim of L54-55 about autoregressive models vs. DocLLM-style models. Why would autoregressive modeling a priori be expected to outperform spatial cross attention for document understanding?\n\nWhile the justification of SSFT (Sec 3.2.2) makes sense, it seems that the issue stems from the use of positional encodings which encode left-to-right order of tokens. Have you considered using positional encodings that directly encode (x, y)-positions of text to avoid this artifact or to give the model an inductive bias towards the layout\u2019s 2D positioning?\n\nWhy was only Llama-2-7b used? Would the proposed method work for other LLM backbones?\n\nWhat is the motivation for using P-LoRA (Sec. 3.1.2)? Is it applied to every layer?\n\nL195 states that LNTP loss is calculated for text tokens only. Does this mean that bounding box tokens are still used as inputs but just not as targets? Why is this done, and is it tested?\n\nThere are a number of minor grammatical errors throughout the paper that need revision, including missing articles (e.g. L163 \u201cto (the) visual modality\u201d, among others) and some awkward wording (e.g. L142 \u201cspecific\u201d, L160 \u201ccommunicated\u201d, L211 \u201ccons and pros\u201d => \u201cadvantages and disadvantages\u201d, L262 \u201cit\u2019s\u201d => \u201cit is\u201d, L332 \u201c(and we provide further) discussion\u201d, among others). The acronym SFT used throughout should be defined somewhere. \n\nTables 1-3: The term \u201cpolys\u201d and the exact meaning of the asterisk * are unclear.\n\n\n\n"", ""id"": ""rSSpEmrN0C0"", ""pdf"": ""openreview.net/pdf?id=rSSpEmrN0C""}","36"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""1. Due to miss the crucial visual information necessary for document understanding, this LayoutTextLLM heavily relies on OCR-derived text and spatial layouts. Other works such as LayoutLLM, layoutLMv3, introduces visual information to enhance the document understanding performance."", ""Target claims"": [19, 39, 40], ""Reasoning"": ""The review critiques LayTextLLM for lacking crucial visual information in document understanding, which relates to Claim19 regarding robustness in challenging scenarios, Claim39 about difficulties in reasoning visual-related information, and Claim40 where LayTextLLM does not achieve better performance due to missing visual inputs. The span points out a limitation related to claims about how LayTextLLM handles document understanding tasks differently than methods incorporating visual information.""}, {""Weakness span"": ""4\u3001Although LayTextLLM shows higher performance on DocVQA compared to LayoutLLM, this comparison is not entirely fair as LayoutLLM was evaluated in a zero-shot setting. Moreover, the zero-shot performance of LayoutLMv3 on DocVQA surpasses that of LayTextLLM."", ""Target claims"": [24, 25], ""Reasoning"": ""This weakness questions the validity of the comparison made in the claims about competing models\u2019 performance on DocVQA tasks. Claim24 mentions outperforming LayoutLLM in zero-shot scenarios, and Claim25 notes a shortfall on document-oriented VQA tasks. Both claims are directly related as they involve comparisons that the review argues may be unfair or inaccurately presented.""}]}, ""meta"": {""id"": ""rSSpEmrN0C"", ""review_idx"": 1, ""title"": ""Title: A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding\nAbstract: Recently, many studies have demonstrated that exclusively incorporating OCR-derived text and spatial layouts with large language models (LLMs) can be highly effective for document understanding tasks. However, existing methods that integrate spatial layouts with text have limitations, such as producing overly long text sequences or failing to fully leverage the autoregressive traits of LLMs. In this work, we introduce nterleaving Layout and Text in a Large Language Model (LayTextLLM)} for document understanding. In particular, LayTextLLM projects each bounding box to a single embedding and interleaves it with text, efficiently avoiding long sequence issues while leveraging autoregressive traits of LLMs. LayTextLLM not only streamlines the interaction of layout and textual data but also shows enhanced performance in Key Information Extraction (KIE) and Visual Question Answering (VQA). Comprehensive benchmark evaluations reveal significant improvements, with a 27.0\\% increase on KIE tasks and 24.1\\% on VQA tasks compared to previous state-of-the-art document understanding MLLMs, as well as a 15.5\\% improvement over other SOTA OCR-based LLMs on KIE tasks.\n\nTLDR:\n\nKeywords:LLM, DocAI, Visually Rich Document Understanding, KIE"", ""claims"": [""Claim1: LayTextLLM not only streamlines the interaction of layout and textual data but also shows enhanced performance in Key Information Extraction (KIE) and Visual Question Answering (VQA)."", ""Claim2: Comprehensive benchmark evaluations reveal significant improvements, with a 27.0% increase on KIE tasks and 24.1% on VQA tasks compared to previous state-of-the-art document understanding MLLMs, as well as a 15.5% improvement over other SOTA OCR-based LLMs on KIE tasks."", ""Claim3: Finally, through the proposed Shuffled-OCR Supervised Fine-tuning, LayTextLLM significantly improves performance on downstream document-related VQA and KIE tasks."", ""Claim4: LayTextLLM significantly outperforms the 175B models, while only slightly increasing or even reducing the sequence length compared to DocLLM."", ""Claim5: This enhancement enables the model to process both spatial layouts and textual inputs simultaneously."", ""Claim6: Compared to the coordinate-as-tokens scheme, the SLP represents each bounding box with a single token."", ""Claim7: This approach significantly reduces the number of input tokens and adheres to the practice of interleaving any modality with text, effectively integrating layout and textual information into a unified sequence."", ""Claim8: This allows the model to process both modalities simultaneously and coherently, fully leveraging the autoregressive traits of LLMs."", ""Claim9: It is important to note that the proposed Layout-aware Next Token Prediction is a completely self-supervised pre-training procedure, unlike previous works that require human annotations of document structure data or synthetic data generated by larger LLMs such as GPT-4."", ""Claim10: Thus, LNTP facilitates the creation of large-scale, high-fidelity pre-training datasets at minimal cost."", ""Claim11: Experimental results in Section 4.6 demonstrate that utilizing SSFT can further enhance model performance, making it more robust to disruptions in input token order."", ""Claim12: The experimental results shown in Tab. 1 demonstrate the outstanding performance of the LayTextLLM series across various tasks."", ""Claim13: LayTextLLM zero significantly outperforms previous SOTA OCR-free methods, such as TextMonkey [12], in zero-shot capabilities, even when these methods use the training set of the dataset."", ""Claim14: For example, in the DocVQA and InfoVQA datasets, LayTextLLM zero achieves accuracies of 71.8% and 33.8%, respectively, which are markedly higher than existing OCR-free methods such as TextMonkey and InternLM-XComposer2."", ""Claim15: When fine-tuned with corresponding datasets, LayTextLLM shows even greater performance improvements, particularly in document-oriented VQA datasets."", ""Claim16: Specifically, its accuracies on DocVQA and InfoVQA increase to 77.4% and 66.1%, respectively, demonstrating the model\u2019s strong ability to leverage task-specific data."", ""Claim17: LayTextLLM zero excels in KIE datasets, particularly on the SROIE and POIE datasets, achieving accuracies of 86.7% and 66.1%, respectively."", ""Claim18: These results significantly surpass those of previous SOTA OCR-free model ( i.e. , TextMonkey + ) by margins of 40.5% and 34.1%, respectively."", ""Claim19: This significant performance gain is likely due to these datasets containing low-resolution images that are too blurred for current MLLMs to extract visual features, whereas LayTextLLM shows robustness in such challenging scenarios."", ""Claim20: The experimental results presented in Tab. 2 showcase significant performance improvements achieved by LayTextLLM models compared to pure OCR-based SOTA methods such as DocLLM [19]."", ""Claim21: Specifically, when comparing with DocLLM, LayTextLLM zero demonstrates notably superior performance, with even its zero-shot capabilities being competitive with supervised SFT approaches."", ""Claim22: Similarly, when contrasting with coordinate-as-tokens employed in Llama2-7B, LayTextLLM zero again outperforms significantly."", ""Claim23: In comparison to LayoutLLM [21], our approach exhibits discrepant performance in different tasks, as shown in Tab. 3."", ""Claim24: In zero-shot scenarios, we outperform LayoutLLM in most KIE datasets, validating our capability to leverage OCR-based results effectively."", ""Claim25: However, we fall short on document-oriented VQA tasks since answering some questions that are strongly related to vision information may challenge our approach."", ""Claim26: However, when fine-tuned with tailored data, LayTextLLM significantly outperforms LayoutLLM, showcasing its strong ability to utilize task-specific data."", ""Claim27: It is evident that both LNTP and SSFT significantly enhance the utility of LayTextLLM."", ""Claim28: Specifically, disabling LNTP results in an 8% decrease in performance on VQA tasks and a 5.5% decrease on KIE tasks."", ""Claim29: Disabling SSFT leads to a decrease in average accuracy by 6.7% and 2.6% for VQA and KIE tasks, respectively."", ""Claim30: Although LayTextLLM has shown significant capabilities in text-rich VQA and KIE tasks, this alone does not suffice for all real-world applications."", ""Claim31: Extensive experiments confirm the effectiveness of LayTextLLM."", ""Claim32: The results indicate that LayTextLLM is highly effective in utilizing spatial layout information to make more accurate predictions for these challenging examples."", ""Claim33: In contrast, LayTextLLM integrates layout information to accurately predict the total price, as demonstrated by the other examples, underscoring the utility of LayTextLLM."", ""Claim34: LayTextLLM consistently outperforms Llama2-7B-base across all levels of shuffling, which further underscores the significance of interleaving spatial layouts with text."", ""Claim35: Particularly at the 100% shuffle level, Llama2-7B-base demonstrates limited accuracy at only 20.3, while LayTextLLM maintains a relatively higher performance."", ""Claim36: It is also interesting to note that Llama2-7B-base generally improves as the shuffling percentage decreases, whereas LayTextLLM performs best when 20% of the examples with OCR-derived text are shuffled."", ""Claim37: This observation suggests that LayTextLLM effectively utilizes spatial layouts and is less dependent on the sequence of input tokens."", ""Claim38: Therefore, a certain proportion of shuffled examples can serve as adversarial examples to enhance the model's robustness, addressing situations such as errors in the text order from the OCR engine, which are caused by subtle differences in horizontal or vertical coordinates."", ""Claim39: However, with only text and layout information as input, the proposed LayTextLLM inevitably have difficulties in reasoning visual-related information."", ""Claim40: Thus, on the ChartQA dataset, LayTextLLM can hardly achieve better performance than previous methods that include visual inputs."", ""Claim41: Although the visual information is not used in LayTextLLM, it can still exhibit better zero-shot ability than UniDoc."", ""Claim42: After incorporating the training set of ChartQA, the performance of LayTextLLM can be boosted to 35.7%.""], ""review"": ""Review: Weakness: 1. Due to miss the crucial visual information necessary for document understanding, this LayoutTextLLM heavily relies on OCR-derived text and spatial layouts. Other works such as LayoutLLM, layoutLMv3, introduces visual information to enhance the document understanding performance.\n2. The exploration of the shuffling ratio was conducted only on Key Information Extraction (KIE) tasks. It should also be validated on Visual Question Answering (VQA) datasets to determine if the 20% shuffling ratio is optimal across different types of tasks.\n3. The effectiveness of LNTP and SSFT methods should be substantiated with more ablation studies. It is recommended to fine-tune Llama2-7B directly using the existing data for a more comparisons.\n4\u3001Although LayTextLLM shows higher performance on DocVQA compared to LayoutLLM, this comparison is not entirely fair as LayoutLLM was evaluated in a zero-shot setting. Moreover, the zero-shot performance of LayoutLMv3 on DocVQA surpasses that of LayTextLLM.\nQuestions: Compared with layoutlm series\uff0cwhat are the advantages of the encoding bounding box method proposed in this paper.\n\n""}, ""review"": ""Title: A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding\nAbstract: Recently, many studies have demonstrated that exclusively incorporating OCR-derived text and spatial layouts with large language models (LLMs) can be highly effective for document understanding tasks. However, existing methods that integrate spatial layouts with text have limitations, such as producing overly long text sequences or failing to fully leverage the autoregressive traits of LLMs. In this work, we introduce nterleaving Layout and Text in a Large Language Model (LayTextLLM)} for document understanding. In particular, LayTextLLM projects each bounding box to a single embedding and interleaves it with text, efficiently avoiding long sequence issues while leveraging autoregressive traits of LLMs. LayTextLLM not only streamlines the interaction of layout and textual data but also shows enhanced performance in Key Information Extraction (KIE) and Visual Question Answering (VQA). Comprehensive benchmark evaluations reveal significant improvements, with a 27.0\\% increase on KIE tasks and 24.1\\% on VQA tasks compared to previous state-of-the-art document understanding MLLMs, as well as a 15.5\\% improvement over other SOTA OCR-based LLMs on KIE tasks.\n\nClaim1: LayTextLLM not only streamlines the interaction of layout and textual data but also shows enhanced performance in Key Information Extraction (KIE) and Visual Question Answering (VQA).\nClaim2: Comprehensive benchmark evaluations reveal significant improvements, with a 27.0% increase on KIE tasks and 24.1% on VQA tasks compared to previous state-of-the-art document understanding MLLMs, as well as a 15.5% improvement over other SOTA OCR-based LLMs on KIE tasks.\nClaim3: Finally, through the proposed Shuffled-OCR Supervised Fine-tuning, LayTextLLM significantly improves performance on downstream document-related VQA and KIE tasks.\nClaim4: LayTextLLM significantly outperforms the 175B models, while only slightly increasing or even reducing the sequence length compared to DocLLM.\nClaim5: This enhancement enables the model to process both spatial layouts and textual inputs simultaneously.\nClaim6: Compared to the coordinate-as-tokens scheme, the SLP represents each bounding box with a single token.\nClaim7: This approach significantly reduces the number of input tokens and adheres to the practice of interleaving any modality with text, effectively integrating layout and textual information into a unified sequence.\nClaim8: This allows the model to process both modalities simultaneously and coherently, fully leveraging the autoregressive traits of LLMs.\nClaim9: It is important to note that the proposed Layout-aware Next Token Prediction is a completely self-supervised pre-training procedure, unlike previous works that require human annotations of document structure data or synthetic data generated by larger LLMs such as GPT-4.\nClaim10: Thus, LNTP facilitates the creation of large-scale, high-fidelity pre-training datasets at minimal cost.\nClaim11: Experimental results in Section 4.6 demonstrate that utilizing SSFT can further enhance model performance, making it more robust to disruptions in input token order.\nClaim12: The experimental results shown in Tab. 1 demonstrate the outstanding performance of the LayTextLLM series across various tasks.\nClaim13: LayTextLLM zero significantly outperforms previous SOTA OCR-free methods, such as TextMonkey [12], in zero-shot capabilities, even when these methods use the training set of the dataset.\nClaim14: For example, in the DocVQA and InfoVQA datasets, LayTextLLM zero achieves accuracies of 71.8% and 33.8%, respectively, which are markedly higher than existing OCR-free methods such as TextMonkey and InternLM-XComposer2.\nClaim15: When fine-tuned with corresponding datasets, LayTextLLM shows even greater performance improvements, particularly in document-oriented VQA datasets.\nClaim16: Specifically, its accuracies on DocVQA and InfoVQA increase to 77.4% and 66.1%, respectively, demonstrating the model\u2019s strong ability to leverage task-specific data.\nClaim17: LayTextLLM zero excels in KIE datasets, particularly on the SROIE and POIE datasets, achieving accuracies of 86.7% and 66.1%, respectively.\nClaim18: These results significantly surpass those of previous SOTA OCR-free model ( i.e. , TextMonkey + ) by margins of 40.5% and 34.1%, respectively.\nClaim19: This significant performance gain is likely due to these datasets containing low-resolution images that are too blurred for current MLLMs to extract visual features, whereas LayTextLLM shows robustness in such challenging scenarios.\nClaim20: The experimental results presented in Tab. 2 showcase significant performance improvements achieved by LayTextLLM models compared to pure OCR-based SOTA methods such as DocLLM [19].\nClaim21: Specifically, when comparing with DocLLM, LayTextLLM zero demonstrates notably superior performance, with even its zero-shot capabilities being competitive with supervised SFT approaches.\nClaim22: Similarly, when contrasting with coordinate-as-tokens employed in Llama2-7B, LayTextLLM zero again outperforms significantly.\nClaim23: In comparison to LayoutLLM [21], our approach exhibits discrepant performance in different tasks, as shown in Tab. 3.\nClaim24: In zero-shot scenarios, we outperform LayoutLLM in most KIE datasets, validating our capability to leverage OCR-based results effectively.\nClaim25: However, we fall short on document-oriented VQA tasks since answering some questions that are strongly related to vision information may challenge our approach.\nClaim26: However, when fine-tuned with tailored data, LayTextLLM significantly outperforms LayoutLLM, showcasing its strong ability to utilize task-specific data.\nClaim27: It is evident that both LNTP and SSFT significantly enhance the utility of LayTextLLM.\nClaim28: Specifically, disabling LNTP results in an 8% decrease in performance on VQA tasks and a 5.5% decrease on KIE tasks.\nClaim29: Disabling SSFT leads to a decrease in average accuracy by 6.7% and 2.6% for VQA and KIE tasks, respectively.\nClaim30: Although LayTextLLM has shown significant capabilities in text-rich VQA and KIE tasks, this alone does not suffice for all real-world applications.\nClaim31: Extensive experiments confirm the effectiveness of LayTextLLM.\nClaim32: The results indicate that LayTextLLM is highly effective in utilizing spatial layout information to make more accurate predictions for these challenging examples.\nClaim33: In contrast, LayTextLLM integrates layout information to accurately predict the total price, as demonstrated by the other examples, underscoring the utility of LayTextLLM.\nClaim34: LayTextLLM consistently outperforms Llama2-7B-base across all levels of shuffling, which further underscores the significance of interleaving spatial layouts with text.\nClaim35: Particularly at the 100% shuffle level, Llama2-7B-base demonstrates limited accuracy at only 20.3, while LayTextLLM maintains a relatively higher performance.\nClaim36: It is also interesting to note that Llama2-7B-base generally improves as the shuffling percentage decreases, whereas LayTextLLM performs best when 20% of the examples with OCR-derived text are shuffled.\nClaim37: This observation suggests that LayTextLLM effectively utilizes spatial layouts and is less dependent on the sequence of input tokens.\nClaim38: Therefore, a certain proportion of shuffled examples can serve as adversarial examples to enhance the model's robustness, addressing situations such as errors in the text order from the OCR engine, which are caused by subtle differences in horizontal or vertical coordinates.\nClaim39: However, with only text and layout information as input, the proposed LayTextLLM inevitably have difficulties in reasoning visual-related information.\nClaim40: Thus, on the ChartQA dataset, LayTextLLM can hardly achieve better performance than previous methods that include visual inputs.\nClaim41: Although the visual information is not used in LayTextLLM, it can still exhibit better zero-shot ability than UniDoc.\nClaim42: After incorporating the training set of ChartQA, the performance of LayTextLLM can be boosted to 35.7%.\n\nReview: Weakness: 1. Due to miss the crucial visual information necessary for document understanding, this LayoutTextLLM heavily relies on OCR-derived text and spatial layouts. Other works such as LayoutLLM, layoutLMv3, introduces visual information to enhance the document understanding performance.\n2. The exploration of the shuffling ratio was conducted only on Key Information Extraction (KIE) tasks. It should also be validated on Visual Question Answering (VQA) datasets to determine if the 20% shuffling ratio is optimal across different types of tasks.\n3. The effectiveness of LNTP and SSFT methods should be substantiated with more ablation studies. It is recommended to fine-tune Llama2-7B directly using the existing data for a more comparisons.\n4\u3001Although LayTextLLM shows higher performance on DocVQA compared to LayoutLLM, this comparison is not entirely fair as LayoutLLM was evaluated in a zero-shot setting. Moreover, the zero-shot performance of LayoutLMv3 on DocVQA surpasses that of LayTextLLM.\nQuestions: Compared with layoutlm series\uff0cwhat are the advantages of the encoding bounding box method proposed in this paper.\n\n\n\n"", ""id"": ""rSSpEmrN0C1"", ""pdf"": ""openreview.net/pdf?id=rSSpEmrN0C""}","37"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""The repeated claim that DocLLM cannot fully exploit autoregressive features is not adequately explained."", ""Target claims"": [1], ""Reasoning"": ""The review explicitly mentions that the repeated claim regarding DocLLM's inability to exploit autoregressive features is not well-explained. This directly questions the grounding of Claim1 where the capability of LayTextLLM to leverage autoregressive traits is highlighted, implying that only LayTextLLM, as opposed to alternatives like DocLLM, effectively utilizes these traits. Therefore, Claim1 is the direct target since it connects to the discussion about the autoregressive features.""}, {""Weakness span"": ""Limited comparisons: There is no comparison with alternative methods that embed coordinates, such as co-as-token approaches (Lmdx, Shikra, ICL-D3IE)."", ""Target claims"": [22], ""Reasoning"": ""The review highlights a lack of comparison with alternative coordinate embedding methods as a limitation. Claim22 directly involves contrasting LayTextLLM with the coordinate-as-tokens scheme employed in Llama2-7B, asserting its superiority. The criticism in the review directly undermines the claim of superiority stated in Claim22 by pointing out the absence of comprehensive comparative evaluations with relevant alternative approaches.""}, {""Weakness span"": ""Marginal token reduction: The reduction in the number of tokens appears to be limited, and the paper does not clarify whether words or lines are encoded, which could have a significant impact on token reduction."", ""Target claims"": [7], ""Reasoning"": ""The criticism about marginal token reduction questions the effectiveness claimed in Claim7 regarding the reduction of input tokens and the integration of layout and textual information. The lack of clarity in encoding details directly challenges the claim, as it suggests that the simplification benefit might not be as significant as asserted.""}]}, ""meta"": {""id"": ""rSSpEmrN0C"", ""review_idx"": 2, ""title"": ""Title: A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding\nAbstract: Recently, many studies have demonstrated that exclusively incorporating OCR-derived text and spatial layouts with large language models (LLMs) can be highly effective for document understanding tasks. However, existing methods that integrate spatial layouts with text have limitations, such as producing overly long text sequences or failing to fully leverage the autoregressive traits of LLMs. In this work, we introduce nterleaving Layout and Text in a Large Language Model (LayTextLLM)} for document understanding. In particular, LayTextLLM projects each bounding box to a single embedding and interleaves it with text, efficiently avoiding long sequence issues while leveraging autoregressive traits of LLMs. LayTextLLM not only streamlines the interaction of layout and textual data but also shows enhanced performance in Key Information Extraction (KIE) and Visual Question Answering (VQA). Comprehensive benchmark evaluations reveal significant improvements, with a 27.0\\% increase on KIE tasks and 24.1\\% on VQA tasks compared to previous state-of-the-art document understanding MLLMs, as well as a 15.5\\% improvement over other SOTA OCR-based LLMs on KIE tasks.\n\nTLDR:\n\nKeywords:LLM, DocAI, Visually Rich Document Understanding, KIE"", ""claims"": [""Claim1: LayTextLLM not only streamlines the interaction of layout and textual data but also shows enhanced performance in Key Information Extraction (KIE) and Visual Question Answering (VQA)."", ""Claim2: Comprehensive benchmark evaluations reveal significant improvements, with a 27.0% increase on KIE tasks and 24.1% on VQA tasks compared to previous state-of-the-art document understanding MLLMs, as well as a 15.5% improvement over other SOTA OCR-based LLMs on KIE tasks."", ""Claim3: Finally, through the proposed Shuffled-OCR Supervised Fine-tuning, LayTextLLM significantly improves performance on downstream document-related VQA and KIE tasks."", ""Claim4: LayTextLLM significantly outperforms the 175B models, while only slightly increasing or even reducing the sequence length compared to DocLLM."", ""Claim5: This enhancement enables the model to process both spatial layouts and textual inputs simultaneously."", ""Claim6: Compared to the coordinate-as-tokens scheme, the SLP represents each bounding box with a single token."", ""Claim7: This approach significantly reduces the number of input tokens and adheres to the practice of interleaving any modality with text, effectively integrating layout and textual information into a unified sequence."", ""Claim8: This allows the model to process both modalities simultaneously and coherently, fully leveraging the autoregressive traits of LLMs."", ""Claim9: It is important to note that the proposed Layout-aware Next Token Prediction is a completely self-supervised pre-training procedure, unlike previous works that require human annotations of document structure data or synthetic data generated by larger LLMs such as GPT-4."", ""Claim10: Thus, LNTP facilitates the creation of large-scale, high-fidelity pre-training datasets at minimal cost."", ""Claim11: Experimental results in Section 4.6 demonstrate that utilizing SSFT can further enhance model performance, making it more robust to disruptions in input token order."", ""Claim12: The experimental results shown in Tab. 1 demonstrate the outstanding performance of the LayTextLLM series across various tasks."", ""Claim13: LayTextLLM zero significantly outperforms previous SOTA OCR-free methods, such as TextMonkey [12], in zero-shot capabilities, even when these methods use the training set of the dataset."", ""Claim14: For example, in the DocVQA and InfoVQA datasets, LayTextLLM zero achieves accuracies of 71.8% and 33.8%, respectively, which are markedly higher than existing OCR-free methods such as TextMonkey and InternLM-XComposer2."", ""Claim15: When fine-tuned with corresponding datasets, LayTextLLM shows even greater performance improvements, particularly in document-oriented VQA datasets."", ""Claim16: Specifically, its accuracies on DocVQA and InfoVQA increase to 77.4% and 66.1%, respectively, demonstrating the model\u2019s strong ability to leverage task-specific data."", ""Claim17: LayTextLLM zero excels in KIE datasets, particularly on the SROIE and POIE datasets, achieving accuracies of 86.7% and 66.1%, respectively."", ""Claim18: These results significantly surpass those of previous SOTA OCR-free model ( i.e. , TextMonkey + ) by margins of 40.5% and 34.1%, respectively."", ""Claim19: This significant performance gain is likely due to these datasets containing low-resolution images that are too blurred for current MLLMs to extract visual features, whereas LayTextLLM shows robustness in such challenging scenarios."", ""Claim20: The experimental results presented in Tab. 2 showcase significant performance improvements achieved by LayTextLLM models compared to pure OCR-based SOTA methods such as DocLLM [19]."", ""Claim21: Specifically, when comparing with DocLLM, LayTextLLM zero demonstrates notably superior performance, with even its zero-shot capabilities being competitive with supervised SFT approaches."", ""Claim22: Similarly, when contrasting with coordinate-as-tokens employed in Llama2-7B, LayTextLLM zero again outperforms significantly."", ""Claim23: In comparison to LayoutLLM [21], our approach exhibits discrepant performance in different tasks, as shown in Tab. 3."", ""Claim24: In zero-shot scenarios, we outperform LayoutLLM in most KIE datasets, validating our capability to leverage OCR-based results effectively."", ""Claim25: However, we fall short on document-oriented VQA tasks since answering some questions that are strongly related to vision information may challenge our approach."", ""Claim26: However, when fine-tuned with tailored data, LayTextLLM significantly outperforms LayoutLLM, showcasing its strong ability to utilize task-specific data."", ""Claim27: It is evident that both LNTP and SSFT significantly enhance the utility of LayTextLLM."", ""Claim28: Specifically, disabling LNTP results in an 8% decrease in performance on VQA tasks and a 5.5% decrease on KIE tasks."", ""Claim29: Disabling SSFT leads to a decrease in average accuracy by 6.7% and 2.6% for VQA and KIE tasks, respectively."", ""Claim30: Although LayTextLLM has shown significant capabilities in text-rich VQA and KIE tasks, this alone does not suffice for all real-world applications."", ""Claim31: Extensive experiments confirm the effectiveness of LayTextLLM."", ""Claim32: The results indicate that LayTextLLM is highly effective in utilizing spatial layout information to make more accurate predictions for these challenging examples."", ""Claim33: In contrast, LayTextLLM integrates layout information to accurately predict the total price, as demonstrated by the other examples, underscoring the utility of LayTextLLM."", ""Claim34: LayTextLLM consistently outperforms Llama2-7B-base across all levels of shuffling, which further underscores the significance of interleaving spatial layouts with text."", ""Claim35: Particularly at the 100% shuffle level, Llama2-7B-base demonstrates limited accuracy at only 20.3, while LayTextLLM maintains a relatively higher performance."", ""Claim36: It is also interesting to note that Llama2-7B-base generally improves as the shuffling percentage decreases, whereas LayTextLLM performs best when 20% of the examples with OCR-derived text are shuffled."", ""Claim37: This observation suggests that LayTextLLM effectively utilizes spatial layouts and is less dependent on the sequence of input tokens."", ""Claim38: Therefore, a certain proportion of shuffled examples can serve as adversarial examples to enhance the model's robustness, addressing situations such as errors in the text order from the OCR engine, which are caused by subtle differences in horizontal or vertical coordinates."", ""Claim39: However, with only text and layout information as input, the proposed LayTextLLM inevitably have difficulties in reasoning visual-related information."", ""Claim40: Thus, on the ChartQA dataset, LayTextLLM can hardly achieve better performance than previous methods that include visual inputs."", ""Claim41: Although the visual information is not used in LayTextLLM, it can still exhibit better zero-shot ability than UniDoc."", ""Claim42: After incorporating the training set of ChartQA, the performance of LayTextLLM can be boosted to 35.7%.""], ""review"": ""Review: Weakness: - Incomplete related work: The paper omits several relevant OCR-based models, such as UDOC, LayoutMask, BROS, LAMBERT, DocFormer and LiLt.\n- Insufficient explanation: The repeated claim that DocLLM cannot fully exploit autoregressive features is not adequately explained.\n- Limited comparisons: There is no comparison with alternative methods that embed coordinates, such as co-as-token approaches (Lmdx, Shikra, ICL-D3IE).\n- Marginal token reduction: The reduction in the number of tokens appears to be limited, and the paper does not clarify whether words or lines are encoded, which could have a significant impact on token reduction.\nQuestions: - Why use an embedding to encode coordinates that are already 4D vectors? What is the gain, considering there is no additional information (e.g., font, style, zone type)?\n- How can SLP be trained if there is no loss computed on bounding box tokens?\n- How do you explain the good performance of LayTextLLM_zero?\n\n""}, ""review"": ""Title: A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding\nAbstract: Recently, many studies have demonstrated that exclusively incorporating OCR-derived text and spatial layouts with large language models (LLMs) can be highly effective for document understanding tasks. However, existing methods that integrate spatial layouts with text have limitations, such as producing overly long text sequences or failing to fully leverage the autoregressive traits of LLMs. In this work, we introduce nterleaving Layout and Text in a Large Language Model (LayTextLLM)} for document understanding. In particular, LayTextLLM projects each bounding box to a single embedding and interleaves it with text, efficiently avoiding long sequence issues while leveraging autoregressive traits of LLMs. LayTextLLM not only streamlines the interaction of layout and textual data but also shows enhanced performance in Key Information Extraction (KIE) and Visual Question Answering (VQA). Comprehensive benchmark evaluations reveal significant improvements, with a 27.0\\% increase on KIE tasks and 24.1\\% on VQA tasks compared to previous state-of-the-art document understanding MLLMs, as well as a 15.5\\% improvement over other SOTA OCR-based LLMs on KIE tasks.\n\nClaim1: LayTextLLM not only streamlines the interaction of layout and textual data but also shows enhanced performance in Key Information Extraction (KIE) and Visual Question Answering (VQA).\nClaim2: Comprehensive benchmark evaluations reveal significant improvements, with a 27.0% increase on KIE tasks and 24.1% on VQA tasks compared to previous state-of-the-art document understanding MLLMs, as well as a 15.5% improvement over other SOTA OCR-based LLMs on KIE tasks.\nClaim3: Finally, through the proposed Shuffled-OCR Supervised Fine-tuning, LayTextLLM significantly improves performance on downstream document-related VQA and KIE tasks.\nClaim4: LayTextLLM significantly outperforms the 175B models, while only slightly increasing or even reducing the sequence length compared to DocLLM.\nClaim5: This enhancement enables the model to process both spatial layouts and textual inputs simultaneously.\nClaim6: Compared to the coordinate-as-tokens scheme, the SLP represents each bounding box with a single token.\nClaim7: This approach significantly reduces the number of input tokens and adheres to the practice of interleaving any modality with text, effectively integrating layout and textual information into a unified sequence.\nClaim8: This allows the model to process both modalities simultaneously and coherently, fully leveraging the autoregressive traits of LLMs.\nClaim9: It is important to note that the proposed Layout-aware Next Token Prediction is a completely self-supervised pre-training procedure, unlike previous works that require human annotations of document structure data or synthetic data generated by larger LLMs such as GPT-4.\nClaim10: Thus, LNTP facilitates the creation of large-scale, high-fidelity pre-training datasets at minimal cost.\nClaim11: Experimental results in Section 4.6 demonstrate that utilizing SSFT can further enhance model performance, making it more robust to disruptions in input token order.\nClaim12: The experimental results shown in Tab. 1 demonstrate the outstanding performance of the LayTextLLM series across various tasks.\nClaim13: LayTextLLM zero significantly outperforms previous SOTA OCR-free methods, such as TextMonkey [12], in zero-shot capabilities, even when these methods use the training set of the dataset.\nClaim14: For example, in the DocVQA and InfoVQA datasets, LayTextLLM zero achieves accuracies of 71.8% and 33.8%, respectively, which are markedly higher than existing OCR-free methods such as TextMonkey and InternLM-XComposer2.\nClaim15: When fine-tuned with corresponding datasets, LayTextLLM shows even greater performance improvements, particularly in document-oriented VQA datasets.\nClaim16: Specifically, its accuracies on DocVQA and InfoVQA increase to 77.4% and 66.1%, respectively, demonstrating the model\u2019s strong ability to leverage task-specific data.\nClaim17: LayTextLLM zero excels in KIE datasets, particularly on the SROIE and POIE datasets, achieving accuracies of 86.7% and 66.1%, respectively.\nClaim18: These results significantly surpass those of previous SOTA OCR-free model ( i.e. , TextMonkey + ) by margins of 40.5% and 34.1%, respectively.\nClaim19: This significant performance gain is likely due to these datasets containing low-resolution images that are too blurred for current MLLMs to extract visual features, whereas LayTextLLM shows robustness in such challenging scenarios.\nClaim20: The experimental results presented in Tab. 2 showcase significant performance improvements achieved by LayTextLLM models compared to pure OCR-based SOTA methods such as DocLLM [19].\nClaim21: Specifically, when comparing with DocLLM, LayTextLLM zero demonstrates notably superior performance, with even its zero-shot capabilities being competitive with supervised SFT approaches.\nClaim22: Similarly, when contrasting with coordinate-as-tokens employed in Llama2-7B, LayTextLLM zero again outperforms significantly.\nClaim23: In comparison to LayoutLLM [21], our approach exhibits discrepant performance in different tasks, as shown in Tab. 3.\nClaim24: In zero-shot scenarios, we outperform LayoutLLM in most KIE datasets, validating our capability to leverage OCR-based results effectively.\nClaim25: However, we fall short on document-oriented VQA tasks since answering some questions that are strongly related to vision information may challenge our approach.\nClaim26: However, when fine-tuned with tailored data, LayTextLLM significantly outperforms LayoutLLM, showcasing its strong ability to utilize task-specific data.\nClaim27: It is evident that both LNTP and SSFT significantly enhance the utility of LayTextLLM.\nClaim28: Specifically, disabling LNTP results in an 8% decrease in performance on VQA tasks and a 5.5% decrease on KIE tasks.\nClaim29: Disabling SSFT leads to a decrease in average accuracy by 6.7% and 2.6% for VQA and KIE tasks, respectively.\nClaim30: Although LayTextLLM has shown significant capabilities in text-rich VQA and KIE tasks, this alone does not suffice for all real-world applications.\nClaim31: Extensive experiments confirm the effectiveness of LayTextLLM.\nClaim32: The results indicate that LayTextLLM is highly effective in utilizing spatial layout information to make more accurate predictions for these challenging examples.\nClaim33: In contrast, LayTextLLM integrates layout information to accurately predict the total price, as demonstrated by the other examples, underscoring the utility of LayTextLLM.\nClaim34: LayTextLLM consistently outperforms Llama2-7B-base across all levels of shuffling, which further underscores the significance of interleaving spatial layouts with text.\nClaim35: Particularly at the 100% shuffle level, Llama2-7B-base demonstrates limited accuracy at only 20.3, while LayTextLLM maintains a relatively higher performance.\nClaim36: It is also interesting to note that Llama2-7B-base generally improves as the shuffling percentage decreases, whereas LayTextLLM performs best when 20% of the examples with OCR-derived text are shuffled.\nClaim37: This observation suggests that LayTextLLM effectively utilizes spatial layouts and is less dependent on the sequence of input tokens.\nClaim38: Therefore, a certain proportion of shuffled examples can serve as adversarial examples to enhance the model's robustness, addressing situations such as errors in the text order from the OCR engine, which are caused by subtle differences in horizontal or vertical coordinates.\nClaim39: However, with only text and layout information as input, the proposed LayTextLLM inevitably have difficulties in reasoning visual-related information.\nClaim40: Thus, on the ChartQA dataset, LayTextLLM can hardly achieve better performance than previous methods that include visual inputs.\nClaim41: Although the visual information is not used in LayTextLLM, it can still exhibit better zero-shot ability than UniDoc.\nClaim42: After incorporating the training set of ChartQA, the performance of LayTextLLM can be boosted to 35.7%.\n\nReview: Weakness: - Incomplete related work: The paper omits several relevant OCR-based models, such as UDOC, LayoutMask, BROS, LAMBERT, DocFormer and LiLt.\n- Insufficient explanation: The repeated claim that DocLLM cannot fully exploit autoregressive features is not adequately explained.\n- Limited comparisons: There is no comparison with alternative methods that embed coordinates, such as co-as-token approaches (Lmdx, Shikra, ICL-D3IE).\n- Marginal token reduction: The reduction in the number of tokens appears to be limited, and the paper does not clarify whether words or lines are encoded, which could have a significant impact on token reduction.\nQuestions: - Why use an embedding to encode coordinates that are already 4D vectors? What is the gain, considering there is no additional information (e.g., font, style, zone type)?\n- How can SLP be trained if there is no loss computed on bounding box tokens?\n- How do you explain the good performance of LayTextLLM_zero?\n\n\n\n"", ""id"": ""rSSpEmrN0C2"", ""pdf"": ""openreview.net/pdf?id=rSSpEmrN0C""}","38"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""The main contribution of the paper is an automatic framework to assess the quality of the IE; however, the authors didn't conduct any experiments to demonstrate the effectiveness of the proposed framework (e.g., whether the evaluation results correlate with human judgments); the other main limitation is the authors evaluate the quality of extraction based on the proportion of successfully extracted needles but totally ignore the correctness of extracted information (precision)"", ""Target claims"": [1, 4, 5, 6], ""Reasoning"": ""This weakness questions the effectiveness and precision of the automatic framework introduced in the paper (Claim1) as there is no experimental evidence showing it correlates with human judgments and that the evaluation ignores precision (Claim4, Claim5). Furthermore, Claim6 is directly challenged because it argues the utility of the MINEA score without showing alignment with human judgments, thus questioning the claim of demonstrated utility.""}, {""Weakness span"": ""The experiments are conducted on private datasets with only several toy examples described in the paper; it will be very difficult for others to reproduce the results. I would suggest conducting experiments at least on some document IE datasets, for example, from news or biomedical domains."", ""Target claims"": [32], ""Reasoning"": ""This portion of the review raises concerns about the limited scope and generalizability of the experiments, which directly targets Claim32. The claim asserts an empirical analysis of the MINEA score on a specialized dataset, but by conducting experiments only on private datasets with toy examples, its applicability to broader domains is questioned.""}]}, ""meta"": {""id"": ""qNYYb4nOKA"", ""review_idx"": 0, ""title"": ""Title: Assessing the quality of information extraction\nAbstract: Advances in large language models have notably enhanced the efficiency of information extraction from unstructured and semi-structured data sources. As these technologies become integral to various applications, establishing an objective measure for the quality of information extraction becomes imperative. However, the scarcity of labeled data presents significant challenges to this endeavor. In this paper, we introduce an automatic framework to assess the quality of the information extraction/retrieval and its completeness. The framework focuses on information extraction in the form of entity and its properties. We discuss how to handle the input/output size limitations of the large language models and analyze their performance when extracting the information. In particular, we introduce scores to evaluate the quality of the extraction and provide an extensive discussion on how to interpret them.\n\nTLDR:\n\nKeywords:information extraction, large language models, quality evaluation, name entity recognition, needle in a haystack test, schema.org"", ""claims"": [""Claim1: In this paper, we introduce an automatic framework to assess the quality of the information extraction/retrieval and its completeness."", ""Claim2: We discuss how to handle the input/output size limitations of the large language models and analyze their performance when extracting the information."", ""Claim3: In particular, we introduce scores to evaluate the quality of the extraction and provide an extensive discussion on how to interpret them."", ""Claim4: To measure the quality of extraction, we propose an evaluation framework that relies on artificially generated complex information which is infused into the document to test the efficiency of LLMs in IE tasks."", ""Claim5: This paper introduces an iterative extraction process and a novel score, MINEA (Multiple Infused Needle Extraction Accuracy), to address the critical need for objective quality assessment measures."", ""Claim6: The empirical analysis demonstrates the utility of MINEA for evaluating LLM-based IE in scenarios where ground truth is unavailable."", ""Claim7: By automating the quality assessment of information extraction, the framework could reduce the need for manual review by experts, saving time and resources and thus enhance the efficiency and accuracy of information extraction from large volumes of unstructured data."", ""Claim8: To demonstrate shortcomings arising from these limitations numerically we use gpt-4-1106-preview model."", ""Claim9: The combination of both improvements \u2013 text splitting and iterated calls, has proven itself to perform the best."", ""Claim10: In the case of long documents, whose extraction consumes almost the whole context window, LLMs are giving more inconsistent results and we can observe a presence of the Lost in the middle phenomenon, see [8]."", ""Claim11: We extract information from several long documents from our business case which are each split into 15 pieces and its processing consumes almost the whole context window."", ""Claim12: Once the information is extracted from data into a structured form defined by the chosen schema, e.g., Figure 1, the quality of such extraction is important to evaluate."", ""Claim13: The score and its subparts are very useful when comparing distinct extractions among themselves, e.g., we can use it to find an optimal number of iterated LLM calls."", ""Claim14: Below, we present a small comparative study regarding the contribution of iterated extraction to its quality."", ""Claim15: Each iteration enriches the extracted information, but the benefit decreases."", ""Claim16: From the third iteration, i.e., after four LLM calls, the majority of scores in Table 2 are either getting worse or stagnating."", ""Claim17: Further, the risk that the LLM will suffer from hallucinations increases as we observe a growth of bias."", ""Claim18: Unfortunately, such an experiment is unfeasible without labeled data."", ""Claim19: As a consequence, it is unfeasible in many specialized tasks because of the absence of suitable labeled data unseen by LLM models."", ""Claim20: This can be the case with very recent datasets as well as proprietary datasets."", ""Claim21: To overcome this issue we use inspiration by NIAH test to build up an automatic and general procedure to access the quality of IE tasks."", ""Claim22: To measure the quality of extraction we propose a multiple infused needle extraction accuracy (MINEA) score."", ""Claim23: We present several alternative ways how to measure whether the extraction of a needle is successful."", ""Claim24: Table 5 shows scores (ratios of successfully extracted entities) based on the above criteria in the case of our business documents."", ""Claim25: Searching for a needle name in all extracted information gives very accurate results if the entities are well characterized by their name (compare for example types \u2018Person\u2019 and \u2019Legislation\u2019 with type \u2019Insight\u2019 where the name is not a natural attribute)."", ""Claim26: An LLM performs well the entity identification and it is an important criterion in the case of more creative types such as \u2018Insight\u2019."", ""Claim27: MINEA score can be used to compare the performance of distinct LLMs, see Table 6."", ""Claim28: Model gpt-3.5-turbo is outperformed by gpt-4-turbo by almost 15% and gpt-4-turbo is outperformed by gpt-4o model by another 12%."", ""Claim29: The versatile method presented in this paper overcomes the issue by adjustment of the data by insertion of an artificial information, a needle, into it."", ""Claim30: By controlling the generation process of the needles, we created a synthetic ground truth that enables us to absolutely measure the extraction quality even when no labeled data is available."", ""Claim31: We introduced a MINEA score to measure the quality of extraction."", ""Claim32: Our empirical analysis of the MINEA score on a specialized dataset demonstrated its utility for evaluation of LLM-based IE tasks when ground truth is unavailable."", ""Claim33: The quality of extraction is evaluated based on the proportion of successfully extracted needles."", ""Claim34: The paper provides detailed descriptions of the experimental setup, including the use of LLMs for IE and the creation of synthetic ground truth data."", ""Claim35: The paper does not provide open access to the data and code due to the proprietary nature of the business documents used in the experiments. However, it provides detailed instructions on how to replicate the methodology."", ""Claim36: The paper specifies the use of LLMs, the schema used for structuring data, and the process of generating needles for evaluation."", ""Claim37: The paper does not include experiments that require statistical significance testing or error bars. The experiments in Sections 4 and 5 present mean values of reasonably large samples."", ""Claim38: The paper does not provide detailed information on the compute resources used for the experiments."", ""Claim39: The research adheres to the NeurIPS Code of Ethics, ensuring that the methods and data used do not violate ethical guidelines. The proprietary data used is handled with confidentiality and integrity."", ""Claim40: The paper is primarily concerned with the technical methodology, the introduction of the MINEA score, and the empirical analysis of the framework\u2019s performance. The potential positive impacts are mentioned in Introduction: by automating the quality assessment of information extraction, the framework could reduce the need for manual review by experts, saving time and resources and thus enhance the efficiency and accuracy of information extraction from large volumes of unstructured data."", ""Claim41: The negative aspects of using LLMs for IE tasks such as inherited bias and potential hallucinations are mentioned especially in Sections 4.2 (Lost in the middle problem) and 5.1 (bias avoidance score)."", ""Claim42: The paper does not release any data or models that pose a high risk for misuse."", ""Claim43: All existing models are properly referenced and credit to their creators is given. These are either LLMs or metrics such as SUSWIR and METEOR (Section 5 and Appendix A).""], ""review"": ""Review: Weakness: * The writing quality is not great, and several areas require further clarification\n\t* The current paper structure is confusing; not sure what role Sections 3 and 4 play in this paper, e.g., whether the authors were proposing a new LLM-based IE approach\n\t* I suggest providing a formal definition of IE studied in this paper because it is very confusing to know what information is extracted. For example, in the abstract, `entity and its properties` is mentioned; in Section 3, `short paragraphs of text` seem to be the information extracted `from the continuous text`; also see Q2 \n* The main contribution of the paper is an automatic framework to assess the quality of the IE; however, the authors didn't conduct any experiments to demonstrate the effectiveness of the proposed framework (e.g., whether the evaluation results correlate with human judgments); the other main limitation is the authors evaluate the quality of extraction based on the proportion of successfully extracted needles but totally ignore the correctness of extracted information (precision)\n* The experiments are conducted on private datasets with only several toy examples described in the paper; it will be very difficult for others to reproduce the results. I would suggest conducting experiments at least on some document IE datasets, for example, from news or biomedical domains.\nQuestions: 1. Line 7: Information retrieval is mentioned once but not explained anywhere else; suggest clarifying its meaning in this paper\n2. Figure 1: should the value of these fields (e.g., name, description, keywords) be directly copied from the original text? what do these numbers (9) (8) in the `keywords` field mean? \n3. Table 1: what higher redundancy scores (more duplicated entities) mean? How do these results tell the `Lost in the Middle` phenomenon? \n4. What LLMs do you use to generate the needles and for identifying needles? How does this affect the extraction models to be tested? e.g., is the model more likely to achieve better performance if the needle is created by the same model? does the model achieve better scores if the same model is used for evaluating whether the needle is found (`llm` column in Table 4)?\n5. Table 3: what does `chosen schema` mean? Do you mean the LLM is not instructed to recognize entities belonging to these categories, but they are still recognized?\n6. What evidence (empirical results) can support the claim that 'the combination of both improvements --- text splitting and iterated calls, has proven itself to perform the best (line 146)'?\n\n""}, ""review"": ""Title: Assessing the quality of information extraction\nAbstract: Advances in large language models have notably enhanced the efficiency of information extraction from unstructured and semi-structured data sources. As these technologies become integral to various applications, establishing an objective measure for the quality of information extraction becomes imperative. However, the scarcity of labeled data presents significant challenges to this endeavor. In this paper, we introduce an automatic framework to assess the quality of the information extraction/retrieval and its completeness. The framework focuses on information extraction in the form of entity and its properties. We discuss how to handle the input/output size limitations of the large language models and analyze their performance when extracting the information. In particular, we introduce scores to evaluate the quality of the extraction and provide an extensive discussion on how to interpret them.\n\nClaim1: In this paper, we introduce an automatic framework to assess the quality of the information extraction/retrieval and its completeness.\nClaim2: We discuss how to handle the input/output size limitations of the large language models and analyze their performance when extracting the information.\nClaim3: In particular, we introduce scores to evaluate the quality of the extraction and provide an extensive discussion on how to interpret them.\nClaim4: To measure the quality of extraction, we propose an evaluation framework that relies on artificially generated complex information which is infused into the document to test the efficiency of LLMs in IE tasks.\nClaim5: This paper introduces an iterative extraction process and a novel score, MINEA (Multiple Infused Needle Extraction Accuracy), to address the critical need for objective quality assessment measures.\nClaim6: The empirical analysis demonstrates the utility of MINEA for evaluating LLM-based IE in scenarios where ground truth is unavailable.\nClaim7: By automating the quality assessment of information extraction, the framework could reduce the need for manual review by experts, saving time and resources and thus enhance the efficiency and accuracy of information extraction from large volumes of unstructured data.\nClaim8: To demonstrate shortcomings arising from these limitations numerically we use gpt-4-1106-preview model.\nClaim9: The combination of both improvements \u2013 text splitting and iterated calls, has proven itself to perform the best.\nClaim10: In the case of long documents, whose extraction consumes almost the whole context window, LLMs are giving more inconsistent results and we can observe a presence of the Lost in the middle phenomenon, see [8].\nClaim11: We extract information from several long documents from our business case which are each split into 15 pieces and its processing consumes almost the whole context window.\nClaim12: Once the information is extracted from data into a structured form defined by the chosen schema, e.g., Figure 1, the quality of such extraction is important to evaluate.\nClaim13: The score and its subparts are very useful when comparing distinct extractions among themselves, e.g., we can use it to find an optimal number of iterated LLM calls.\nClaim14: Below, we present a small comparative study regarding the contribution of iterated extraction to its quality.\nClaim15: Each iteration enriches the extracted information, but the benefit decreases.\nClaim16: From the third iteration, i.e., after four LLM calls, the majority of scores in Table 2 are either getting worse or stagnating.\nClaim17: Further, the risk that the LLM will suffer from hallucinations increases as we observe a growth of bias.\nClaim18: Unfortunately, such an experiment is unfeasible without labeled data.\nClaim19: As a consequence, it is unfeasible in many specialized tasks because of the absence of suitable labeled data unseen by LLM models.\nClaim20: This can be the case with very recent datasets as well as proprietary datasets.\nClaim21: To overcome this issue we use inspiration by NIAH test to build up an automatic and general procedure to access the quality of IE tasks.\nClaim22: To measure the quality of extraction we propose a multiple infused needle extraction accuracy (MINEA) score.\nClaim23: We present several alternative ways how to measure whether the extraction of a needle is successful.\nClaim24: Table 5 shows scores (ratios of successfully extracted entities) based on the above criteria in the case of our business documents.\nClaim25: Searching for a needle name in all extracted information gives very accurate results if the entities are well characterized by their name (compare for example types \u2018Person\u2019 and \u2019Legislation\u2019 with type \u2019Insight\u2019 where the name is not a natural attribute).\nClaim26: An LLM performs well the entity identification and it is an important criterion in the case of more creative types such as \u2018Insight\u2019.\nClaim27: MINEA score can be used to compare the performance of distinct LLMs, see Table 6.\nClaim28: Model gpt-3.5-turbo is outperformed by gpt-4-turbo by almost 15% and gpt-4-turbo is outperformed by gpt-4o model by another 12%.\nClaim29: The versatile method presented in this paper overcomes the issue by adjustment of the data by insertion of an artificial information, a needle, into it.\nClaim30: By controlling the generation process of the needles, we created a synthetic ground truth that enables us to absolutely measure the extraction quality even when no labeled data is available.\nClaim31: We introduced a MINEA score to measure the quality of extraction.\nClaim32: Our empirical analysis of the MINEA score on a specialized dataset demonstrated its utility for evaluation of LLM-based IE tasks when ground truth is unavailable.\nClaim33: The quality of extraction is evaluated based on the proportion of successfully extracted needles.\nClaim34: The paper provides detailed descriptions of the experimental setup, including the use of LLMs for IE and the creation of synthetic ground truth data.\nClaim35: The paper does not provide open access to the data and code due to the proprietary nature of the business documents used in the experiments. However, it provides detailed instructions on how to replicate the methodology.\nClaim36: The paper specifies the use of LLMs, the schema used for structuring data, and the process of generating needles for evaluation.\nClaim37: The paper does not include experiments that require statistical significance testing or error bars. The experiments in Sections 4 and 5 present mean values of reasonably large samples.\nClaim38: The paper does not provide detailed information on the compute resources used for the experiments.\nClaim39: The research adheres to the NeurIPS Code of Ethics, ensuring that the methods and data used do not violate ethical guidelines. The proprietary data used is handled with confidentiality and integrity.\nClaim40: The paper is primarily concerned with the technical methodology, the introduction of the MINEA score, and the empirical analysis of the framework\u2019s performance. The potential positive impacts are mentioned in Introduction: by automating the quality assessment of information extraction, the framework could reduce the need for manual review by experts, saving time and resources and thus enhance the efficiency and accuracy of information extraction from large volumes of unstructured data.\nClaim41: The negative aspects of using LLMs for IE tasks such as inherited bias and potential hallucinations are mentioned especially in Sections 4.2 (Lost in the middle problem) and 5.1 (bias avoidance score).\nClaim42: The paper does not release any data or models that pose a high risk for misuse.\nClaim43: All existing models are properly referenced and credit to their creators is given. These are either LLMs or metrics such as SUSWIR and METEOR (Section 5 and Appendix A).\n\nReview: Weakness: * The writing quality is not great, and several areas require further clarification\n\t* The current paper structure is confusing; not sure what role Sections 3 and 4 play in this paper, e.g., whether the authors were proposing a new LLM-based IE approach\n\t* I suggest providing a formal definition of IE studied in this paper because it is very confusing to know what information is extracted. For example, in the abstract, `entity and its properties` is mentioned; in Section 3, `short paragraphs of text` seem to be the information extracted `from the continuous text`; also see Q2 \n* The main contribution of the paper is an automatic framework to assess the quality of the IE; however, the authors didn't conduct any experiments to demonstrate the effectiveness of the proposed framework (e.g., whether the evaluation results correlate with human judgments); the other main limitation is the authors evaluate the quality of extraction based on the proportion of successfully extracted needles but totally ignore the correctness of extracted information (precision)\n* The experiments are conducted on private datasets with only several toy examples described in the paper; it will be very difficult for others to reproduce the results. I would suggest conducting experiments at least on some document IE datasets, for example, from news or biomedical domains.\nQuestions: 1. Line 7: Information retrieval is mentioned once but not explained anywhere else; suggest clarifying its meaning in this paper\n2. Figure 1: should the value of these fields (e.g., name, description, keywords) be directly copied from the original text? what do these numbers (9) (8) in the `keywords` field mean? \n3. Table 1: what higher redundancy scores (more duplicated entities) mean? How do these results tell the `Lost in the Middle` phenomenon? \n4. What LLMs do you use to generate the needles and for identifying needles? How does this affect the extraction models to be tested? e.g., is the model more likely to achieve better performance if the needle is created by the same model? does the model achieve better scores if the same model is used for evaluating whether the needle is found (`llm` column in Table 4)?\n5. Table 3: what does `chosen schema` mean? Do you mean the LLM is not instructed to recognize entities belonging to these categories, but they are still recognized?\n6. What evidence (empirical results) can support the claim that 'the combination of both improvements --- text splitting and iterated calls, has proven itself to perform the best (line 146)'?\n\n\n\n"", ""id"": ""qNYYb4nOKA0"", ""pdf"": ""openreview.net/pdf?id=qNYYb4nOKA""}","39"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""Efficient T2I network: there is no justification about why the network is \u201cefficient\u201d. Simply adopting a smaller LLM like Phi-2 can hardly be claimed as efficient network design."", ""Target claims"": [5, 6, 28], ""Reasoning"": ""The review questions the claim regarding the efficiency of the network presented in claims 5 and 28, which mention reducing training demands and computational requirements. Claim 6 also discusses the computation reductions. The review argues that the justification for claiming the network as efficient is insufficient, directly targeting claims about efficiency made by the authors.""}, {""Weakness span"": ""The author acknowledged that their model lags behind diffusion-based models in visual appeal, limited by the 8K size of VQGAN\u2019s codebook and not targeting visual appeal. However, there is no solution or further study for solving this problem, which limits the scalability of the model."", ""Target claims"": [16], ""Reasoning"": ""This review comment directly questions the completeness and scope of the study around the visual appeal and scalability issues discussed in Claim 16, which acknowledges limitations in visual appeal compared to diffusion-based models. The review points out the absence of further study or solutions for these limitations, directly targeting this claim.""}]}, ""meta"": {""id"": ""qL4nN6Ew7U"", ""review_idx"": 0, ""title"": ""Title: Fantasy: Transformer Meets Transformer in Text-to-Image Generation\nAbstract: We present Fantasy, an efficient text-to-image generation model marrying the decoder-only Large Language Models (LLMs) and transformer-based masked image modeling (MIM). While diffusion models are currently in a leading position in this task, we demonstrate that with appropriate training strategies and high-quality data, MIM can also achieve comparable performance. By incorporating pre-trained decoder-only LLMs as the text encoder, we observe a significant improvement in text fidelity compared to the widely used CLIP text encoder, enhancing the text-image alignment. Our training approach involves two stages: 1) large-scale concept alignment pre-training, and 2) fine-tuning with high-quality instruction-image data. Evaluations on FID, HPSv2 benchmarks, and human feedback demonstrate the competitive performance of Fantasy against state-of-the-art diffusion and autoregressive models.\n\nTLDR:\n\nKeywords:Text-to-Imgae Generation, Transformer, Masked Image Modeling"", ""claims"": [""Claim1: We present Fantasy, an efficient text-to-image generation model marrying the decoder-only Large Language Models (LLMs) and transformer-based masked image modeling (MIM)."", ""Claim2: While diffusion models are currently in a leading position in this task, we demonstrate that with appropriate training strategies and high-quality data, MIM can also achieve comparable performance."", ""Claim3: By incorporating pre-trained decoder-only LLMs as the text encoder, we observe a significant improvement in text fidelity compared to the widely used CLIP text encoder, enhancing the text-image alignment."", ""Claim4: Evaluations on FID, HPSv2 benchmarks, and human feedback demonstrate the competitive performance of Fantasy against state-of-the-art diffusion and autoregressive models."", ""Claim5: In this paper, we present Fantasy, significantly reducing training demands while maintaining the capability of instruction understanding and competitive image generation quality, as shown in Fig. 2."", ""Claim6: With this approach, Fantasy trains a 0.6B parameter T2I model in about 69 A100 GPU days, significantly reducing computation compared to existing diffusion-based methods, while maintaining comparable visual and numerical fidelity."", ""Claim7: Throughout this paper, we present a comprehensive evaluation of Fantasy\u2019s efficacy, showcasing the potential in training high-quality transformer-based image synthesis models compared to diffusion-based models in future."", ""Claim8: Fantasy exhibited outstanding performance across all key aspects among previous Transformer-based methods like CogView2 [12], which is expected."", ""Claim9: The results also reveal its competitive performance compared to prior diffusion-based methods, especially in concept-art and painting, demonstrating similar performance to DALL\u00b7E 2 [26]."", ""Claim10: This remarkable performance is primarily attributed to the text-image alignment learning in fine-tuning stage, where high-quality text-image pairs were leveraged to achieve superior alignment capabilities."", ""Claim11: We observe that the FID of Fantasy is substantially higher compared to other state-of-the-art models."", ""Claim12: Visual inspections reveal that images generated by Fantasy are smoother than those from other leading T2I models."", ""Claim13: Although the state-of-the-art models [43, 11, 29] exhibit lower FID, it relies on unaffordable resources."", ""Claim14: Participants are asked to select a preference of the images based on the visual appeal of the generated images and the precision of alignments between the text prompts and the corresponding images."", ""Claim15: Fig. 4a clearly demonstrates that images generated on relatively long text prompts (longer than 30 words) by Fantasy are distinctly favored among the four models in both two perspective, especially for text-image alignment, aligning closely with the intended use case of Fantasy."", ""Claim16: Our model slightly lags behind diffusion-based models in visual appeal, limited by the 8,192 size of VQGAN\u2019s codebook and not targeting visual appeal."", ""Claim17: Simultaneously, Fantasy lacks a distinct advantage in text-image alignment in the short subset."", ""Claim18: In summary, the human preference experiments confirm the observation made in the HPSv2 benchmarks."", ""Claim19: Fig. 5 vividly illustrates Fantasy\u2019s superior visual appeal and text-image alignment over leading open-source transformer-based T2I models [12, 8] and diffusion-based T2I models [29, 26]."", ""Claim20: Fantasy significantly surpasses existing transformer-based T2I models, matches the performance of SDXL [29], and qualitatively outperforms Dall\u00b7E 2 [26]."", ""Claim21: Despite being trained on images with a resolution of 256 \u00d7 256, Fantasy ensures generated low-resolution images contain sufficient details, indirectly supporting long prompts."", ""Claim22: our observations suggest that Fantasy more closely aligns details with prompts than ParaDiffusion [43]."", ""Claim23: Compared to the base model, our synergy fine-tuning with Phi-2 demonstrates a notable improvement in all aspects in Tab. 3."", ""Claim24: The insight indicates that as trainable parameters increase from 257 million to 611 million, performance consistently improves."", ""Claim25: models with fewer parameters underperform on the HPSv2 benchmark, frequently resulting in distorted images and omitted details, yet they may still generate acceptable outcomes."", ""Claim26: the visual quality diverges as model size increases, highlighting the potential for scaling up masked image modeling to enhance instruction-image alignment and elevate generation quality."", ""Claim27: In this paper, we introduce Fantasy, a lightweight and efficient text-to-image model that combines Large Language Models (LLMs) with a transformer-based masked image modeling (MIM), effectively transferring semantic understanding capabilities from LLMs to the text-to-image generation."", ""Claim28: With our proposed two-stage training strategy and high-quality dataset, Fantasy significantly reduces computational requirements while producing high-fidelity images."", ""Claim29: Extensive experiments demonstrate that Fantasy achieves comparable performance to models trained with significantly more computational resources, illustrating the viability of our approach and suggesting potential efficient scalability to even larger masked image modeling for text-to-image generation."", ""Claim30: All theoretical results are accompanied by a clear statement of assumptions and are supported by complete proofs provided in the supplementary materials."", ""Claim31: The paper provides detailed descriptions of the experimental setup, including data splits, hyperparameters, and the type of optimizer used."", ""Claim32: We also provide access to the source code and datasets in the supplementary materials to ensure full reproducibility."", ""Claim33: The paper does not propose a benchmark and we will release the code if the paper is accepted."", ""Claim34: The model depends on non-open-sourced dataset, and the copyright of the checkpoint belongs to the company."", ""Claim35: Detailed instructions for training our model, including command lines, are provided in the supplementary materials."", ""Claim36: The experimental section of the paper provides comprehensive details about the training and test setups, including the rationale behind choosing specific hyperparameters and the types of optimizers used."", ""Claim37: All experimental results are presented with error bars reflecting the standard deviation across multiple runs."", ""Claim38: We provide a detailed explanation of how these were calculated and the assumptions underlying our statistical tests."", ""Claim39: The paper details the computational resources required for each experiment, including the types of GPUs used, the amount of memory, and the execution time."", ""Claim40: Our research adheres strictly to the NeurIPS Code of Ethics."", ""Claim41: We have considered ethical implications, especially regarding the generation of images from text, and have implemented measures to prevent misuse."", ""Claim42: The paper includes content about broader impacts that discusses both the potential positive applications of our method in educational and creative industries, and potential negative impacts, such as the misuse of generated images."", ""Claim43: We also suggest mitigation strategies for potential negative uses.""], ""review"": ""Review: Weakness: - the major concern of the work is unclear contributions. The claimed three contributions or core designs are quite similar with existing works.\n- Efficient T2I network: there is no justification about why the network is \u201cefficient\u201d. Simply adopting a smaller LLM like Phi-2 can hardly be claimed as efficient network design. \n- The hierarchical training strategy was also proposed before, it is not clear what is the difference with existing work.\n- High quality data: the training data utilize Laion-2B and use existing filtering strategy. The collection high quality synthesized images from existing datasets.\n- The evaluation metrics are mainly based on HPSv2, which has a limited range of values, e.g., HPSv2 has close values for SDv1.4 and SD2.0, e.g., 27.26 vs 27.48. Why SDXL is missing in Table 1?\n- The author acknowledged that their model lags behind diffusion-based models in visual appeal, limited by the 8K size of VQGAN\u2019s codebook and not targeting visual appeal. However, there is no solution or further study for solving this problem, which limits the scalability of the model.\n- The scaling study in section 4.2 seems pretty premature and it is unclear what is the limit of the scaling. Increasing the model depth can improve the performance, which has been verified from previous work such as in https://arxiv.org/abs/2212.09748 or https://arxiv.org/abs/2404.02883.\nQuestions: - what is the major difference with existing MIM based methods such as Muse, beside different components and data strategy?\n\n""}, ""review"": ""Title: Fantasy: Transformer Meets Transformer in Text-to-Image Generation\nAbstract: We present Fantasy, an efficient text-to-image generation model marrying the decoder-only Large Language Models (LLMs) and transformer-based masked image modeling (MIM). While diffusion models are currently in a leading position in this task, we demonstrate that with appropriate training strategies and high-quality data, MIM can also achieve comparable performance. By incorporating pre-trained decoder-only LLMs as the text encoder, we observe a significant improvement in text fidelity compared to the widely used CLIP text encoder, enhancing the text-image alignment. Our training approach involves two stages: 1) large-scale concept alignment pre-training, and 2) fine-tuning with high-quality instruction-image data. Evaluations on FID, HPSv2 benchmarks, and human feedback demonstrate the competitive performance of Fantasy against state-of-the-art diffusion and autoregressive models.\n\nClaim1: We present Fantasy, an efficient text-to-image generation model marrying the decoder-only Large Language Models (LLMs) and transformer-based masked image modeling (MIM).\nClaim2: While diffusion models are currently in a leading position in this task, we demonstrate that with appropriate training strategies and high-quality data, MIM can also achieve comparable performance.\nClaim3: By incorporating pre-trained decoder-only LLMs as the text encoder, we observe a significant improvement in text fidelity compared to the widely used CLIP text encoder, enhancing the text-image alignment.\nClaim4: Evaluations on FID, HPSv2 benchmarks, and human feedback demonstrate the competitive performance of Fantasy against state-of-the-art diffusion and autoregressive models.\nClaim5: In this paper, we present Fantasy, significantly reducing training demands while maintaining the capability of instruction understanding and competitive image generation quality, as shown in Fig. 2.\nClaim6: With this approach, Fantasy trains a 0.6B parameter T2I model in about 69 A100 GPU days, significantly reducing computation compared to existing diffusion-based methods, while maintaining comparable visual and numerical fidelity.\nClaim7: Throughout this paper, we present a comprehensive evaluation of Fantasy\u2019s efficacy, showcasing the potential in training high-quality transformer-based image synthesis models compared to diffusion-based models in future.\nClaim8: Fantasy exhibited outstanding performance across all key aspects among previous Transformer-based methods like CogView2 [12], which is expected.\nClaim9: The results also reveal its competitive performance compared to prior diffusion-based methods, especially in concept-art and painting, demonstrating similar performance to DALL\u00b7E 2 [26].\nClaim10: This remarkable performance is primarily attributed to the text-image alignment learning in fine-tuning stage, where high-quality text-image pairs were leveraged to achieve superior alignment capabilities.\nClaim11: We observe that the FID of Fantasy is substantially higher compared to other state-of-the-art models.\nClaim12: Visual inspections reveal that images generated by Fantasy are smoother than those from other leading T2I models.\nClaim13: Although the state-of-the-art models [43, 11, 29] exhibit lower FID, it relies on unaffordable resources.\nClaim14: Participants are asked to select a preference of the images based on the visual appeal of the generated images and the precision of alignments between the text prompts and the corresponding images.\nClaim15: Fig. 4a clearly demonstrates that images generated on relatively long text prompts (longer than 30 words) by Fantasy are distinctly favored among the four models in both two perspective, especially for text-image alignment, aligning closely with the intended use case of Fantasy.\nClaim16: Our model slightly lags behind diffusion-based models in visual appeal, limited by the 8,192 size of VQGAN\u2019s codebook and not targeting visual appeal.\nClaim17: Simultaneously, Fantasy lacks a distinct advantage in text-image alignment in the short subset.\nClaim18: In summary, the human preference experiments confirm the observation made in the HPSv2 benchmarks.\nClaim19: Fig. 5 vividly illustrates Fantasy\u2019s superior visual appeal and text-image alignment over leading open-source transformer-based T2I models [12, 8] and diffusion-based T2I models [29, 26].\nClaim20: Fantasy significantly surpasses existing transformer-based T2I models, matches the performance of SDXL [29], and qualitatively outperforms Dall\u00b7E 2 [26].\nClaim21: Despite being trained on images with a resolution of 256 \u00d7 256, Fantasy ensures generated low-resolution images contain sufficient details, indirectly supporting long prompts.\nClaim22: our observations suggest that Fantasy more closely aligns details with prompts than ParaDiffusion [43].\nClaim23: Compared to the base model, our synergy fine-tuning with Phi-2 demonstrates a notable improvement in all aspects in Tab. 3.\nClaim24: The insight indicates that as trainable parameters increase from 257 million to 611 million, performance consistently improves.\nClaim25: models with fewer parameters underperform on the HPSv2 benchmark, frequently resulting in distorted images and omitted details, yet they may still generate acceptable outcomes.\nClaim26: the visual quality diverges as model size increases, highlighting the potential for scaling up masked image modeling to enhance instruction-image alignment and elevate generation quality.\nClaim27: In this paper, we introduce Fantasy, a lightweight and efficient text-to-image model that combines Large Language Models (LLMs) with a transformer-based masked image modeling (MIM), effectively transferring semantic understanding capabilities from LLMs to the text-to-image generation.\nClaim28: With our proposed two-stage training strategy and high-quality dataset, Fantasy significantly reduces computational requirements while producing high-fidelity images.\nClaim29: Extensive experiments demonstrate that Fantasy achieves comparable performance to models trained with significantly more computational resources, illustrating the viability of our approach and suggesting potential efficient scalability to even larger masked image modeling for text-to-image generation.\nClaim30: All theoretical results are accompanied by a clear statement of assumptions and are supported by complete proofs provided in the supplementary materials.\nClaim31: The paper provides detailed descriptions of the experimental setup, including data splits, hyperparameters, and the type of optimizer used.\nClaim32: We also provide access to the source code and datasets in the supplementary materials to ensure full reproducibility.\nClaim33: The paper does not propose a benchmark and we will release the code if the paper is accepted.\nClaim34: The model depends on non-open-sourced dataset, and the copyright of the checkpoint belongs to the company.\nClaim35: Detailed instructions for training our model, including command lines, are provided in the supplementary materials.\nClaim36: The experimental section of the paper provides comprehensive details about the training and test setups, including the rationale behind choosing specific hyperparameters and the types of optimizers used.\nClaim37: All experimental results are presented with error bars reflecting the standard deviation across multiple runs.\nClaim38: We provide a detailed explanation of how these were calculated and the assumptions underlying our statistical tests.\nClaim39: The paper details the computational resources required for each experiment, including the types of GPUs used, the amount of memory, and the execution time.\nClaim40: Our research adheres strictly to the NeurIPS Code of Ethics.\nClaim41: We have considered ethical implications, especially regarding the generation of images from text, and have implemented measures to prevent misuse.\nClaim42: The paper includes content about broader impacts that discusses both the potential positive applications of our method in educational and creative industries, and potential negative impacts, such as the misuse of generated images.\nClaim43: We also suggest mitigation strategies for potential negative uses.\n\nReview: Weakness: - the major concern of the work is unclear contributions. The claimed three contributions or core designs are quite similar with existing works.\n- Efficient T2I network: there is no justification about why the network is \u201cefficient\u201d. Simply adopting a smaller LLM like Phi-2 can hardly be claimed as efficient network design. \n- The hierarchical training strategy was also proposed before, it is not clear what is the difference with existing work.\n- High quality data: the training data utilize Laion-2B and use existing filtering strategy. The collection high quality synthesized images from existing datasets.\n- The evaluation metrics are mainly based on HPSv2, which has a limited range of values, e.g., HPSv2 has close values for SDv1.4 and SD2.0, e.g., 27.26 vs 27.48. Why SDXL is missing in Table 1?\n- The author acknowledged that their model lags behind diffusion-based models in visual appeal, limited by the 8K size of VQGAN\u2019s codebook and not targeting visual appeal. However, there is no solution or further study for solving this problem, which limits the scalability of the model.\n- The scaling study in section 4.2 seems pretty premature and it is unclear what is the limit of the scaling. Increasing the model depth can improve the performance, which has been verified from previous work such as in https://arxiv.org/abs/2212.09748 or https://arxiv.org/abs/2404.02883.\nQuestions: - what is the major difference with existing MIM based methods such as Muse, beside different components and data strategy?\n\n\n\n"", ""id"": ""qL4nN6Ew7U0"", ""pdf"": ""openreview.net/pdf?id=qL4nN6Ew7U""}","40"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""The paper is overall very critical of the AR field, i.e. \""Why hasn't this field engagement with any real world deployments\"". However, it's also not entirely clear in the current draft how any of the general recommendations would be applied in specific AR domains."", ""Target claims"": [5, 6, 7], ""Reasoning"": ""The reviewer points out that while the paper criticizes the field for lack of real-world engagement, it fails to clarify how the general recommendations can be specifically applied in AR domains. Claims 5, 6, and 7 imply a focus on practical applications and recommendations based on a systematized review. Therefore, this weakness directly questions the effectiveness and clarity of these claims.""}, {""Weakness span"": ""One aspect of the paper that I think would have been most directly helpful to the NeurIPS audience in particular would be to take a stance on how recourse should be defined -- is the definition on line 62 \""endorsed\"" by the paper? Is the \""imagine a counterfactual input x*\"" an advisable approach to take for future work. Does this review support the definition, highlight core definitional or epistemic issues, etc."", ""Target claims"": [], ""Reasoning"": ""This comment questions the paper's stance on the definition of algorithmic recourse, which is a foundational aspect that could underpin many claims. However, without specific reference to a claim in the list, it appears more as a general concern about foundational clarity rather than a weakness directed at a particular claim.""}]}, ""meta"": {""id"": ""oEmyoy5H5P"", ""review_idx"": 0, ""title"": ""Title: Grounding and Validation of Algorithmic Recourse in Real-World Contexts: A Systematized Literature Review\nAbstract: The aim of algorithmic recourse (AR) is generally understood to be the provision of \""actionable\"" recommendations to individuals affected by algorithmic decision-making systems, in an attempt to offer the capacity for taking actions that may lead to more desirable outcomes in the future. Over the past few years, AR literature has largely focused on theoretical frameworks to generate \""actionable\"" counterfactual explanations that further satisfy various desiderata, such as diversity or robustness. We believe that algorithmic recourse, by its nature, should be seen as a practical problem: real-world socio-technical decision-making systems are complex dynamic entities involving various actors (end users, domain experts, civil servants, system owners, etc.) engaged in social and technical processes. Thus, research needs to account for the specificities of systems where it would be applied. To evaluate how authors envision AR \""in the wild\"", we carry out a systematized review of 127 publications pertaining to the problem and identify the real-world considerations that motivate them. Among others, we look at the ways to make recourse (individually) actionable, the involved stakeholders, the perceived challenges, and the availability of practitioner-friendly open-source codebases. We find that there is a strong disconnect between the existing research and the practical requirements for AR. Most importantly, the grounding and validation of algorithmic recourse in real-world contexts remain underexplored. As an attempt to bridge this gap, we provide other authors with five recommendations to make future solutions easier to adapt to their potential real-world applications.\n\nTLDR:We carry out a systematized literature review of algorithmic recourse and find strong disconnects between current approaches to the problem and requirements in realistic applications\n\nKeywords:Algorithmic Recourse, Counterfactual Explanations, Explainable AI, Real-world Systems"", ""claims"": [""Claim1: We find that there is a strong disconnect between the existing research and the practical requirements for AR."", ""Claim2: Most importantly, the grounding and validation of algorithmic recourse in real-world contexts remain underexplored."", ""Claim3: Deploying AR in realistic systems without analyzing its mechanics in a broader context and without knowing what types of dynamics are expected to arise is bound to lead to unanticipated outcomes."", ""Claim4: Many of them will be undesirable and even potentially unsafe, and impossible to validate with respect to a set of requirements because the requirements for AR are necessarily socio-technical."", ""Claim5: Societal and institutional components of algorithmic recourse are the focal point of our work, as we look beyond the typical technical considerations to assess the practical aspects of the problem."", ""Claim6: To that end, we contribute a systematized review of 127 publications that address the goals of algorithmic recourse and we evaluate to what extent they incorporate such practical considerations."", ""Claim7: Based on our analysis, we also provide other authors with five recommendations on how to improve the practicality of AR research."", ""Claim8: Existing research has generally considered AR in simplistic settings that are far removed from real-world socio-technical decision-making systems, where it would be implemented as a process."", ""Claim9: Several groups of authors have previously surveyed the landscape of counterfactual explanations in general, and algorithmic recourse specifically."", ""Claim10: We observe that some important publications may be missing from our results."", ""Claim11: The most common theme is overturning undesirable decisions, present in 47 definitions (63.5% of all definitions), but specifically overturning algorithmic decisions is mentioned only 43 (58.1%) times."", ""Claim12: In general, end users subject to algorithmic decisions are envisioned to be the recipients of AR, but this is not always the case: it may also be provided to experts or organizations, which highlights that in some cases AR may be carried out on behalf of the affected individuals."", ""Claim13: With the multitude of challenges that stand ahead of real-world AR, we are interested in the considerations that motivate existing work."", ""Claim14: We find that there is a strong disconnect between the existing research and the practical requirements for AR."", ""Claim15: Most importantly, the grounding and validation of algorithmic recourse in real-world contexts remain underexplored."", ""Claim16: Among the remaining 26 documents we find considerations for several different group-level effects."", ""Claim17: We find two further similar perspectives on recourse: [38] proposes auditing and subsidies to minimize the risks of strategic behaviors in a multi-agent setting, while [136] attempts to incentivize actual improvement for a population of agents."", ""Claim18: Finally, [65] provides a framework that generates transparent and consistent recourses for a sub-population."", ""Claim19: Finally, we note that the lack of availability of well-documented open-source code may be an important obstacle to the application of AR in real-world systems."", ""Claim20: Ultimately, we collect open-source implementations for 64 (55.2%) publications."", ""Claim21: It is perhaps most telling that only 12% of surveyed publications attempt to apply recourse in realistic settings."", ""Claim22: For AR to be truly useful, it must be able to satisfy the preferences of its end users."", ""Claim23: Research is also necessary to learn about the needs of the affected individuals concerning recourse, and to validate its potential contributions and inherent limitations."", ""Claim24: Although a socio-technical perspective on AR brings its own challenges, such as accounting for the roles of stakeholders involved in the provision of recourse, it creates important opportunities."", ""Claim25: Research on AR should, from the onset, explore group-level effects such as external costs or fairness."", ""Claim26: Finally, the artifacts of AR research should be practitioner-friendly."", ""Claim27: Existing research on the topic is predominantly theoretical, even though recourse, in expectation, is a real-world problem with strong practical implications."", ""Claim28: Our findings indicate that, indeed, AR tends to be perceived as a (predominantly) technical problem."", ""Claim29: Although we think highly of fundamental research, we note that for algorithmic recourse to leave computer science labs, it must be more strongly grounded and validated in the real world, and consider the requirements for systems that include not only technical but also social and institutional components."", ""Claim30: Our claim is supported by a systematized literature review which is the contribution of this work (Section 3)."", ""Claim31: positive and negative societal impacts of our work (and algorithmic recourse in general) are a key consideration throughout this paper.""], ""review"": ""Review: Weakness: Overall, I do think the current draft may not achieve the full impact that a future revision could provide.\n\nThe current discussion section feels like it largely echoes other calls in the community to apply systems thinking to ethical/responsible/pro-social AI/ML initiatives, and while each of the suggestions has some connection to one of the analyses, the current draft is not totally clear about the extent to which these recommendations stem from the findings vs. are motivated by first principles. The paper is overall very critical of the AR field, i.e. \""Why hasn't this field engagement with any real world deployments\"". However, it's also not entirely clear in the current draft how any of the general recommendations would be applied in specific AR domains.\n\nOne aspect of the paper that I think would have been most directly helpful to the NeurIPS audience in particular would be to take a stance on how recourse should be defined -- is the definition on line 62 \""endorsed\"" by the paper? Is the \""imagine a counterfactual input x*\"" an advisable approach to take for future work. Does this review support the definition, highlight core definitional or epistemic issues, etc.\n\nUltimately, given the intended goals of this draft, it seems success and impact (on top of the core empirical contribution provided by writing a systematized review) here are dependent on the ability for the provided recommendations to shape future research positively. While the five recommendations here could have a some positive impact, taking a stronger stance on the core definitions and framing of recourse \""tasks\"" could have an even larger impact.\nQuestions: The primary questions that I would pose to the authors would be:\n- Is it possible to use the current \""data\"" (i.e. selected papers) to provide more actionable domain-specific recommendations and/or pragmatic guidance about which contexts are likely to see real-world engagement with AR?\n- What is needed to get organizations that develop and/or operate algorithmic systems to engage with recourse? Are there circumstances that the current literature treats as \""futile\""?\n\nI think using the data that's already been collected and analyzed, focusing on domain-specificity of recommendations could go very far in strengthening the draft.\n\n""}, ""review"": ""Title: Grounding and Validation of Algorithmic Recourse in Real-World Contexts: A Systematized Literature Review\nAbstract: The aim of algorithmic recourse (AR) is generally understood to be the provision of \""actionable\"" recommendations to individuals affected by algorithmic decision-making systems, in an attempt to offer the capacity for taking actions that may lead to more desirable outcomes in the future. Over the past few years, AR literature has largely focused on theoretical frameworks to generate \""actionable\"" counterfactual explanations that further satisfy various desiderata, such as diversity or robustness. We believe that algorithmic recourse, by its nature, should be seen as a practical problem: real-world socio-technical decision-making systems are complex dynamic entities involving various actors (end users, domain experts, civil servants, system owners, etc.) engaged in social and technical processes. Thus, research needs to account for the specificities of systems where it would be applied. To evaluate how authors envision AR \""in the wild\"", we carry out a systematized review of 127 publications pertaining to the problem and identify the real-world considerations that motivate them. Among others, we look at the ways to make recourse (individually) actionable, the involved stakeholders, the perceived challenges, and the availability of practitioner-friendly open-source codebases. We find that there is a strong disconnect between the existing research and the practical requirements for AR. Most importantly, the grounding and validation of algorithmic recourse in real-world contexts remain underexplored. As an attempt to bridge this gap, we provide other authors with five recommendations to make future solutions easier to adapt to their potential real-world applications.\n\nClaim1: We find that there is a strong disconnect between the existing research and the practical requirements for AR.\nClaim2: Most importantly, the grounding and validation of algorithmic recourse in real-world contexts remain underexplored.\nClaim3: Deploying AR in realistic systems without analyzing its mechanics in a broader context and without knowing what types of dynamics are expected to arise is bound to lead to unanticipated outcomes.\nClaim4: Many of them will be undesirable and even potentially unsafe, and impossible to validate with respect to a set of requirements because the requirements for AR are necessarily socio-technical.\nClaim5: Societal and institutional components of algorithmic recourse are the focal point of our work, as we look beyond the typical technical considerations to assess the practical aspects of the problem.\nClaim6: To that end, we contribute a systematized review of 127 publications that address the goals of algorithmic recourse and we evaluate to what extent they incorporate such practical considerations.\nClaim7: Based on our analysis, we also provide other authors with five recommendations on how to improve the practicality of AR research.\nClaim8: Existing research has generally considered AR in simplistic settings that are far removed from real-world socio-technical decision-making systems, where it would be implemented as a process.\nClaim9: Several groups of authors have previously surveyed the landscape of counterfactual explanations in general, and algorithmic recourse specifically.\nClaim10: We observe that some important publications may be missing from our results.\nClaim11: The most common theme is overturning undesirable decisions, present in 47 definitions (63.5% of all definitions), but specifically overturning algorithmic decisions is mentioned only 43 (58.1%) times.\nClaim12: In general, end users subject to algorithmic decisions are envisioned to be the recipients of AR, but this is not always the case: it may also be provided to experts or organizations, which highlights that in some cases AR may be carried out on behalf of the affected individuals.\nClaim13: With the multitude of challenges that stand ahead of real-world AR, we are interested in the considerations that motivate existing work.\nClaim14: We find that there is a strong disconnect between the existing research and the practical requirements for AR.\nClaim15: Most importantly, the grounding and validation of algorithmic recourse in real-world contexts remain underexplored.\nClaim16: Among the remaining 26 documents we find considerations for several different group-level effects.\nClaim17: We find two further similar perspectives on recourse: [38] proposes auditing and subsidies to minimize the risks of strategic behaviors in a multi-agent setting, while [136] attempts to incentivize actual improvement for a population of agents.\nClaim18: Finally, [65] provides a framework that generates transparent and consistent recourses for a sub-population.\nClaim19: Finally, we note that the lack of availability of well-documented open-source code may be an important obstacle to the application of AR in real-world systems.\nClaim20: Ultimately, we collect open-source implementations for 64 (55.2%) publications.\nClaim21: It is perhaps most telling that only 12% of surveyed publications attempt to apply recourse in realistic settings.\nClaim22: For AR to be truly useful, it must be able to satisfy the preferences of its end users.\nClaim23: Research is also necessary to learn about the needs of the affected individuals concerning recourse, and to validate its potential contributions and inherent limitations.\nClaim24: Although a socio-technical perspective on AR brings its own challenges, such as accounting for the roles of stakeholders involved in the provision of recourse, it creates important opportunities.\nClaim25: Research on AR should, from the onset, explore group-level effects such as external costs or fairness.\nClaim26: Finally, the artifacts of AR research should be practitioner-friendly.\nClaim27: Existing research on the topic is predominantly theoretical, even though recourse, in expectation, is a real-world problem with strong practical implications.\nClaim28: Our findings indicate that, indeed, AR tends to be perceived as a (predominantly) technical problem.\nClaim29: Although we think highly of fundamental research, we note that for algorithmic recourse to leave computer science labs, it must be more strongly grounded and validated in the real world, and consider the requirements for systems that include not only technical but also social and institutional components.\nClaim30: Our claim is supported by a systematized literature review which is the contribution of this work (Section 3).\nClaim31: positive and negative societal impacts of our work (and algorithmic recourse in general) are a key consideration throughout this paper.\n\nReview: Weakness: Overall, I do think the current draft may not achieve the full impact that a future revision could provide.\n\nThe current discussion section feels like it largely echoes other calls in the community to apply systems thinking to ethical/responsible/pro-social AI/ML initiatives, and while each of the suggestions has some connection to one of the analyses, the current draft is not totally clear about the extent to which these recommendations stem from the findings vs. are motivated by first principles. The paper is overall very critical of the AR field, i.e. \""Why hasn't this field engagement with any real world deployments\"". However, it's also not entirely clear in the current draft how any of the general recommendations would be applied in specific AR domains.\n\nOne aspect of the paper that I think would have been most directly helpful to the NeurIPS audience in particular would be to take a stance on how recourse should be defined -- is the definition on line 62 \""endorsed\"" by the paper? Is the \""imagine a counterfactual input x*\"" an advisable approach to take for future work. Does this review support the definition, highlight core definitional or epistemic issues, etc.\n\nUltimately, given the intended goals of this draft, it seems success and impact (on top of the core empirical contribution provided by writing a systematized review) here are dependent on the ability for the provided recommendations to shape future research positively. While the five recommendations here could have a some positive impact, taking a stronger stance on the core definitions and framing of recourse \""tasks\"" could have an even larger impact.\nQuestions: The primary questions that I would pose to the authors would be:\n- Is it possible to use the current \""data\"" (i.e. selected papers) to provide more actionable domain-specific recommendations and/or pragmatic guidance about which contexts are likely to see real-world engagement with AR?\n- What is needed to get organizations that develop and/or operate algorithmic systems to engage with recourse? Are there circumstances that the current literature treats as \""futile\""?\n\nI think using the data that's already been collected and analyzed, focusing on domain-specificity of recommendations could go very far in strengthening the draft.\n\n\n\n"", ""id"": ""oEmyoy5H5P0"", ""pdf"": ""openreview.net/pdf?id=oEmyoy5H5P""}","41"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""While I agree with the points being made and appreciate the findings in the paper, I question their novelty. As mentioned in Section 4.6, there are papers (albeit in smaller numbers than we would want) that already provide real-world examples and attempt to discuss ethics within recourse. Previous work by [Doshi-Velez and Kim], [Vaughan and Wallach] have called for more user studies in interpretable ML, which resulted in studies like [Sixt et al.]. Considering that many researchers working on recourse is also in the field of ML interpretability, I am not sure if the paper's results and call for more user studies are very substantive."", ""Target claims"": [1, 2, 5, 7, 27, 29], ""Reasoning"": ""The reviewer questions the novelty of the findings and calls to action presented in the paper, noting that other works have already provided real-world examples and discussed ethics associated with recourse. This directly challenges Claim 1 ('strong disconnect between existing research and practical requirements'), Claim 2 ('underexplored validation of recourse in real-world contexts'), Claim 5 (societal/institutional components focus), Claim 7 (providing recommendations to improve practicality), Claim 27 ('existing research predominantly theoretical'), and Claim 29 ('recourse must be more grounded in the real world'). The reviewer implies that these claims might not significantly advance the current state of research.""}]}, ""meta"": {""id"": ""oEmyoy5H5P"", ""review_idx"": 1, ""title"": ""Title: Grounding and Validation of Algorithmic Recourse in Real-World Contexts: A Systematized Literature Review\nAbstract: The aim of algorithmic recourse (AR) is generally understood to be the provision of \""actionable\"" recommendations to individuals affected by algorithmic decision-making systems, in an attempt to offer the capacity for taking actions that may lead to more desirable outcomes in the future. Over the past few years, AR literature has largely focused on theoretical frameworks to generate \""actionable\"" counterfactual explanations that further satisfy various desiderata, such as diversity or robustness. We believe that algorithmic recourse, by its nature, should be seen as a practical problem: real-world socio-technical decision-making systems are complex dynamic entities involving various actors (end users, domain experts, civil servants, system owners, etc.) engaged in social and technical processes. Thus, research needs to account for the specificities of systems where it would be applied. To evaluate how authors envision AR \""in the wild\"", we carry out a systematized review of 127 publications pertaining to the problem and identify the real-world considerations that motivate them. Among others, we look at the ways to make recourse (individually) actionable, the involved stakeholders, the perceived challenges, and the availability of practitioner-friendly open-source codebases. We find that there is a strong disconnect between the existing research and the practical requirements for AR. Most importantly, the grounding and validation of algorithmic recourse in real-world contexts remain underexplored. As an attempt to bridge this gap, we provide other authors with five recommendations to make future solutions easier to adapt to their potential real-world applications.\n\nTLDR:We carry out a systematized literature review of algorithmic recourse and find strong disconnects between current approaches to the problem and requirements in realistic applications\n\nKeywords:Algorithmic Recourse, Counterfactual Explanations, Explainable AI, Real-world Systems"", ""claims"": [""Claim1: We find that there is a strong disconnect between the existing research and the practical requirements for AR."", ""Claim2: Most importantly, the grounding and validation of algorithmic recourse in real-world contexts remain underexplored."", ""Claim3: Deploying AR in realistic systems without analyzing its mechanics in a broader context and without knowing what types of dynamics are expected to arise is bound to lead to unanticipated outcomes."", ""Claim4: Many of them will be undesirable and even potentially unsafe, and impossible to validate with respect to a set of requirements because the requirements for AR are necessarily socio-technical."", ""Claim5: Societal and institutional components of algorithmic recourse are the focal point of our work, as we look beyond the typical technical considerations to assess the practical aspects of the problem."", ""Claim6: To that end, we contribute a systematized review of 127 publications that address the goals of algorithmic recourse and we evaluate to what extent they incorporate such practical considerations."", ""Claim7: Based on our analysis, we also provide other authors with five recommendations on how to improve the practicality of AR research."", ""Claim8: Existing research has generally considered AR in simplistic settings that are far removed from real-world socio-technical decision-making systems, where it would be implemented as a process."", ""Claim9: Several groups of authors have previously surveyed the landscape of counterfactual explanations in general, and algorithmic recourse specifically."", ""Claim10: We observe that some important publications may be missing from our results."", ""Claim11: The most common theme is overturning undesirable decisions, present in 47 definitions (63.5% of all definitions), but specifically overturning algorithmic decisions is mentioned only 43 (58.1%) times."", ""Claim12: In general, end users subject to algorithmic decisions are envisioned to be the recipients of AR, but this is not always the case: it may also be provided to experts or organizations, which highlights that in some cases AR may be carried out on behalf of the affected individuals."", ""Claim13: With the multitude of challenges that stand ahead of real-world AR, we are interested in the considerations that motivate existing work."", ""Claim14: We find that there is a strong disconnect between the existing research and the practical requirements for AR."", ""Claim15: Most importantly, the grounding and validation of algorithmic recourse in real-world contexts remain underexplored."", ""Claim16: Among the remaining 26 documents we find considerations for several different group-level effects."", ""Claim17: We find two further similar perspectives on recourse: [38] proposes auditing and subsidies to minimize the risks of strategic behaviors in a multi-agent setting, while [136] attempts to incentivize actual improvement for a population of agents."", ""Claim18: Finally, [65] provides a framework that generates transparent and consistent recourses for a sub-population."", ""Claim19: Finally, we note that the lack of availability of well-documented open-source code may be an important obstacle to the application of AR in real-world systems."", ""Claim20: Ultimately, we collect open-source implementations for 64 (55.2%) publications."", ""Claim21: It is perhaps most telling that only 12% of surveyed publications attempt to apply recourse in realistic settings."", ""Claim22: For AR to be truly useful, it must be able to satisfy the preferences of its end users."", ""Claim23: Research is also necessary to learn about the needs of the affected individuals concerning recourse, and to validate its potential contributions and inherent limitations."", ""Claim24: Although a socio-technical perspective on AR brings its own challenges, such as accounting for the roles of stakeholders involved in the provision of recourse, it creates important opportunities."", ""Claim25: Research on AR should, from the onset, explore group-level effects such as external costs or fairness."", ""Claim26: Finally, the artifacts of AR research should be practitioner-friendly."", ""Claim27: Existing research on the topic is predominantly theoretical, even though recourse, in expectation, is a real-world problem with strong practical implications."", ""Claim28: Our findings indicate that, indeed, AR tends to be perceived as a (predominantly) technical problem."", ""Claim29: Although we think highly of fundamental research, we note that for algorithmic recourse to leave computer science labs, it must be more strongly grounded and validated in the real world, and consider the requirements for systems that include not only technical but also social and institutional components."", ""Claim30: Our claim is supported by a systematized literature review which is the contribution of this work (Section 3)."", ""Claim31: positive and negative societal impacts of our work (and algorithmic recourse in general) are a key consideration throughout this paper.""], ""review"": ""Review: Weakness: While I agree with the points being made and appreciate the findings in the paper, I question their novelty. As mentioned in Section 4.6, there are papers (albeit in smaller numbers than we would want) that already provide real-world examples and attempt to discuss ethics within recourse. Previous work by [Doshi-Velez and Kim](https://arxiv.org/pdf/1702.08608), [Vaughan and Wallach](https://www.jennwv.com/papers/intel-chapter.pdf) have called for more user studies in interpretable ML, which resulted in studies like [Sixt et al.](https://openreview.net/pdf?id=v6s3HVjPerv). Considering that many researchers working on recourse is also in the field of ML interpretability, I am not sure if the paper's results and call for more user studies are very substantive.\n\nSpending more time differentiating this work from other related works (especially other literature reviews like [70]) rather than listing their contributions in Section 2.2 may be helpful in making your case.\n\nA more thorough discussion and evaluation of results (attempted in Section 5) may resolve some of these questions. As it stands, there is a disconnect between Section 4 and 5. The message of the first part of Section 5 (lines 318-350) is not clear. The second paragraph of the section does not seem to be a discussion of survey results but rather an argument the authors are trying to make (without using the results). The paper would benefit from expanding on the contents in lines 352 to 356, pointing to results in Section 4 and bridging them to the suggestions in Section 5.1.\n\nThe paper reads more like a position paper, trying to convince researchers in algorithmic recourse to not only focus on technical methods (which, again, I agree with). But I am not sure if a literature review or a position paper suitable for NeurIPS, considering its call for papers, does not seem to suggest so.\nQuestions: - What was the motivation for this literature review?\n- Why do you say that recourse should be aligned with the *preferences* of the end-users?\n\n""}, ""review"": ""Title: Grounding and Validation of Algorithmic Recourse in Real-World Contexts: A Systematized Literature Review\nAbstract: The aim of algorithmic recourse (AR) is generally understood to be the provision of \""actionable\"" recommendations to individuals affected by algorithmic decision-making systems, in an attempt to offer the capacity for taking actions that may lead to more desirable outcomes in the future. Over the past few years, AR literature has largely focused on theoretical frameworks to generate \""actionable\"" counterfactual explanations that further satisfy various desiderata, such as diversity or robustness. We believe that algorithmic recourse, by its nature, should be seen as a practical problem: real-world socio-technical decision-making systems are complex dynamic entities involving various actors (end users, domain experts, civil servants, system owners, etc.) engaged in social and technical processes. Thus, research needs to account for the specificities of systems where it would be applied. To evaluate how authors envision AR \""in the wild\"", we carry out a systematized review of 127 publications pertaining to the problem and identify the real-world considerations that motivate them. Among others, we look at the ways to make recourse (individually) actionable, the involved stakeholders, the perceived challenges, and the availability of practitioner-friendly open-source codebases. We find that there is a strong disconnect between the existing research and the practical requirements for AR. Most importantly, the grounding and validation of algorithmic recourse in real-world contexts remain underexplored. As an attempt to bridge this gap, we provide other authors with five recommendations to make future solutions easier to adapt to their potential real-world applications.\n\nClaim1: We find that there is a strong disconnect between the existing research and the practical requirements for AR.\nClaim2: Most importantly, the grounding and validation of algorithmic recourse in real-world contexts remain underexplored.\nClaim3: Deploying AR in realistic systems without analyzing its mechanics in a broader context and without knowing what types of dynamics are expected to arise is bound to lead to unanticipated outcomes.\nClaim4: Many of them will be undesirable and even potentially unsafe, and impossible to validate with respect to a set of requirements because the requirements for AR are necessarily socio-technical.\nClaim5: Societal and institutional components of algorithmic recourse are the focal point of our work, as we look beyond the typical technical considerations to assess the practical aspects of the problem.\nClaim6: To that end, we contribute a systematized review of 127 publications that address the goals of algorithmic recourse and we evaluate to what extent they incorporate such practical considerations.\nClaim7: Based on our analysis, we also provide other authors with five recommendations on how to improve the practicality of AR research.\nClaim8: Existing research has generally considered AR in simplistic settings that are far removed from real-world socio-technical decision-making systems, where it would be implemented as a process.\nClaim9: Several groups of authors have previously surveyed the landscape of counterfactual explanations in general, and algorithmic recourse specifically.\nClaim10: We observe that some important publications may be missing from our results.\nClaim11: The most common theme is overturning undesirable decisions, present in 47 definitions (63.5% of all definitions), but specifically overturning algorithmic decisions is mentioned only 43 (58.1%) times.\nClaim12: In general, end users subject to algorithmic decisions are envisioned to be the recipients of AR, but this is not always the case: it may also be provided to experts or organizations, which highlights that in some cases AR may be carried out on behalf of the affected individuals.\nClaim13: With the multitude of challenges that stand ahead of real-world AR, we are interested in the considerations that motivate existing work.\nClaim14: We find that there is a strong disconnect between the existing research and the practical requirements for AR.\nClaim15: Most importantly, the grounding and validation of algorithmic recourse in real-world contexts remain underexplored.\nClaim16: Among the remaining 26 documents we find considerations for several different group-level effects.\nClaim17: We find two further similar perspectives on recourse: [38] proposes auditing and subsidies to minimize the risks of strategic behaviors in a multi-agent setting, while [136] attempts to incentivize actual improvement for a population of agents.\nClaim18: Finally, [65] provides a framework that generates transparent and consistent recourses for a sub-population.\nClaim19: Finally, we note that the lack of availability of well-documented open-source code may be an important obstacle to the application of AR in real-world systems.\nClaim20: Ultimately, we collect open-source implementations for 64 (55.2%) publications.\nClaim21: It is perhaps most telling that only 12% of surveyed publications attempt to apply recourse in realistic settings.\nClaim22: For AR to be truly useful, it must be able to satisfy the preferences of its end users.\nClaim23: Research is also necessary to learn about the needs of the affected individuals concerning recourse, and to validate its potential contributions and inherent limitations.\nClaim24: Although a socio-technical perspective on AR brings its own challenges, such as accounting for the roles of stakeholders involved in the provision of recourse, it creates important opportunities.\nClaim25: Research on AR should, from the onset, explore group-level effects such as external costs or fairness.\nClaim26: Finally, the artifacts of AR research should be practitioner-friendly.\nClaim27: Existing research on the topic is predominantly theoretical, even though recourse, in expectation, is a real-world problem with strong practical implications.\nClaim28: Our findings indicate that, indeed, AR tends to be perceived as a (predominantly) technical problem.\nClaim29: Although we think highly of fundamental research, we note that for algorithmic recourse to leave computer science labs, it must be more strongly grounded and validated in the real world, and consider the requirements for systems that include not only technical but also social and institutional components.\nClaim30: Our claim is supported by a systematized literature review which is the contribution of this work (Section 3).\nClaim31: positive and negative societal impacts of our work (and algorithmic recourse in general) are a key consideration throughout this paper.\n\nReview: Weakness: While I agree with the points being made and appreciate the findings in the paper, I question their novelty. As mentioned in Section 4.6, there are papers (albeit in smaller numbers than we would want) that already provide real-world examples and attempt to discuss ethics within recourse. Previous work by [Doshi-Velez and Kim](https://arxiv.org/pdf/1702.08608), [Vaughan and Wallach](https://www.jennwv.com/papers/intel-chapter.pdf) have called for more user studies in interpretable ML, which resulted in studies like [Sixt et al.](https://openreview.net/pdf?id=v6s3HVjPerv). Considering that many researchers working on recourse is also in the field of ML interpretability, I am not sure if the paper's results and call for more user studies are very substantive.\n\nSpending more time differentiating this work from other related works (especially other literature reviews like [70]) rather than listing their contributions in Section 2.2 may be helpful in making your case.\n\nA more thorough discussion and evaluation of results (attempted in Section 5) may resolve some of these questions. As it stands, there is a disconnect between Section 4 and 5. The message of the first part of Section 5 (lines 318-350) is not clear. The second paragraph of the section does not seem to be a discussion of survey results but rather an argument the authors are trying to make (without using the results). The paper would benefit from expanding on the contents in lines 352 to 356, pointing to results in Section 4 and bridging them to the suggestions in Section 5.1.\n\nThe paper reads more like a position paper, trying to convince researchers in algorithmic recourse to not only focus on technical methods (which, again, I agree with). But I am not sure if a literature review or a position paper suitable for NeurIPS, considering its call for papers, does not seem to suggest so.\nQuestions: - What was the motivation for this literature review?\n- Why do you say that recourse should be aligned with the *preferences* of the end-users?\n\n\n\n"", ""id"": ""oEmyoy5H5P1"", ""pdf"": ""openreview.net/pdf?id=oEmyoy5H5P""}","42"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""Accuracy of the answers is not the only important requirement in generating SQL from text. Database users also expect query generation to be time-efficient. The proposed workflow includes several computationally intensive steps: first, solving an NP-complete problem (CSP), and second, creating a potentially enormous materialized view by joining many tables."", ""Target claims"": [1, 6], ""Reasoning"": ""This span discusses the computational efficiency issues concerning the proposed workflow. Claim1 involves the proposal of a new solution combining LLMs with reasoning techniques to handle complex database constraints, which includes computational steps. Claim6 mentions the separation of tasks focusing on NLP and constraint reasoning subproblems, likely implying computational demands. The weakness questions whether this workflow can meet the practical requirements for efficiency, suggesting that the claims about the method's capability to manage complexity (Claim1) and reasoning task separation (Claim6) may overlook computational inefficiency.""}, {""Weakness span"": ""The authors claim that their approach guarantees the generated query respects database constraints, but this guarantee is not clearly defined."", ""Target claims"": [1], ""Reasoning"": ""This statement refers directly to Claim1, which discusses handling complex database constraints. The reviewer questions the clarity of how these constraints are respected, implying that the claim may not be fully substantiated.""}, {""Weakness span"": ""The exact SQL fragment covered by this approach is also unclear. While the limitations section mentions that queries requiring the union operator are not supported, it is unclear if other standard SQL constructs are also unsupported."", ""Target claims"": [1, 7], ""Reasoning"": ""Claim1 mentions the handling of complex database constraints, and Claim7 specifically talks about supporting complex queries, including those with multiple filtering operators and aggregators. The reviewer points out a lack of clarity on the SQL fragments that the approach supports, directly challenging the completeness of the handling of SQL queries as claimed.""}]}, ""meta"": {""id"": ""lEDuaGGiCV"", ""review_idx"": 0, ""title"": ""Title: Lucy: Think and Reason to Solve Text-to-SQL\nAbstract: Large Language Models (LLMs) have made significant progress in assisting users to query databases in natural language. While LLM-based techniques provide state-of-the-art results on many standard benchmarks, their performance significantly drops when applied to large enterprise databases. The reason is that these databases have a large number of tables with complex relationships that are challenging for LLMs to reason about. We analyze challenges that LLMs face in these settings and propose a new solution that combines the power of LLMs in understanding questions with automated reasoning techniques to handle complex database constraints. Based on these ideas, we have developed a new framework that outperforms state-of-the-art techniques in zero-shot text-to-SQL on complex benchmarks\n\nTLDR:Integrating language understanding and automated reasoning for solving Text-to-SQL\n\nKeywords:Large Language Models, Automated Reasoning, Text-to-SQL"", ""claims"": [""Claim1: We analyze challenges that LLMs face in these settings and propose a new solution that combines the power of LLMs in understanding questions with automated reasoning techniques to handle complex database constraints."", ""Claim2: Based on these ideas, we have developed a new framework that outperforms state-of-the-art techniques in zero-shot text-to-SQL on complex benchmarks"", ""Claim3: Recently, LLM -based solutions have made significant progress in addressing the text-to-SQL problem [Gao et al., 2024, Li et al., 2024a]."", ""Claim4: While GPT-based methods have quickly reached near-human performance on academic benchmarks, like Spider [Yu et al., 2018], they struggle to provide high-quality user assistance on large industrial databases [Sequeda et al., 2023, Li et al., 2023]."", ""Claim5: In this work, we propose a new text-to-SQL framework, Lucy , designed for large databases with complex relationships between objects."", ""Claim6: Lucy is designed based on the principle of separation of responsibilities between generative tasks and automated reasoning tasks: each step focuses on either an NLP-related subproblem or a constraint reasoning subproblem."", ""Claim7: Lucy can support complex queries that include multiple filtering operators and aggregators, e.g. average or sum."", ""Claim8: Lucy shows very high coverage and solves about 30 of benchmarks according to the ex metric, which outperforms dw that solves 24 and other methods."", ""Claim9: Lucy and c2q have higher coverage and good accuracy. Lucy shows the best results in most cases."", ""Claim10: Lucy outperforms other models in both coverage and accuracy."", ""Claim11: We focus on the zero-shot text-to-SQL problem, which has received significant attention in the last few years."", ""Claim12: Liu et al. [2023] performed a comprehensive evaluation of ChatGPT on the Spider dataset and demonstrated that it shows good performance."", ""Claim13: [Dong et al., 2023], a new framework based on the GPT model was proposed, involving several techniques for promoting and post-processing the output to get more consistent results."", ""Claim14: Chang and Fosler-Lussier [2023] proposed several techniques to improve the performance of ChatGPT."", ""Claim15: According to the API documentation [TiDBCloud, 2020b], the authors construct a data summary object that contains \u2018AI exploration information of the given database.\u2019 This method performs very well on the BIRD dataset."", ""Claim16: Sequeda et al. [2023] performed an interesting investigation of the performance of LLM s on large industrial databases."", ""Claim17: They identified that GPT does not perform well when it needs to reason about complex relationships."", ""Claim18: The authors proposed a two-step approach to tackle this problem."", ""Claim19: We noticed that better results are obtained if we provide a description of tables that are used to generate this view together with their relevant attributes."", ""Claim20: We run experiments on a laptop Intel(r) Core 2.40Hz and 32GB of memory."", ""Claim21: We provide all benchmarks and their results in the supplementary materials."", ""Claim22: As the authors are working on corrections, we analyzed the reported issues and manually corrected the ground truth."", ""Claim23: We only adjusted the SQL ground truth values to match the user questions; we did not alter the questions or evidences."", ""Claim24: Interestingly, the performance of all frameworks improved by a few percentage points when we fixed these ground truth SQL statements."", ""Claim25: We discuss three major groups of challenging questions with examples."", ""Claim26: Note on the cost of running. One note here is that GPT and c2q models are costly to run. For example, in the Cloud Resources experiment, the costs are as follows: c2q costs $15, gpt4 $2, and gpt4ex $5, while Lucy costs $0.5.""], ""review"": ""Review: Weakness: The paper lacks depth, and the writing does not, in my opinion, meet the quality standards required for a venue like NeurIPS. Additionally, several potential limitations of the proposed workflow are not discussed.\n\t\u2022\tAccuracy of the answers is not the only important requirement in generating SQL from text. Database users also expect query generation to be time-efficient. The proposed workflow includes several computationally intensive steps: first, solving an NP-complete problem (CSP), and second, creating a potentially enormous materialized view by joining many tables. I would have expected a discussion on the computational limitations of this approach.\n\t\u2022\tThe workflow lacks sufficient precision and clarity. For example, it is unclear whether the final query is expressed with respect to the materialized view as the only table or with respect to the original schema. Additionally, how lookup tables and various schema design patterns are identified in the input database is not well-explained. The authors claim that their approach guarantees the generated query respects database constraints, but this guarantee is not clearly defined. Algorithm 1 is underspecified; at this high level of detail, the algorithm seems redundant and could be subsumed by the text description. The exact SQL fragment covered by this approach is also unclear. While the limitations section mentions that queries requiring the union operator are not supported, it is unclear if other standard SQL constructs are also unsupported.\n\t\u2022\tWhile relevant related work is cited, the main body of the paper lacks a detailed discussion on the contribution in relation to recent approaches.\n\t\u2022\tThe evaluation section is somewhat lacking. The tables are confusing, and it is unclear what each of the rows actually represents.\nQuestions: Please comment on the limitations concerning unsupported SQL constructs as well as on the computational limitations of the proposed approach.\n\n""}, ""review"": ""Title: Lucy: Think and Reason to Solve Text-to-SQL\nAbstract: Large Language Models (LLMs) have made significant progress in assisting users to query databases in natural language. While LLM-based techniques provide state-of-the-art results on many standard benchmarks, their performance significantly drops when applied to large enterprise databases. The reason is that these databases have a large number of tables with complex relationships that are challenging for LLMs to reason about. We analyze challenges that LLMs face in these settings and propose a new solution that combines the power of LLMs in understanding questions with automated reasoning techniques to handle complex database constraints. Based on these ideas, we have developed a new framework that outperforms state-of-the-art techniques in zero-shot text-to-SQL on complex benchmarks\n\nClaim1: We analyze challenges that LLMs face in these settings and propose a new solution that combines the power of LLMs in understanding questions with automated reasoning techniques to handle complex database constraints.\nClaim2: Based on these ideas, we have developed a new framework that outperforms state-of-the-art techniques in zero-shot text-to-SQL on complex benchmarks\nClaim3: Recently, LLM -based solutions have made significant progress in addressing the text-to-SQL problem [Gao et al., 2024, Li et al., 2024a].\nClaim4: While GPT-based methods have quickly reached near-human performance on academic benchmarks, like Spider [Yu et al., 2018], they struggle to provide high-quality user assistance on large industrial databases [Sequeda et al., 2023, Li et al., 2023].\nClaim5: In this work, we propose a new text-to-SQL framework, Lucy , designed for large databases with complex relationships between objects.\nClaim6: Lucy is designed based on the principle of separation of responsibilities between generative tasks and automated reasoning tasks: each step focuses on either an NLP-related subproblem or a constraint reasoning subproblem.\nClaim7: Lucy can support complex queries that include multiple filtering operators and aggregators, e.g. average or sum.\nClaim8: Lucy shows very high coverage and solves about 30 of benchmarks according to the ex metric, which outperforms dw that solves 24 and other methods.\nClaim9: Lucy and c2q have higher coverage and good accuracy. Lucy shows the best results in most cases.\nClaim10: Lucy outperforms other models in both coverage and accuracy.\nClaim11: We focus on the zero-shot text-to-SQL problem, which has received significant attention in the last few years.\nClaim12: Liu et al. [2023] performed a comprehensive evaluation of ChatGPT on the Spider dataset and demonstrated that it shows good performance.\nClaim13: [Dong et al., 2023], a new framework based on the GPT model was proposed, involving several techniques for promoting and post-processing the output to get more consistent results.\nClaim14: Chang and Fosler-Lussier [2023] proposed several techniques to improve the performance of ChatGPT.\nClaim15: According to the API documentation [TiDBCloud, 2020b], the authors construct a data summary object that contains \u2018AI exploration information of the given database.\u2019 This method performs very well on the BIRD dataset.\nClaim16: Sequeda et al. [2023] performed an interesting investigation of the performance of LLM s on large industrial databases.\nClaim17: They identified that GPT does not perform well when it needs to reason about complex relationships.\nClaim18: The authors proposed a two-step approach to tackle this problem.\nClaim19: We noticed that better results are obtained if we provide a description of tables that are used to generate this view together with their relevant attributes.\nClaim20: We run experiments on a laptop Intel(r) Core 2.40Hz and 32GB of memory.\nClaim21: We provide all benchmarks and their results in the supplementary materials.\nClaim22: As the authors are working on corrections, we analyzed the reported issues and manually corrected the ground truth.\nClaim23: We only adjusted the SQL ground truth values to match the user questions; we did not alter the questions or evidences.\nClaim24: Interestingly, the performance of all frameworks improved by a few percentage points when we fixed these ground truth SQL statements.\nClaim25: We discuss three major groups of challenging questions with examples.\nClaim26: Note on the cost of running. One note here is that GPT and c2q models are costly to run. For example, in the Cloud Resources experiment, the costs are as follows: c2q costs $15, gpt4 $2, and gpt4ex $5, while Lucy costs $0.5.\n\nReview: Weakness: The paper lacks depth, and the writing does not, in my opinion, meet the quality standards required for a venue like NeurIPS. Additionally, several potential limitations of the proposed workflow are not discussed.\n\t\u2022\tAccuracy of the answers is not the only important requirement in generating SQL from text. Database users also expect query generation to be time-efficient. The proposed workflow includes several computationally intensive steps: first, solving an NP-complete problem (CSP), and second, creating a potentially enormous materialized view by joining many tables. I would have expected a discussion on the computational limitations of this approach.\n\t\u2022\tThe workflow lacks sufficient precision and clarity. For example, it is unclear whether the final query is expressed with respect to the materialized view as the only table or with respect to the original schema. Additionally, how lookup tables and various schema design patterns are identified in the input database is not well-explained. The authors claim that their approach guarantees the generated query respects database constraints, but this guarantee is not clearly defined. Algorithm 1 is underspecified; at this high level of detail, the algorithm seems redundant and could be subsumed by the text description. The exact SQL fragment covered by this approach is also unclear. While the limitations section mentions that queries requiring the union operator are not supported, it is unclear if other standard SQL constructs are also unsupported.\n\t\u2022\tWhile relevant related work is cited, the main body of the paper lacks a detailed discussion on the contribution in relation to recent approaches.\n\t\u2022\tThe evaluation section is somewhat lacking. The tables are confusing, and it is unclear what each of the rows actually represents.\nQuestions: Please comment on the limitations concerning unsupported SQL constructs as well as on the computational limitations of the proposed approach.\n\n\n\n"", ""id"": ""lEDuaGGiCV0"", ""pdf"": ""openreview.net/pdf?id=lEDuaGGiCV""}","43"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""I am not convinced by the motivation of zero-shot text-to-SQL with the example of industrial databases having complex relationships. Text-to-SQL systems deployment in real-industry requires high performance. I doubt that people won't be using zero-shot models for real use applications. In KaggleDBQA, it also states \""we believe the zero-shot setting is overly-restrictive compared to how text-to-SQL systems are likely to be actually used in practice.\"" I would like to hear the authors' thoughts on this."", ""Target claims"": [11], ""Reasoning"": ""This segment expresses skepticism about the practicality and motivation of addressing zero-shot text-to-SQL issues, which is a central part of Claim11. The reviewer doubts the real-world applicability and use cases for zero-shot approaches, directly questioning the focus of Claim11 on this problem.""}, {""Weakness span"": ""The paper does not appear to be well-grounded in text-to-SQL research. For example, one way to handle complex relationships in text-to-SQL using LLMs is through schema linking. However, the paper does not mention this area of research and instead proposes MatchTables, seemingly ignoring the rich literature of text-to-SQL works. Other approaches include least-to-most prompting attempts in text-to-SQL for task decomposition and Natural SQL for intermediate representation (although it does not handle query nesting). Properly discussing these relevant methods of the proposed method will better situate the work."", ""Target claims"": [1, 6], ""Reasoning"": ""This feedback questions Claim1 and Claim6's proposition of new solutions without adequately considering existing approaches, such as schema linking and task decomposition methods. By stating that the paper ignores rich text-to-SQL research literature, the review questions the originality and comprehensiveness of the proposed techniques, thus targeting the novelty and adequacy of the solutions claimed.""}]}, ""meta"": {""id"": ""lEDuaGGiCV"", ""review_idx"": 1, ""title"": ""Title: Lucy: Think and Reason to Solve Text-to-SQL\nAbstract: Large Language Models (LLMs) have made significant progress in assisting users to query databases in natural language. While LLM-based techniques provide state-of-the-art results on many standard benchmarks, their performance significantly drops when applied to large enterprise databases. The reason is that these databases have a large number of tables with complex relationships that are challenging for LLMs to reason about. We analyze challenges that LLMs face in these settings and propose a new solution that combines the power of LLMs in understanding questions with automated reasoning techniques to handle complex database constraints. Based on these ideas, we have developed a new framework that outperforms state-of-the-art techniques in zero-shot text-to-SQL on complex benchmarks\n\nTLDR:Integrating language understanding and automated reasoning for solving Text-to-SQL\n\nKeywords:Large Language Models, Automated Reasoning, Text-to-SQL"", ""claims"": [""Claim1: We analyze challenges that LLMs face in these settings and propose a new solution that combines the power of LLMs in understanding questions with automated reasoning techniques to handle complex database constraints."", ""Claim2: Based on these ideas, we have developed a new framework that outperforms state-of-the-art techniques in zero-shot text-to-SQL on complex benchmarks"", ""Claim3: Recently, LLM -based solutions have made significant progress in addressing the text-to-SQL problem [Gao et al., 2024, Li et al., 2024a]."", ""Claim4: While GPT-based methods have quickly reached near-human performance on academic benchmarks, like Spider [Yu et al., 2018], they struggle to provide high-quality user assistance on large industrial databases [Sequeda et al., 2023, Li et al., 2023]."", ""Claim5: In this work, we propose a new text-to-SQL framework, Lucy , designed for large databases with complex relationships between objects."", ""Claim6: Lucy is designed based on the principle of separation of responsibilities between generative tasks and automated reasoning tasks: each step focuses on either an NLP-related subproblem or a constraint reasoning subproblem."", ""Claim7: Lucy can support complex queries that include multiple filtering operators and aggregators, e.g. average or sum."", ""Claim8: Lucy shows very high coverage and solves about 30 of benchmarks according to the ex metric, which outperforms dw that solves 24 and other methods."", ""Claim9: Lucy and c2q have higher coverage and good accuracy. Lucy shows the best results in most cases."", ""Claim10: Lucy outperforms other models in both coverage and accuracy."", ""Claim11: We focus on the zero-shot text-to-SQL problem, which has received significant attention in the last few years."", ""Claim12: Liu et al. [2023] performed a comprehensive evaluation of ChatGPT on the Spider dataset and demonstrated that it shows good performance."", ""Claim13: [Dong et al., 2023], a new framework based on the GPT model was proposed, involving several techniques for promoting and post-processing the output to get more consistent results."", ""Claim14: Chang and Fosler-Lussier [2023] proposed several techniques to improve the performance of ChatGPT."", ""Claim15: According to the API documentation [TiDBCloud, 2020b], the authors construct a data summary object that contains \u2018AI exploration information of the given database.\u2019 This method performs very well on the BIRD dataset."", ""Claim16: Sequeda et al. [2023] performed an interesting investigation of the performance of LLM s on large industrial databases."", ""Claim17: They identified that GPT does not perform well when it needs to reason about complex relationships."", ""Claim18: The authors proposed a two-step approach to tackle this problem."", ""Claim19: We noticed that better results are obtained if we provide a description of tables that are used to generate this view together with their relevant attributes."", ""Claim20: We run experiments on a laptop Intel(r) Core 2.40Hz and 32GB of memory."", ""Claim21: We provide all benchmarks and their results in the supplementary materials."", ""Claim22: As the authors are working on corrections, we analyzed the reported issues and manually corrected the ground truth."", ""Claim23: We only adjusted the SQL ground truth values to match the user questions; we did not alter the questions or evidences."", ""Claim24: Interestingly, the performance of all frameworks improved by a few percentage points when we fixed these ground truth SQL statements."", ""Claim25: We discuss three major groups of challenging questions with examples."", ""Claim26: Note on the cost of running. One note here is that GPT and c2q models are costly to run. For example, in the Cloud Resources experiment, the costs are as follows: c2q costs $15, gpt4 $2, and gpt4ex $5, while Lucy costs $0.5.""], ""review"": ""Review: Weakness: - I am not convinced by the motivation of zero-shot text-to-SQL with the example of industrial databases having complex relationships. Text-to-SQL systems deployment in real-industry requires high performance. I doubt that people won't be using zero-shot models for real use applications. In KaggleDBQA, it also states \""we believe the zero-shot setting is overly-restrictive compared to how text-to-SQL systems are likely to be actually used in practice.\"" I would like to hear the authors' thoughts on this.\n- The paper does not appear to be well-grounded in text-to-SQL research. For example, one way to handle complex relationships in text-to-SQL using LLMs is through schema linking. However, the paper does not mention this area of research and instead proposes MatchTables, seemingly ignoring the rich literature of text-to-SQL works. Other approaches include least-to-most prompting attempts in text-to-SQL for task decomposition and Natural SQL for intermediate representation (although it does not handle query nesting). Properly discussing these relevant methods of the proposed method will better situate the work.\nQuestions: Please address my above concerns. I am willing to raise the score if convinced.\n\n""}, ""review"": ""Title: Lucy: Think and Reason to Solve Text-to-SQL\nAbstract: Large Language Models (LLMs) have made significant progress in assisting users to query databases in natural language. While LLM-based techniques provide state-of-the-art results on many standard benchmarks, their performance significantly drops when applied to large enterprise databases. The reason is that these databases have a large number of tables with complex relationships that are challenging for LLMs to reason about. We analyze challenges that LLMs face in these settings and propose a new solution that combines the power of LLMs in understanding questions with automated reasoning techniques to handle complex database constraints. Based on these ideas, we have developed a new framework that outperforms state-of-the-art techniques in zero-shot text-to-SQL on complex benchmarks\n\nClaim1: We analyze challenges that LLMs face in these settings and propose a new solution that combines the power of LLMs in understanding questions with automated reasoning techniques to handle complex database constraints.\nClaim2: Based on these ideas, we have developed a new framework that outperforms state-of-the-art techniques in zero-shot text-to-SQL on complex benchmarks\nClaim3: Recently, LLM -based solutions have made significant progress in addressing the text-to-SQL problem [Gao et al., 2024, Li et al., 2024a].\nClaim4: While GPT-based methods have quickly reached near-human performance on academic benchmarks, like Spider [Yu et al., 2018], they struggle to provide high-quality user assistance on large industrial databases [Sequeda et al., 2023, Li et al., 2023].\nClaim5: In this work, we propose a new text-to-SQL framework, Lucy , designed for large databases with complex relationships between objects.\nClaim6: Lucy is designed based on the principle of separation of responsibilities between generative tasks and automated reasoning tasks: each step focuses on either an NLP-related subproblem or a constraint reasoning subproblem.\nClaim7: Lucy can support complex queries that include multiple filtering operators and aggregators, e.g. average or sum.\nClaim8: Lucy shows very high coverage and solves about 30 of benchmarks according to the ex metric, which outperforms dw that solves 24 and other methods.\nClaim9: Lucy and c2q have higher coverage and good accuracy. Lucy shows the best results in most cases.\nClaim10: Lucy outperforms other models in both coverage and accuracy.\nClaim11: We focus on the zero-shot text-to-SQL problem, which has received significant attention in the last few years.\nClaim12: Liu et al. [2023] performed a comprehensive evaluation of ChatGPT on the Spider dataset and demonstrated that it shows good performance.\nClaim13: [Dong et al., 2023], a new framework based on the GPT model was proposed, involving several techniques for promoting and post-processing the output to get more consistent results.\nClaim14: Chang and Fosler-Lussier [2023] proposed several techniques to improve the performance of ChatGPT.\nClaim15: According to the API documentation [TiDBCloud, 2020b], the authors construct a data summary object that contains \u2018AI exploration information of the given database.\u2019 This method performs very well on the BIRD dataset.\nClaim16: Sequeda et al. [2023] performed an interesting investigation of the performance of LLM s on large industrial databases.\nClaim17: They identified that GPT does not perform well when it needs to reason about complex relationships.\nClaim18: The authors proposed a two-step approach to tackle this problem.\nClaim19: We noticed that better results are obtained if we provide a description of tables that are used to generate this view together with their relevant attributes.\nClaim20: We run experiments on a laptop Intel(r) Core 2.40Hz and 32GB of memory.\nClaim21: We provide all benchmarks and their results in the supplementary materials.\nClaim22: As the authors are working on corrections, we analyzed the reported issues and manually corrected the ground truth.\nClaim23: We only adjusted the SQL ground truth values to match the user questions; we did not alter the questions or evidences.\nClaim24: Interestingly, the performance of all frameworks improved by a few percentage points when we fixed these ground truth SQL statements.\nClaim25: We discuss three major groups of challenging questions with examples.\nClaim26: Note on the cost of running. One note here is that GPT and c2q models are costly to run. For example, in the Cloud Resources experiment, the costs are as follows: c2q costs $15, gpt4 $2, and gpt4ex $5, while Lucy costs $0.5.\n\nReview: Weakness: - I am not convinced by the motivation of zero-shot text-to-SQL with the example of industrial databases having complex relationships. Text-to-SQL systems deployment in real-industry requires high performance. I doubt that people won't be using zero-shot models for real use applications. In KaggleDBQA, it also states \""we believe the zero-shot setting is overly-restrictive compared to how text-to-SQL systems are likely to be actually used in practice.\"" I would like to hear the authors' thoughts on this.\n- The paper does not appear to be well-grounded in text-to-SQL research. For example, one way to handle complex relationships in text-to-SQL using LLMs is through schema linking. However, the paper does not mention this area of research and instead proposes MatchTables, seemingly ignoring the rich literature of text-to-SQL works. Other approaches include least-to-most prompting attempts in text-to-SQL for task decomposition and Natural SQL for intermediate representation (although it does not handle query nesting). Properly discussing these relevant methods of the proposed method will better situate the work.\nQuestions: Please address my above concerns. I am willing to raise the score if convinced.\n\n\n\n"", ""id"": ""lEDuaGGiCV1"", ""pdf"": ""openreview.net/pdf?id=lEDuaGGiCV""}","44"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""The empirical studies could be more convincing by following others' evaluation protocols, such as BIRD."", ""Target claims"": [8, 9, 10], ""Reasoning"": ""The review comments that the empirical studies of the paper could be more convincing if they followed established evaluation protocols, suggesting that the claims about Lucy's high coverage and outperformance in benchmarks (Claim8, Claim9, and Claim10) might not be fully supported if alternative evaluation methods were used. This directly questions the truthfulness and robustness of these performance claims.""}, {""Weakness span"": ""Lack of comparison to other competitors."", ""Target claims"": [2, 8, 9, 10], ""Reasoning"": ""The lack of comparisons to other competitors puts into question the claims regarding Lucy's superior performance (Claim2, Claim8, Claim9, Claim10). Without direct comparisons, the claims of Lucy outperforming other techniques might be seen as insufficiently supported.""}]}, ""meta"": {""id"": ""lEDuaGGiCV"", ""review_idx"": 2, ""title"": ""Title: Lucy: Think and Reason to Solve Text-to-SQL\nAbstract: Large Language Models (LLMs) have made significant progress in assisting users to query databases in natural language. While LLM-based techniques provide state-of-the-art results on many standard benchmarks, their performance significantly drops when applied to large enterprise databases. The reason is that these databases have a large number of tables with complex relationships that are challenging for LLMs to reason about. We analyze challenges that LLMs face in these settings and propose a new solution that combines the power of LLMs in understanding questions with automated reasoning techniques to handle complex database constraints. Based on these ideas, we have developed a new framework that outperforms state-of-the-art techniques in zero-shot text-to-SQL on complex benchmarks\n\nTLDR:Integrating language understanding and automated reasoning for solving Text-to-SQL\n\nKeywords:Large Language Models, Automated Reasoning, Text-to-SQL"", ""claims"": [""Claim1: We analyze challenges that LLMs face in these settings and propose a new solution that combines the power of LLMs in understanding questions with automated reasoning techniques to handle complex database constraints."", ""Claim2: Based on these ideas, we have developed a new framework that outperforms state-of-the-art techniques in zero-shot text-to-SQL on complex benchmarks"", ""Claim3: Recently, LLM -based solutions have made significant progress in addressing the text-to-SQL problem [Gao et al., 2024, Li et al., 2024a]."", ""Claim4: While GPT-based methods have quickly reached near-human performance on academic benchmarks, like Spider [Yu et al., 2018], they struggle to provide high-quality user assistance on large industrial databases [Sequeda et al., 2023, Li et al., 2023]."", ""Claim5: In this work, we propose a new text-to-SQL framework, Lucy , designed for large databases with complex relationships between objects."", ""Claim6: Lucy is designed based on the principle of separation of responsibilities between generative tasks and automated reasoning tasks: each step focuses on either an NLP-related subproblem or a constraint reasoning subproblem."", ""Claim7: Lucy can support complex queries that include multiple filtering operators and aggregators, e.g. average or sum."", ""Claim8: Lucy shows very high coverage and solves about 30 of benchmarks according to the ex metric, which outperforms dw that solves 24 and other methods."", ""Claim9: Lucy and c2q have higher coverage and good accuracy. Lucy shows the best results in most cases."", ""Claim10: Lucy outperforms other models in both coverage and accuracy."", ""Claim11: We focus on the zero-shot text-to-SQL problem, which has received significant attention in the last few years."", ""Claim12: Liu et al. [2023] performed a comprehensive evaluation of ChatGPT on the Spider dataset and demonstrated that it shows good performance."", ""Claim13: [Dong et al., 2023], a new framework based on the GPT model was proposed, involving several techniques for promoting and post-processing the output to get more consistent results."", ""Claim14: Chang and Fosler-Lussier [2023] proposed several techniques to improve the performance of ChatGPT."", ""Claim15: According to the API documentation [TiDBCloud, 2020b], the authors construct a data summary object that contains \u2018AI exploration information of the given database.\u2019 This method performs very well on the BIRD dataset."", ""Claim16: Sequeda et al. [2023] performed an interesting investigation of the performance of LLM s on large industrial databases."", ""Claim17: They identified that GPT does not perform well when it needs to reason about complex relationships."", ""Claim18: The authors proposed a two-step approach to tackle this problem."", ""Claim19: We noticed that better results are obtained if we provide a description of tables that are used to generate this view together with their relevant attributes."", ""Claim20: We run experiments on a laptop Intel(r) Core 2.40Hz and 32GB of memory."", ""Claim21: We provide all benchmarks and their results in the supplementary materials."", ""Claim22: As the authors are working on corrections, we analyzed the reported issues and manually corrected the ground truth."", ""Claim23: We only adjusted the SQL ground truth values to match the user questions; we did not alter the questions or evidences."", ""Claim24: Interestingly, the performance of all frameworks improved by a few percentage points when we fixed these ground truth SQL statements."", ""Claim25: We discuss three major groups of challenging questions with examples."", ""Claim26: Note on the cost of running. One note here is that GPT and c2q models are costly to run. For example, in the Cloud Resources experiment, the costs are as follows: c2q costs $15, gpt4 $2, and gpt4ex $5, while Lucy costs $0.5.""], ""review"": ""Review: Weakness: The draft seems far from complete, so leave some high-level suggestions.\n1. Make the title, abstract, and introduction more concrete. It is hard to tell the contribution or uniqueness of this work among other papers about Text2SQL by LLMs.\n2. Survey related works and clearly state the contribution/novelty of the proposed method against others.\n3. Define the terminologies or abbreviations before their first appearance.\n4. Make the draft concise by removing unnecessary content. For example, the first challenge introduced in Motivation section is not relevant to this work.\n5. The empirical studies could be more convincing by following others' evaluation protocols, such as BIRD.\n6. Lack of comparison to other competitors.\n7. The figures, tables, and their captions should be self-explanatory.\nQuestions: 1. What is the key difference between the proposed framework and competitors (e.g., MCS-SQL, MAC-SQL, Chat2Query)?\n2. How does the proposed method perform if evaluated by the same protocol as the BIRD-SQL leaderboard?\n\n""}, ""review"": ""Title: Lucy: Think and Reason to Solve Text-to-SQL\nAbstract: Large Language Models (LLMs) have made significant progress in assisting users to query databases in natural language. While LLM-based techniques provide state-of-the-art results on many standard benchmarks, their performance significantly drops when applied to large enterprise databases. The reason is that these databases have a large number of tables with complex relationships that are challenging for LLMs to reason about. We analyze challenges that LLMs face in these settings and propose a new solution that combines the power of LLMs in understanding questions with automated reasoning techniques to handle complex database constraints. Based on these ideas, we have developed a new framework that outperforms state-of-the-art techniques in zero-shot text-to-SQL on complex benchmarks\n\nClaim1: We analyze challenges that LLMs face in these settings and propose a new solution that combines the power of LLMs in understanding questions with automated reasoning techniques to handle complex database constraints.\nClaim2: Based on these ideas, we have developed a new framework that outperforms state-of-the-art techniques in zero-shot text-to-SQL on complex benchmarks\nClaim3: Recently, LLM -based solutions have made significant progress in addressing the text-to-SQL problem [Gao et al., 2024, Li et al., 2024a].\nClaim4: While GPT-based methods have quickly reached near-human performance on academic benchmarks, like Spider [Yu et al., 2018], they struggle to provide high-quality user assistance on large industrial databases [Sequeda et al., 2023, Li et al., 2023].\nClaim5: In this work, we propose a new text-to-SQL framework, Lucy , designed for large databases with complex relationships between objects.\nClaim6: Lucy is designed based on the principle of separation of responsibilities between generative tasks and automated reasoning tasks: each step focuses on either an NLP-related subproblem or a constraint reasoning subproblem.\nClaim7: Lucy can support complex queries that include multiple filtering operators and aggregators, e.g. average or sum.\nClaim8: Lucy shows very high coverage and solves about 30 of benchmarks according to the ex metric, which outperforms dw that solves 24 and other methods.\nClaim9: Lucy and c2q have higher coverage and good accuracy. Lucy shows the best results in most cases.\nClaim10: Lucy outperforms other models in both coverage and accuracy.\nClaim11: We focus on the zero-shot text-to-SQL problem, which has received significant attention in the last few years.\nClaim12: Liu et al. [2023] performed a comprehensive evaluation of ChatGPT on the Spider dataset and demonstrated that it shows good performance.\nClaim13: [Dong et al., 2023], a new framework based on the GPT model was proposed, involving several techniques for promoting and post-processing the output to get more consistent results.\nClaim14: Chang and Fosler-Lussier [2023] proposed several techniques to improve the performance of ChatGPT.\nClaim15: According to the API documentation [TiDBCloud, 2020b], the authors construct a data summary object that contains \u2018AI exploration information of the given database.\u2019 This method performs very well on the BIRD dataset.\nClaim16: Sequeda et al. [2023] performed an interesting investigation of the performance of LLM s on large industrial databases.\nClaim17: They identified that GPT does not perform well when it needs to reason about complex relationships.\nClaim18: The authors proposed a two-step approach to tackle this problem.\nClaim19: We noticed that better results are obtained if we provide a description of tables that are used to generate this view together with their relevant attributes.\nClaim20: We run experiments on a laptop Intel(r) Core 2.40Hz and 32GB of memory.\nClaim21: We provide all benchmarks and their results in the supplementary materials.\nClaim22: As the authors are working on corrections, we analyzed the reported issues and manually corrected the ground truth.\nClaim23: We only adjusted the SQL ground truth values to match the user questions; we did not alter the questions or evidences.\nClaim24: Interestingly, the performance of all frameworks improved by a few percentage points when we fixed these ground truth SQL statements.\nClaim25: We discuss three major groups of challenging questions with examples.\nClaim26: Note on the cost of running. One note here is that GPT and c2q models are costly to run. For example, in the Cloud Resources experiment, the costs are as follows: c2q costs $15, gpt4 $2, and gpt4ex $5, while Lucy costs $0.5.\n\nReview: Weakness: The draft seems far from complete, so leave some high-level suggestions.\n1. Make the title, abstract, and introduction more concrete. It is hard to tell the contribution or uniqueness of this work among other papers about Text2SQL by LLMs.\n2. Survey related works and clearly state the contribution/novelty of the proposed method against others.\n3. Define the terminologies or abbreviations before their first appearance.\n4. Make the draft concise by removing unnecessary content. For example, the first challenge introduced in Motivation section is not relevant to this work.\n5. The empirical studies could be more convincing by following others' evaluation protocols, such as BIRD.\n6. Lack of comparison to other competitors.\n7. The figures, tables, and their captions should be self-explanatory.\nQuestions: 1. What is the key difference between the proposed framework and competitors (e.g., MCS-SQL, MAC-SQL, Chat2Query)?\n2. How does the proposed method perform if evaluated by the same protocol as the BIRD-SQL leaderboard?\n\n\n\n"", ""id"": ""lEDuaGGiCV2"", ""pdf"": ""openreview.net/pdf?id=lEDuaGGiCV""}","45"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""The VQA benchmarks reported in Table 1 are not very convincing. It would be beneficial to consider more modern and challenging benchmarks such as MMBench, MathVista, and SeedBench."", ""Target claims"": [1, 3], ""Reasoning"": ""This span questions the convincing nature of the VQA benchmarks used in the paper. Claim 1 mentions achieving state-of-the-art performance across benchmarks, and Claim 3 lists the specific benchmarks used. The reviewer suggests that the benchmarks are not modern or challenging enough, which directly questions the robustness of these claims in terms of benchmark selection and thus their support for demonstrating improvement or superiority.""}, {""Weakness span"": ""The comparison of baseline methods seems to be based on relatively outdated approaches. It might be more informative to compare them with more recent LVLMs like LLaVA-1.5, Monkey, and ShareGPT4V."", ""Target claims"": [1, 4], ""Reasoning"": ""This weakness points out that the methods used for baseline comparisons are outdated, which can undermine the claims regarding improvements or state-of-the-art performance. Claim 1 discusses achieving state-of-the-art performance, and Claim 4 talks about specific improvements and superior performance metrics. Both claims are questioned as the benchmarks used to substantiate them are considered outdated, impacting their validity.""}, {""Weakness span"": ""The performance improvements of CogCom in comparison to CogVLM appear to be relatively insignificant. Could this be attributed to the low success rate (around 55% in Figure 6)? Further explanations are necessary to address the situation where the planned pathway failed."", ""Target claims"": [1, 4, 23], ""Reasoning"": ""This weakness questions the significance of performance improvements claimed in the paper. Claim 1 mentions state-of-the-art performance, Claim 4 states specific improvements in accuracy, and Claim 23 discusses performance outcomes with training steps. The reviewer points out a potentially low success rate, undermining claims of substantial improvements or effectiveness, thus questioning the claims' substantiation.""}]}, ""meta"": {""id"": ""k0qTnbQxzR"", ""review_idx"": 0, ""title"": ""Title: CogCoM: Train Large Vision-Language Models Diving into Details  through\u000b Chain of Manipulations\nAbstract: Vision-Language Models (VLMs) have demonstrated their broad effectiveness thanks to extensive training in aligning visual instructions to responses. However, such training of conclusive alignment leads models to ignore essential visual reasoning, further resulting in failures in meticulous visual problems and unfaithful responses. Drawing inspiration from human cognition in solving visual problems (e.g., marking, zoom in), this paper introduces Chain of Manipulations, a mechanism that enables VLMs to solve problems step-by-step with evidence. After training, models can solve various visual problems by eliciting intrinsic manipulations (e.g., grounding, zoom in) and results (e.g., boxes, image) actively without involving external tools, while also allowing users to trace error causes. We study the roadmap to implement this mechanism, including (1) a flexible design of manipulations upon extensive analysis, (2) an efficient automated data generation pipeline, (3) a compatible VLM architecture capable of multi-turn multi-image, and (4) a model training process for versatile capabilities.  With the design, we also manually annotate 6K high-quality samples for the challenging graphical mathematical problems. Our trained model, CogCoM, equipped with this mechanism with 17B parameters achieves state-of-the-art performance across 9 benchmarks from 4 categories, demonstrating the effectiveness while preserving the interpretability. Our code, model weights, and collected data will be publicly available.\n\nTLDR:\n\nKeywords:Vision-Language Models, Multimodal Large Language Models, Visual Reasoning, Chain of Manipulations"", ""claims"": [""Claim1: Our trained model, CogCoM, equipped with this mechanism with 17B parameters achieves state-of-the-art performance across 9 benchmarks from 4 categories, demonstrating the effectiveness while preserving the interpretability."", ""Claim2: We collect 70K CoM samples with this pipeline."", ""Claim3: We conduct extensive experiments on 9 benchmarks from 4 categories, including TextVQA (Singh et al., 2019), ST-VQA (Biten et al., 2019), TallyVQA (Acharya et al., 2019), and GQA Hudson & Manning (2019) for detailed visual question answering, RefCOCO (Yu et al., 2016), RefCOCO+(Yu et al., 2016), and RefCOCOg (Mao et al., 2016) for visual grounding, POPE (Li et al., 2023c) for hallucination validation, and MM-Vet (Yu et al., 2023b) for general multimodal ability."", ""Claim4: Our model achieves up to 9.0 and 1.09 accuracy improvement on the detailed VQA and grounding benchmarks, respectively, and the superior performance on the general multimodal benchmark."", ""Claim5: The results demonstrate the effectiveness of the mechanism while maintaining the interpretability of outputs."", ""Claim6: We empirically find that more complicated goals can be derived from these fundamental manipulations."", ""Claim7: In this section, we first introduces the automated data generation pipeline (illustrated in Figure 4), that employs reliable LLMs as linguistic annotators and VFMs as the visual annotators to produce error-free CoM samples upon prevalent VQA corpus, and then present the manual annotation of high-quality CoM samples for the challenging graphical mathematical problems."", ""Claim8: Based on this method, the generated CoM samples with positive paths are guaranteed to be error-free."", ""Claim9: We implement this pipeline on 3 existing datasets that require detailed recognition or objects counting, TextVQA (Singh et al., 2019), ST-VQA (Biten et al., 2019), and TDIUC (Shrestha et al., 2019), to build 70K CoM samples."", ""Claim10: The analysis from Fig.1 of AlphaGeometry (Trinh et al., 2024) shows that outputting auxiliary lines in linguistic reasoning process helps LLMs to solve complex geometry problems."", ""Claim11: Benefiting from the expressive capability of CoM structure, we have also manually annotated high-quality CoM samples for the graphical mathematical problems to facilitate VLMs in solving this challenging scenario."", ""Claim12: Similar to the automated pipeline, we engage 10 human experts as the linguistic annotators and visual annotators, where each expert is asked to annotate the linguistic solving steps and the use of manipulations, as well as the results of manipulations on images."", ""Claim13: We perform this annotation on the MathVista (Lu et al., 2023) and ChartQA (Masry et al., 2022), which include geometric and chart math problems, resulting in the collection of 6K high-quality CoM math samples."", ""Claim14: Finally, we adapt the CoM samples to be compatible with VQA-style training samples. For each CoM sample including n images from manipulations outputs ( I 0 , Q, C 0 , I 1 , C 1 , ..., I n , A ) , we convert it into a multi-turn VQA sample segmented by the images [( I 0 , Q, C 0 ) , ( I 1 , \u00af Q, C 1 ) , ..., ( I n , \u00af Q, A )] , where C i represents the intermediate steps between I i and I i +1 , and \u00af Q is a simple prompt asking model to answer question based on history. This transformation converts CoM samples into multi-turn VQA samples that are compatible with existing VLMs training data."", ""Claim15: The proposed CogCoM-17B relies on two main stages of training, to develop the capabilities of general multimodal task-solving as well as the visual reasoning."", ""Claim16: We empirically show that the model can effectively learn the evidential visual reasoning by ingesting this portion of CoM data."", ""Claim17: These two stages of training result in our standard version of CogCoM involving both chat and reasoning capabilities."", ""Claim18: Following previous works, we train two generalist versions of CogCoM for adapting to the different scenarios of Visual Question Answering and Visual Grounding, and evaluate the standard version with a qualitative analysis."", ""Claim19: VLMs have demonstrated the well-known superiority in visual scenes with salient content understanding."", ""Claim20: This training results in a generalist VQA model incorporating CoM reasoning."", ""Claim21: Results As the results shown in Table 2, CogCoM achieves the state-of-the-art performance in comparison with all generalist models, and achieves significant improvements over the baseline model."", ""Claim22: On GQA and TextVQA, CogCoM also obtains comparable results with the large-scale specialist SOTAs."", ""Claim23: We find that by training with the CoM chains, our model can swiftly achieve the satisfactory performance of 48.41 accuracy score with 2k training steps, and obtain the optimal result of 55.59 with 8K steps."", ""Claim24: From the results we find that compared to baseline model, CogCoM produces information-intensive reasoning content (e.g., detection boxes, auxiliary lines) without incurring infeasible time overhead."", ""Claim25: Without quantitive optimization, CogCoM outputs 262.9 informative tokens in approximately 9 seconds."", ""Claim26: With the advantages in long-context optimization techniques, we believe that it is crucial for models to produce informative content and accurate responses."", ""Claim27: Results As shown in Figure 2, CogCoM achieves the best performance in 6 out of all 8 sub-sets."", ""Claim28: This result indicates that our model exhibits superior grounding abilities while offers potential to solve a variety of tasks."", ""Claim29: CogCoM improves the performance by 0.6 points compared to the baseline model on MM-Vet, and achieves the superior performance on POPE which is in consistent with the baseline model."", ""Claim30: This result suggests that our model maintains superior reasoning capabilities while preserving effectiveness in general multimodal tasks, and simultaneously exhibits lower hallucination."", ""Claim31: Experiments on 9 public benchmarks show that our trained 17B general VLM can produce informative reasoning content while achieving superior performance on diverse multimodal problems."", ""Claim32: This work presents a general visual reasoning mechanism that alleviate the problems caused by existing conclusion-alignment training for VLMs, introduces a data production framework involving LLMs and visual tools as reliable annotators, and devises a memory-based compatible VLM architecture."", ""Claim33: We expect this work to bring three benefits to the community. First, the proposed visual reasoning mechanism may push the progress of VLMs in solving complex visual problems. Second, the introduced data production framework may be applied to widespread training scenarios to promote the development of current data-driven machine learning. Third, we hope that the memory-based architecture will be helpful for VLMs in multi-turn long contexts."", ""Claim34: We investigate the evidential reasoning capability of CogCoM on scenarios that requires different types of meticulous reasoning, including recognizing textual details, reading time, understanding charts and counting objects."", ""Claim35: The first case demonstrates that CogCoM finds the region corresponding to the plane logo through two steps of grounding and then achieves the answer based on zooming in the cropped region."", ""Claim36: The second case illustrates the ability of CogCoM in reading time, by locating the device that displays time and then transforming the time into words based on the read_timne manipulation."", ""Claim37: In the forth example, CogCoM first identifies all visible truck wheels through grounding, and then leverages the counting manipulation to consolidate the total number.""], ""review"": ""Review: Weakness: 1. The VQA benchmarks reported in Table 1 are not very convincing. It would be beneficial to consider more modern and challenging benchmarks such as MMBench, MathVista, and SeedBench.\n2. The comparison of baseline methods seems to be based on relatively outdated approaches. It might be more informative to compare them with more recent LVLMs like LLaVA-1.5, Monkey, and ShareGPT4V.\n3. It would be helpful to discuss a closely related work ViperGPT [3] and V* [4]. ViperGPT shares an idea for solving visual problems via planning tool pathways. V* shares the idea of searching and zoom-in progressively.\n4. The differences with some other related works should be discussed [5][6]. \n\n[1] Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models\n\n[2] ShareGPT4V: Improving Large Multi-Modal Models with Better Captions\n\n[3] ViperGPT: Visual Inference via Python Execution for Reasoning\n\n[4] V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs\n\n[5] CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding\n\n[6] DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models\nQuestions: 1. The performance improvements of CogCom in comparison to CogVLM appear to be relatively insignificant. Could this be attributed to the low success rate (around 55% in Figure 6)? Further explanations are necessary to address the situation where the planned pathway failed.\n\nI will be pleased to raise my rating if my concerns as in weaknesses and questions are resolved.\n\n""}, ""review"": ""Title: CogCoM: Train Large Vision-Language Models Diving into Details  through\u000b Chain of Manipulations\nAbstract: Vision-Language Models (VLMs) have demonstrated their broad effectiveness thanks to extensive training in aligning visual instructions to responses. However, such training of conclusive alignment leads models to ignore essential visual reasoning, further resulting in failures in meticulous visual problems and unfaithful responses. Drawing inspiration from human cognition in solving visual problems (e.g., marking, zoom in), this paper introduces Chain of Manipulations, a mechanism that enables VLMs to solve problems step-by-step with evidence. After training, models can solve various visual problems by eliciting intrinsic manipulations (e.g., grounding, zoom in) and results (e.g., boxes, image) actively without involving external tools, while also allowing users to trace error causes. We study the roadmap to implement this mechanism, including (1) a flexible design of manipulations upon extensive analysis, (2) an efficient automated data generation pipeline, (3) a compatible VLM architecture capable of multi-turn multi-image, and (4) a model training process for versatile capabilities.  With the design, we also manually annotate 6K high-quality samples for the challenging graphical mathematical problems. Our trained model, CogCoM, equipped with this mechanism with 17B parameters achieves state-of-the-art performance across 9 benchmarks from 4 categories, demonstrating the effectiveness while preserving the interpretability. Our code, model weights, and collected data will be publicly available.\n\nClaim1: Our trained model, CogCoM, equipped with this mechanism with 17B parameters achieves state-of-the-art performance across 9 benchmarks from 4 categories, demonstrating the effectiveness while preserving the interpretability.\nClaim2: We collect 70K CoM samples with this pipeline.\nClaim3: We conduct extensive experiments on 9 benchmarks from 4 categories, including TextVQA (Singh et al., 2019), ST-VQA (Biten et al., 2019), TallyVQA (Acharya et al., 2019), and GQA Hudson & Manning (2019) for detailed visual question answering, RefCOCO (Yu et al., 2016), RefCOCO+(Yu et al., 2016), and RefCOCOg (Mao et al., 2016) for visual grounding, POPE (Li et al., 2023c) for hallucination validation, and MM-Vet (Yu et al., 2023b) for general multimodal ability.\nClaim4: Our model achieves up to 9.0 and 1.09 accuracy improvement on the detailed VQA and grounding benchmarks, respectively, and the superior performance on the general multimodal benchmark.\nClaim5: The results demonstrate the effectiveness of the mechanism while maintaining the interpretability of outputs.\nClaim6: We empirically find that more complicated goals can be derived from these fundamental manipulations.\nClaim7: In this section, we first introduces the automated data generation pipeline (illustrated in Figure 4), that employs reliable LLMs as linguistic annotators and VFMs as the visual annotators to produce error-free CoM samples upon prevalent VQA corpus, and then present the manual annotation of high-quality CoM samples for the challenging graphical mathematical problems.\nClaim8: Based on this method, the generated CoM samples with positive paths are guaranteed to be error-free.\nClaim9: We implement this pipeline on 3 existing datasets that require detailed recognition or objects counting, TextVQA (Singh et al., 2019), ST-VQA (Biten et al., 2019), and TDIUC (Shrestha et al., 2019), to build 70K CoM samples.\nClaim10: The analysis from Fig.1 of AlphaGeometry (Trinh et al., 2024) shows that outputting auxiliary lines in linguistic reasoning process helps LLMs to solve complex geometry problems.\nClaim11: Benefiting from the expressive capability of CoM structure, we have also manually annotated high-quality CoM samples for the graphical mathematical problems to facilitate VLMs in solving this challenging scenario.\nClaim12: Similar to the automated pipeline, we engage 10 human experts as the linguistic annotators and visual annotators, where each expert is asked to annotate the linguistic solving steps and the use of manipulations, as well as the results of manipulations on images.\nClaim13: We perform this annotation on the MathVista (Lu et al., 2023) and ChartQA (Masry et al., 2022), which include geometric and chart math problems, resulting in the collection of 6K high-quality CoM math samples.\nClaim14: Finally, we adapt the CoM samples to be compatible with VQA-style training samples. For each CoM sample including n images from manipulations outputs ( I 0 , Q, C 0 , I 1 , C 1 , ..., I n , A ) , we convert it into a multi-turn VQA sample segmented by the images [( I 0 , Q, C 0 ) , ( I 1 , \u00af Q, C 1 ) , ..., ( I n , \u00af Q, A )] , where C i represents the intermediate steps between I i and I i +1 , and \u00af Q is a simple prompt asking model to answer question based on history. This transformation converts CoM samples into multi-turn VQA samples that are compatible with existing VLMs training data.\nClaim15: The proposed CogCoM-17B relies on two main stages of training, to develop the capabilities of general multimodal task-solving as well as the visual reasoning.\nClaim16: We empirically show that the model can effectively learn the evidential visual reasoning by ingesting this portion of CoM data.\nClaim17: These two stages of training result in our standard version of CogCoM involving both chat and reasoning capabilities.\nClaim18: Following previous works, we train two generalist versions of CogCoM for adapting to the different scenarios of Visual Question Answering and Visual Grounding, and evaluate the standard version with a qualitative analysis.\nClaim19: VLMs have demonstrated the well-known superiority in visual scenes with salient content understanding.\nClaim20: This training results in a generalist VQA model incorporating CoM reasoning.\nClaim21: Results As the results shown in Table 2, CogCoM achieves the state-of-the-art performance in comparison with all generalist models, and achieves significant improvements over the baseline model.\nClaim22: On GQA and TextVQA, CogCoM also obtains comparable results with the large-scale specialist SOTAs.\nClaim23: We find that by training with the CoM chains, our model can swiftly achieve the satisfactory performance of 48.41 accuracy score with 2k training steps, and obtain the optimal result of 55.59 with 8K steps.\nClaim24: From the results we find that compared to baseline model, CogCoM produces information-intensive reasoning content (e.g., detection boxes, auxiliary lines) without incurring infeasible time overhead.\nClaim25: Without quantitive optimization, CogCoM outputs 262.9 informative tokens in approximately 9 seconds.\nClaim26: With the advantages in long-context optimization techniques, we believe that it is crucial for models to produce informative content and accurate responses.\nClaim27: Results As shown in Figure 2, CogCoM achieves the best performance in 6 out of all 8 sub-sets.\nClaim28: This result indicates that our model exhibits superior grounding abilities while offers potential to solve a variety of tasks.\nClaim29: CogCoM improves the performance by 0.6 points compared to the baseline model on MM-Vet, and achieves the superior performance on POPE which is in consistent with the baseline model.\nClaim30: This result suggests that our model maintains superior reasoning capabilities while preserving effectiveness in general multimodal tasks, and simultaneously exhibits lower hallucination.\nClaim31: Experiments on 9 public benchmarks show that our trained 17B general VLM can produce informative reasoning content while achieving superior performance on diverse multimodal problems.\nClaim32: This work presents a general visual reasoning mechanism that alleviate the problems caused by existing conclusion-alignment training for VLMs, introduces a data production framework involving LLMs and visual tools as reliable annotators, and devises a memory-based compatible VLM architecture.\nClaim33: We expect this work to bring three benefits to the community. First, the proposed visual reasoning mechanism may push the progress of VLMs in solving complex visual problems. Second, the introduced data production framework may be applied to widespread training scenarios to promote the development of current data-driven machine learning. Third, we hope that the memory-based architecture will be helpful for VLMs in multi-turn long contexts.\nClaim34: We investigate the evidential reasoning capability of CogCoM on scenarios that requires different types of meticulous reasoning, including recognizing textual details, reading time, understanding charts and counting objects.\nClaim35: The first case demonstrates that CogCoM finds the region corresponding to the plane logo through two steps of grounding and then achieves the answer based on zooming in the cropped region.\nClaim36: The second case illustrates the ability of CogCoM in reading time, by locating the device that displays time and then transforming the time into words based on the read_timne manipulation.\nClaim37: In the forth example, CogCoM first identifies all visible truck wheels through grounding, and then leverages the counting manipulation to consolidate the total number.\n\nReview: Weakness: 1. The VQA benchmarks reported in Table 1 are not very convincing. It would be beneficial to consider more modern and challenging benchmarks such as MMBench, MathVista, and SeedBench.\n2. The comparison of baseline methods seems to be based on relatively outdated approaches. It might be more informative to compare them with more recent LVLMs like LLaVA-1.5, Monkey, and ShareGPT4V.\n3. It would be helpful to discuss a closely related work ViperGPT [3] and V* [4]. ViperGPT shares an idea for solving visual problems via planning tool pathways. V* shares the idea of searching and zoom-in progressively.\n4. The differences with some other related works should be discussed [5][6]. \n\n[1] Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models\n\n[2] ShareGPT4V: Improving Large Multi-Modal Models with Better Captions\n\n[3] ViperGPT: Visual Inference via Python Execution for Reasoning\n\n[4] V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs\n\n[5] CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding\n\n[6] DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models\nQuestions: 1. The performance improvements of CogCom in comparison to CogVLM appear to be relatively insignificant. Could this be attributed to the low success rate (around 55% in Figure 6)? Further explanations are necessary to address the situation where the planned pathway failed.\n\nI will be pleased to raise my rating if my concerns as in weaknesses and questions are resolved.\n\n\n\n"", ""id"": ""k0qTnbQxzR0"", ""pdf"": ""openreview.net/pdf?id=k0qTnbQxzR""}","46"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""Weakness: More experiments on other LLMs are needed to further validate the claim."", ""Target claims"": [3], ""Reasoning"": ""The reviewer questions the generalizability of the experimental results reported in the paper by suggesting that more experiments on other LLMs are necessary to validate Claim 3. This directly targets Claim 3, which involves experiments showing that classifiers do not predict knowledge but instead predict a different prominent feature, implying that the current experimental evidence is insufficient to support the claim comprehensively.""}]}, ""meta"": {""id"": ""hGhLd2ByoR"", ""review_idx"": 0, ""title"": ""Title: Challenges with unsupervised LLM knowledge discovery\nAbstract: We reveal novel pathologies in existing unsupervised methods seeking to discover latent knowledge from large language model (LLM) activations---instead of knowledge they seem to discover whatever feature of the activations is most prominent. These methods search for hypothesised consistency structures of latent knowledge. We first prove theoretically that arbitrary features (not just knowledge) satisfy the consistency structure of a popular unsupervised knowledge-elicitation method: contrast-consistent search. We then present a series of experiments showing settings in which this and other unsupervised methods result in classifiers that do not predict knowledge, but instead predict a different prominent feature. We conclude that existing unsupervised methods for discovering latent knowledge are insufficient, and we contribute sanity checks to apply to evaluating future knowledge elicitation methods. We offer conceptual arguments grounded in identification issues such as distinguishing a model's knowledge from that of a simulated character's that are likely to persist in future unsupervised methods.\n\nTLDR:Theorems and experiments reveal that past attempts to discover latent knowledge in large language models in fact discover things other than knowledge, and this challenge seems likely to continue.\n\nKeywords:Eliciting latent knowledge, large language models, deception"", ""claims"": [""Claim1: We reveal novel pathologies in existing unsupervised methods seeking to discover latent knowledge from large language model (LLM) activations---instead of knowledge they seem to discover whatever feature of the activations is most prominent."", ""Claim2: We first prove theoretically that arbitrary features (not just knowledge) satisfy the consistency structure of a popular unsupervised knowledge-elicitation method: contrast-consistent search."", ""Claim3: We then present a series of experiments showing settings in which this and other unsupervised methods result in classifiers that do not predict knowledge, but instead predict a different prominent feature."", ""Claim4: We conclude that existing unsupervised methods for discovering latent knowledge are insufficient, and we contribute sanity checks to apply to evaluating future knowledge elicitation methods."", ""Claim5: We refute these claims by identifying classes of LLM features that also satisfy this consistency structure but are not knowledge."", ""Claim6: We prove two theorems: 1) a class of arbitrary binary classifiers are optimal under the CCS loss; 2) any classifier can be transformed to an arbitrary classifier with the same CCS loss."", ""Claim7: We then show that other unsupervised methods in addition to CCS empirically do not discover knowledge, regardless of any inductive biases that might hypothetically be present."", ""Claim8: We conclude that existing unsupervised knowledge-discovery methods are insufficient in practice, and we propose principles for evaluating knowledge elicitation methods to prevent future \u201cfalse-positives\u201d in the literature."", ""Claim9: We hypothesise that our conclusions will generalise to more sophisticated methods, though perhaps not the exact experimental results: using different consistency structures of knowledge will likely suffer from similar issues to what we show here."", ""Claim10: Our theoretical results focus on CCS, showing that CCS\u2019s consistency structure isn\u2019t specific to knowledge."", ""Claim11: This implies that arguments for CCS\u2019s effectiveness cannot be grounded in conceptual or principled motivations from the loss construction."", ""Claim12: Theorem 1 . Let feature h : Q \u2192 { 0 , 1 } , be any arbitrary map from questions to binary outcomes. Let ( x + i , x \u2212 i ) be the contrast pair corresponding to question q i and let c ( x + i ) = 1 , c ( x + i ) = 0 . Then the probe defined as p ( x \u00b1 i ) = h ( q i ) \u2295 c ( x \u00b1 i ) achieves optimal loss, and the averaged prediction satisfies \u02dc p ( q i ) = h ( q i ) ."", ""Claim13: Our experiments largely agree with this finding\u2014see Appendix D.6 for an additional experiment showing agreement between the predictions of these methods."", ""Claim14: In fact, experimentally, we now show that, in practice, several methods including CCS often discover probes for features other than knowledge."", ""Claim15: Motivated by our theoretical results, we first introduce a distracting binary feature and show the unsupervised methods discover this feature rather than knowledge."", ""Claim16: Our results are shown in Figure 2a, displaying accuracy of each method (x-axis groups)."", ""Claim17: The takeaway is that CCS and other unsupervised methods do not optimise for ground-truth knowledge, but rather track whatever feature (in this case, banana/shed) is most prominent in the activations."", ""Claim18: Similarly to our previous results, we show that CCS and other unsupervised methods have low ground-truth accuracy, but high accuracy at predicting Alice\u2019s belief (Figure 3a)."", ""Claim19: The default setting (blue) produces high accuracy classifiers both when the topic is company (dark blue) and other categories (light blue)."", ""Claim20: In the modified setting (red) CCS gives a bimodal distribution when the topic is company (dark red), with almost half of the probes (differing only in random initialisation) predicting Alice\u2019s opinion, rather than the actual topic."", ""Claim21: This shows these methods are still sensitive to the modified setting, which was not evident from the accuracy metric alone."", ""Claim22: We find that a \u201cnon-default\u201d prompt gives the \u201cbest performance\u201d in the sense of the highest test-set accuracy."", ""Claim23: PCA visualisations are shown in Figure 5b, coloured by whether the question is True/False, in the default (left), literal (middle) and professor (right) settings."", ""Claim24: Though the accuracy for K-means in the Professor prompt for T5-FLAN-XXL are clearly stronger than others."", ""Claim25: We have refuted this claim, theoretically and empirically, showing that CCS performs similarly to other unsupervised methods which do not use special structure of knowledge."", ""Claim26: We prove theoretically that CCS does not optimise for knowledge, and show empirically what non-knowledge features CCS instead finds."", ""Claim27: Our result is analogous to the finding that disentangled representations seemingly cannot be identified without supervision."", ""Claim28: We show that none of the methods we consider in this paper satisfy these desiderata."", ""Claim29: Existing unsupervised methods are insufficient for discovering latent knowledge, though constructing contrastive activations may still serve as a useful interpretability tool."", ""Claim30: We contribute sanity checks for evaluating methods using modified prompts and metrics for features which are not knowledge."", ""Claim31: here we demonstrate that CCS as a proposed method for discovering latent knowledge has not faced questions along the lines this paper proposes at time of writing. In Table 1, we review the 20 most-cited papers citing CCS according to Google Scholar at time of writing (26 March 2024). We find that the concerns we raise are overlooked by the current literature."", ""Claim32: Here we display results for the discovering random words experiments using datasets IMDb, BoolQ and DBpedia and on each model."", ""Claim33: For Chinchilla-70B BoolQ and DBPedia see Figure 6 (for IMDb see Figure 2)."", ""Claim34: We see that BoolQ follows a roughly similar pattern to IMDb, except that the default ground truth accuracy is not high (BoolQ is arguably a more challenging task)."", ""Claim35: DBpedia shows more of a noisy pattern which is best explained by first inspecting the PCA visualisation for the modified prompt (right): there are groupings into both choice 1 true/false (blue orange) which is more prominent and sits along the top principal component (x-axis), and also a grouping into banana/shed (dark/light), along second component (y-axis)."", ""Claim36: This is reflected in the PCA and K-means performance here doing well on ground-truth accuracy."", ""Claim37: CCS is similar, but more bimodal, sometimes finding the ground-truth, and sometimes the banana/shed feature."", ""Claim38: For T5-11B (Figure 7) on IMDB and BoolQ we see a similar pattern of results to Chinchilla, though with lower accuracies."", ""Claim39: On DBpedia, all of the results are around random chance, though logistic regression is able to solve the task, meaning this information is linearly encoded but perhaps not salient enough for the unsupervised methods to pick up."", ""Claim40: T5-FLAN-XXL (Figure 8) shows more resistance to our modified prompt, suggesting fine-tuning hardens the activations in such a way that unsupervised learning can still recover knowledge."", ""Claim41: For Chinchilla and T5, only one repetition is enough for the method to track the speaker\u2019s opinion."", ""Claim42: T5-FLAN-XXL requires more repetitions, but eventually shows the same pattern."", ""Claim43: Overall, we find our results are not that sensitive to layer, though often layer 30 is a good choice for both standard and sycophantic templates."", ""Claim44: For T5-11B we see CCS, under both default and modified prompts, performs at about 60% on non-company questions, and much better on company questions."", ""Claim45: PCA and K-means are similar, though with less variation amongst seeds (showing less bimodal behaviour)."", ""Claim46: Overall we do not see a major effect."", ""Claim47: Burns et al. [9] claim that knowledge has special structure that few other features in an LLM are likely to satisfy and use this to motivate CCS."", ""Claim48: Nevertheless, we find that CCS and PCA 8 make similar predictions."", ""Claim49: There is higher agreement (top-line number) in all cases than what one would expect from independent methods (notated \u201cInd:\u201d) with the observed accuracies (shown in parentheses in the heatmap)."", ""Claim50: This supports the hypothesis of Emmons [16] and suggests that the consistency-condition does not do much."", ""Claim51: But the fact that two methods with such different motivations behave similarly also supports the idea that results on current unsupervised methods may be predictive of future methods which have different motivations.""], ""review"": ""Review: Weakness: More experiments on other LLMs are needed to further validate the claim.\n\nIt would be better to offer possible solutions to address the problems in existing unsupervised methods.\nQuestions: More experiments on other LLMs are needed to further validate the claim.\n\nIt would be better to offer some possible solutions to address the problems in existing unsupervised methods.\n\n""}, ""review"": ""Title: Challenges with unsupervised LLM knowledge discovery\nAbstract: We reveal novel pathologies in existing unsupervised methods seeking to discover latent knowledge from large language model (LLM) activations---instead of knowledge they seem to discover whatever feature of the activations is most prominent. These methods search for hypothesised consistency structures of latent knowledge. We first prove theoretically that arbitrary features (not just knowledge) satisfy the consistency structure of a popular unsupervised knowledge-elicitation method: contrast-consistent search. We then present a series of experiments showing settings in which this and other unsupervised methods result in classifiers that do not predict knowledge, but instead predict a different prominent feature. We conclude that existing unsupervised methods for discovering latent knowledge are insufficient, and we contribute sanity checks to apply to evaluating future knowledge elicitation methods. We offer conceptual arguments grounded in identification issues such as distinguishing a model's knowledge from that of a simulated character's that are likely to persist in future unsupervised methods.\n\nClaim1: We reveal novel pathologies in existing unsupervised methods seeking to discover latent knowledge from large language model (LLM) activations---instead of knowledge they seem to discover whatever feature of the activations is most prominent.\nClaim2: We first prove theoretically that arbitrary features (not just knowledge) satisfy the consistency structure of a popular unsupervised knowledge-elicitation method: contrast-consistent search.\nClaim3: We then present a series of experiments showing settings in which this and other unsupervised methods result in classifiers that do not predict knowledge, but instead predict a different prominent feature.\nClaim4: We conclude that existing unsupervised methods for discovering latent knowledge are insufficient, and we contribute sanity checks to apply to evaluating future knowledge elicitation methods.\nClaim5: We refute these claims by identifying classes of LLM features that also satisfy this consistency structure but are not knowledge.\nClaim6: We prove two theorems: 1) a class of arbitrary binary classifiers are optimal under the CCS loss; 2) any classifier can be transformed to an arbitrary classifier with the same CCS loss.\nClaim7: We then show that other unsupervised methods in addition to CCS empirically do not discover knowledge, regardless of any inductive biases that might hypothetically be present.\nClaim8: We conclude that existing unsupervised knowledge-discovery methods are insufficient in practice, and we propose principles for evaluating knowledge elicitation methods to prevent future \u201cfalse-positives\u201d in the literature.\nClaim9: We hypothesise that our conclusions will generalise to more sophisticated methods, though perhaps not the exact experimental results: using different consistency structures of knowledge will likely suffer from similar issues to what we show here.\nClaim10: Our theoretical results focus on CCS, showing that CCS\u2019s consistency structure isn\u2019t specific to knowledge.\nClaim11: This implies that arguments for CCS\u2019s effectiveness cannot be grounded in conceptual or principled motivations from the loss construction.\nClaim12: Theorem 1 . Let feature h : Q \u2192 { 0 , 1 } , be any arbitrary map from questions to binary outcomes. Let ( x + i , x \u2212 i ) be the contrast pair corresponding to question q i and let c ( x + i ) = 1 , c ( x + i ) = 0 . Then the probe defined as p ( x \u00b1 i ) = h ( q i ) \u2295 c ( x \u00b1 i ) achieves optimal loss, and the averaged prediction satisfies \u02dc p ( q i ) = h ( q i ) .\nClaim13: Our experiments largely agree with this finding\u2014see Appendix D.6 for an additional experiment showing agreement between the predictions of these methods.\nClaim14: In fact, experimentally, we now show that, in practice, several methods including CCS often discover probes for features other than knowledge.\nClaim15: Motivated by our theoretical results, we first introduce a distracting binary feature and show the unsupervised methods discover this feature rather than knowledge.\nClaim16: Our results are shown in Figure 2a, displaying accuracy of each method (x-axis groups).\nClaim17: The takeaway is that CCS and other unsupervised methods do not optimise for ground-truth knowledge, but rather track whatever feature (in this case, banana/shed) is most prominent in the activations.\nClaim18: Similarly to our previous results, we show that CCS and other unsupervised methods have low ground-truth accuracy, but high accuracy at predicting Alice\u2019s belief (Figure 3a).\nClaim19: The default setting (blue) produces high accuracy classifiers both when the topic is company (dark blue) and other categories (light blue).\nClaim20: In the modified setting (red) CCS gives a bimodal distribution when the topic is company (dark red), with almost half of the probes (differing only in random initialisation) predicting Alice\u2019s opinion, rather than the actual topic.\nClaim21: This shows these methods are still sensitive to the modified setting, which was not evident from the accuracy metric alone.\nClaim22: We find that a \u201cnon-default\u201d prompt gives the \u201cbest performance\u201d in the sense of the highest test-set accuracy.\nClaim23: PCA visualisations are shown in Figure 5b, coloured by whether the question is True/False, in the default (left), literal (middle) and professor (right) settings.\nClaim24: Though the accuracy for K-means in the Professor prompt for T5-FLAN-XXL are clearly stronger than others.\nClaim25: We have refuted this claim, theoretically and empirically, showing that CCS performs similarly to other unsupervised methods which do not use special structure of knowledge.\nClaim26: We prove theoretically that CCS does not optimise for knowledge, and show empirically what non-knowledge features CCS instead finds.\nClaim27: Our result is analogous to the finding that disentangled representations seemingly cannot be identified without supervision.\nClaim28: We show that none of the methods we consider in this paper satisfy these desiderata.\nClaim29: Existing unsupervised methods are insufficient for discovering latent knowledge, though constructing contrastive activations may still serve as a useful interpretability tool.\nClaim30: We contribute sanity checks for evaluating methods using modified prompts and metrics for features which are not knowledge.\nClaim31: here we demonstrate that CCS as a proposed method for discovering latent knowledge has not faced questions along the lines this paper proposes at time of writing. In Table 1, we review the 20 most-cited papers citing CCS according to Google Scholar at time of writing (26 March 2024). We find that the concerns we raise are overlooked by the current literature.\nClaim32: Here we display results for the discovering random words experiments using datasets IMDb, BoolQ and DBpedia and on each model.\nClaim33: For Chinchilla-70B BoolQ and DBPedia see Figure 6 (for IMDb see Figure 2).\nClaim34: We see that BoolQ follows a roughly similar pattern to IMDb, except that the default ground truth accuracy is not high (BoolQ is arguably a more challenging task).\nClaim35: DBpedia shows more of a noisy pattern which is best explained by first inspecting the PCA visualisation for the modified prompt (right): there are groupings into both choice 1 true/false (blue orange) which is more prominent and sits along the top principal component (x-axis), and also a grouping into banana/shed (dark/light), along second component (y-axis).\nClaim36: This is reflected in the PCA and K-means performance here doing well on ground-truth accuracy.\nClaim37: CCS is similar, but more bimodal, sometimes finding the ground-truth, and sometimes the banana/shed feature.\nClaim38: For T5-11B (Figure 7) on IMDB and BoolQ we see a similar pattern of results to Chinchilla, though with lower accuracies.\nClaim39: On DBpedia, all of the results are around random chance, though logistic regression is able to solve the task, meaning this information is linearly encoded but perhaps not salient enough for the unsupervised methods to pick up.\nClaim40: T5-FLAN-XXL (Figure 8) shows more resistance to our modified prompt, suggesting fine-tuning hardens the activations in such a way that unsupervised learning can still recover knowledge.\nClaim41: For Chinchilla and T5, only one repetition is enough for the method to track the speaker\u2019s opinion.\nClaim42: T5-FLAN-XXL requires more repetitions, but eventually shows the same pattern.\nClaim43: Overall, we find our results are not that sensitive to layer, though often layer 30 is a good choice for both standard and sycophantic templates.\nClaim44: For T5-11B we see CCS, under both default and modified prompts, performs at about 60% on non-company questions, and much better on company questions.\nClaim45: PCA and K-means are similar, though with less variation amongst seeds (showing less bimodal behaviour).\nClaim46: Overall we do not see a major effect.\nClaim47: Burns et al. [9] claim that knowledge has special structure that few other features in an LLM are likely to satisfy and use this to motivate CCS.\nClaim48: Nevertheless, we find that CCS and PCA 8 make similar predictions.\nClaim49: There is higher agreement (top-line number) in all cases than what one would expect from independent methods (notated \u201cInd:\u201d) with the observed accuracies (shown in parentheses in the heatmap).\nClaim50: This supports the hypothesis of Emmons [16] and suggests that the consistency-condition does not do much.\nClaim51: But the fact that two methods with such different motivations behave similarly also supports the idea that results on current unsupervised methods may be predictive of future methods which have different motivations.\n\nReview: Weakness: More experiments on other LLMs are needed to further validate the claim.\n\nIt would be better to offer possible solutions to address the problems in existing unsupervised methods.\nQuestions: More experiments on other LLMs are needed to further validate the claim.\n\nIt would be better to offer some possible solutions to address the problems in existing unsupervised methods.\n\n\n\n"", ""id"": ""hGhLd2ByoR0"", ""pdf"": ""openreview.net/pdf?id=hGhLd2ByoR""}","47"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""The authors claim to separate the model parameters into integers and floating-point components. However, as far as I understand, this practice is not a novel contribution but rather a common approach in quantization."", ""Target claims"": [12], ""Reasoning"": ""The reviewer argues that the decoupling of model parameters into integers and floating points, as described in Claim12, is not a novel contribution and is considered a common approach in quantization. This criticism directly questions the originality and novelty of Claim12, which makes a specific assertion about the core idea of decoupleQ being the decoupling of model weights into these components.""}, {""Weakness span"": ""The authors acknowledge that their solution may not be optimal."", ""Target claims"": [5, 6], ""Reasoning"": ""This weakness suggests that the proposed solution might not reach the optimal solutions in practice. Claims 5 and 6 relate to the proposed optimization approach claiming robustness and hardware friendliness. The potential non-optimality of the solution introduces doubt about the efficacy and adequacy of the claims, which are premised on solving the constrained optimization problem effectively.""}]}, ""meta"": {""id"": ""h8goI8uPXM"", ""review_idx"": 0, ""title"": ""Title: decoupleQ: Towards 2-bit Post-Training Uniform Quantization via decoupling Parameters into Integer and Floating Points\nAbstract: Quantization emerges as one of the most promising compression technologies for deploying efficient large models in recent years. \nHowever, existing quantization schemes suffer from significant accuracy degradation at very low bits, or require some additional computational overhead when deployed, making it difficult to be applied to large-scale applications in industry.\nIn this paper, we propose decoupleQ, achieving a substantial increase in model accuracy, especially at very low bits.\n\ndecoupleQ abandons the traditional heuristic quantization paradigm and decouples the model parameters into integer and floating-point parts, then transforming the quantization problem into a mathematical constrained optimization problem, which is then solved alternatively by off-the-shelf solution methods. decoupleQ gets rid of any tricks for dealing with outliers, sensitive channels, etc., and focuses only on the basic optimization objective to achieve high model accuracy on extreme low bit quantization.\n Quantization via decoupleQ is linear and uniform, making it hardware-friendlier than non-uniform counterpart, and enabling the idea to be migrated to high-bit quantization to enhance its robustness. \n \ndecoupleQ has achieved comparable accuracy as fp16/bf16 for 2-bit quantization of large speech models in our company.\nThe code (including the W2 CUDA kernels) is attached and will be made public.\n\nTLDR:transform model quantization into a constrained optimization problem\n\nKeywords:quantization; large language model; optimization"", ""claims"": [""Claim1: In this paper, we propose decoupleQ, achieving a substantial increase in model accuracy, especially at very low bits."", ""Claim2: decoupleQ gets rid of any tricks for dealing with outliers, sensitive channels, etc., and focuses only on the basic optimization objective to achieve high model accuracy on extreme low bit quantization."", ""Claim3: Quantization via decoupleQ is linear and uniform, making it hardware-friendlier than non-uniform counterpart, and enabling the idea to be migrated to high-bit quantization to enhance its robustness."", ""Claim4: decoupleQ has achieved comparable accuracy as fp16/bf16 for 2-bit quantization of large speech models in our company."", ""Claim5: In order to alleviate the above pains in industry, we proposed decoupleQ, which completely abandons the traditional heuristic quantization paradigm and instead decouples the model parameters into integer and floating-point parts, then transforming the quantization problem into a mathematical constrained optimization problem, which is then solved alternatively by off-the-shelf solution methods."", ""Claim6: Quantization via decoupleQ is linear and uniform, making it hardware-friendlier than non-uniform counterpart, and enabling the idea to be migrated to high-bit quantization to enhance its robustness."", ""Claim7: decoupleQ implements 2-bit uniform quantization and achieves state-of-the-art accuracy in Llama67 1/2."", ""Claim8: Our main highlights are summarized as follows: Quantization can be roughly divided into Quantization Aware Training (QAT) and Post82 Training Quantization (PTQ)."", ""Claim9: In decoupleQ, we abandon patchwork solutions and transform the quantization into a principled traditional optimization problem by decoupling the model parameters into integer and floating-point parts."", ""Claim10: GPTQ does not optimize for the scale and zero point reduced by quantization."", ""Claim11: decoupleQ takes this idea a step further, optimizing the scales, zero points and norm 98 layers with supervised fine-tuning, while freezing the integer weights."", ""Claim12: The core idea of decoupleQ is to decouple the model weights into the integer part w and the floating-point part (s, z), with the integer part occupying most of the model\u2019s expressive power."", ""Claim13: The extensibility of the idea of decoupleQ is that we can freeze the integer part of the entire model, and use labeled data to train the (s, z) as well as other floating point parameters."", ""Claim14: The advantage of this is that on the one hand, it can further improve the accuracy of the model, on the other hand, it can fit specific downstream sub-tasks while maintaining the generalization ability of the model."", ""Claim15: The problem 6 is not easy to solve because of the non-convex constraint wi \u2208 Z."", ""Claim16: After solving problem 6, we obtain a solution for the layer-wise minimization stage and a reasonable model accuracy."", ""Claim17: But minimizing the \u2113 2 loss at the layer level does not necessarily lead to the minimizing the \u2113 2 loss at the block level."", ""Claim18: We found that the model accuracy can be further improved via optimization 2."", ""Claim19: BRECQ (18) also shows that block-reconstruction results in a better model accuracy than layer-reconstruction."", ""Claim20: In this experiment, we have to choose the second level approxima250tion(11) because the intolerable runtime of solving the first(10). For a fair comparison, the calibration dataset contains 128 samples, although a larger calibration dataset will result in stronger results."", ""Claim21: We can see that decoupleQ outperforms others almost in all settings, although we use a weaker approximation(11) to save time."", ""Claim22: First of all, in the blue line, we use only the layer-wise minimization of decooupleQ to quantize the model."", ""Claim23: After the quantization is finished, in the red line, we use the labeled dataset with the common 1.2 millions images to fine-tune all the ( s, z ) and norm layers for one epoch, with the integer part being frozen."", ""Claim24: Fig. 2 clearly indicates the following conclusions: 1. As the number of iterations K increases, the model accuracy increases almost monotonically; 2. When K > 4 , model accuracy via the first approximation(10) is better than via the second(11). This is to be expected, since the second approximation(11) drops the constraint \u03b1 \u2264 w i \u2264 \u03b2 , leading to a looser approximation; 3. By the supervised fine-tuning (sft), the model accuracy is further improved."", ""Claim25: It shows that, when K > 1 , solving approximation 10 yields better model accuracy than approximation 11."", ""Claim26: But all PPLs are inferior to when the second-level approximation (11) is used."", ""Claim27: As K increases, the loss decreases strictly monotonically, and when K > 2 , the loss falls below the case when the approximation 11 is used."", ""Claim28: This suggests that the correlation between PPL and loss is perhaps weak, and we will investigate this in the future."", ""Claim29: For the first risk, we find experimentally that the correlation between Top-1 and the loss is strong in the Imagenet classification task; however, the correlation between PPL and the loss is slightly weaker in LLM."", ""Claim30: In our practice, we found that the accuracy of quantization models can rise monotonically with the increase of the size of the calibration dataset, especially in W2 quantization, but the runtime of quantization rise as well."", ""Claim31: The idea of decoupleQ is helpful for the adaptation of large model to downstream sub-task.""], ""review"": ""Review: Weakness: 1. The paper's writing lacks cohesion and clarity regarding its ultimate goal. The paper also has several spelling mistakes.\n2. The authors claim to separate the model parameters into integers and floating-point components. However, as far as I understand, this practice is not a novel contribution but rather a common approach in quantization.\n3. They address a portion of the optimization problem using GPTQ and another portion similar to BRECQ.\n4. The authors acknowledge that their solution may not be optimal. \n5. The quantization process in decoupleQ can be more time-consuming than other methods.\nQuestions: 1. The paper mentions achieving state-of-the-art accuracy in Llama-1/2. It would be helpful to see a more detailed comparison with other state-of-the-art quantization methods on this specific model.\n2. The authors could elaborate on the potential impact of the weak correlation between PPL (perplexity) and loss in LLMs. How might this affect the practical application of decoupleQ for LLM quantization?\n3. The paper briefly mentions the risk of overfitting. Providing more insights into mitigation strategies for this risk would be beneficial, especially when dealing with underdetermined matrices.\n4. Given the longer runtime compared to other methods, it would be interesting to see a more comprehensive analysis of the trade-off between quantization time and model accuracy.\n5. The authors could consider adding a section on future work, outlining potential directions for further research and improvement of the decoupleQ method.\n\n""}, ""review"": ""Title: decoupleQ: Towards 2-bit Post-Training Uniform Quantization via decoupling Parameters into Integer and Floating Points\nAbstract: Quantization emerges as one of the most promising compression technologies for deploying efficient large models in recent years. \nHowever, existing quantization schemes suffer from significant accuracy degradation at very low bits, or require some additional computational overhead when deployed, making it difficult to be applied to large-scale applications in industry.\nIn this paper, we propose decoupleQ, achieving a substantial increase in model accuracy, especially at very low bits.\n\ndecoupleQ abandons the traditional heuristic quantization paradigm and decouples the model parameters into integer and floating-point parts, then transforming the quantization problem into a mathematical constrained optimization problem, which is then solved alternatively by off-the-shelf solution methods. decoupleQ gets rid of any tricks for dealing with outliers, sensitive channels, etc., and focuses only on the basic optimization objective to achieve high model accuracy on extreme low bit quantization.\n Quantization via decoupleQ is linear and uniform, making it hardware-friendlier than non-uniform counterpart, and enabling the idea to be migrated to high-bit quantization to enhance its robustness. \n \ndecoupleQ has achieved comparable accuracy as fp16/bf16 for 2-bit quantization of large speech models in our company.\nThe code (including the W2 CUDA kernels) is attached and will be made public.\n\nClaim1: In this paper, we propose decoupleQ, achieving a substantial increase in model accuracy, especially at very low bits.\nClaim2: decoupleQ gets rid of any tricks for dealing with outliers, sensitive channels, etc., and focuses only on the basic optimization objective to achieve high model accuracy on extreme low bit quantization.\nClaim3: Quantization via decoupleQ is linear and uniform, making it hardware-friendlier than non-uniform counterpart, and enabling the idea to be migrated to high-bit quantization to enhance its robustness.\nClaim4: decoupleQ has achieved comparable accuracy as fp16/bf16 for 2-bit quantization of large speech models in our company.\nClaim5: In order to alleviate the above pains in industry, we proposed decoupleQ, which completely abandons the traditional heuristic quantization paradigm and instead decouples the model parameters into integer and floating-point parts, then transforming the quantization problem into a mathematical constrained optimization problem, which is then solved alternatively by off-the-shelf solution methods.\nClaim6: Quantization via decoupleQ is linear and uniform, making it hardware-friendlier than non-uniform counterpart, and enabling the idea to be migrated to high-bit quantization to enhance its robustness.\nClaim7: decoupleQ implements 2-bit uniform quantization and achieves state-of-the-art accuracy in Llama67 1/2.\nClaim8: Our main highlights are summarized as follows: Quantization can be roughly divided into Quantization Aware Training (QAT) and Post82 Training Quantization (PTQ).\nClaim9: In decoupleQ, we abandon patchwork solutions and transform the quantization into a principled traditional optimization problem by decoupling the model parameters into integer and floating-point parts.\nClaim10: GPTQ does not optimize for the scale and zero point reduced by quantization.\nClaim11: decoupleQ takes this idea a step further, optimizing the scales, zero points and norm 98 layers with supervised fine-tuning, while freezing the integer weights.\nClaim12: The core idea of decoupleQ is to decouple the model weights into the integer part w and the floating-point part (s, z), with the integer part occupying most of the model\u2019s expressive power.\nClaim13: The extensibility of the idea of decoupleQ is that we can freeze the integer part of the entire model, and use labeled data to train the (s, z) as well as other floating point parameters.\nClaim14: The advantage of this is that on the one hand, it can further improve the accuracy of the model, on the other hand, it can fit specific downstream sub-tasks while maintaining the generalization ability of the model.\nClaim15: The problem 6 is not easy to solve because of the non-convex constraint wi \u2208 Z.\nClaim16: After solving problem 6, we obtain a solution for the layer-wise minimization stage and a reasonable model accuracy.\nClaim17: But minimizing the \u2113 2 loss at the layer level does not necessarily lead to the minimizing the \u2113 2 loss at the block level.\nClaim18: We found that the model accuracy can be further improved via optimization 2.\nClaim19: BRECQ (18) also shows that block-reconstruction results in a better model accuracy than layer-reconstruction.\nClaim20: In this experiment, we have to choose the second level approxima250tion(11) because the intolerable runtime of solving the first(10). For a fair comparison, the calibration dataset contains 128 samples, although a larger calibration dataset will result in stronger results.\nClaim21: We can see that decoupleQ outperforms others almost in all settings, although we use a weaker approximation(11) to save time.\nClaim22: First of all, in the blue line, we use only the layer-wise minimization of decooupleQ to quantize the model.\nClaim23: After the quantization is finished, in the red line, we use the labeled dataset with the common 1.2 millions images to fine-tune all the ( s, z ) and norm layers for one epoch, with the integer part being frozen.\nClaim24: Fig. 2 clearly indicates the following conclusions: 1. As the number of iterations K increases, the model accuracy increases almost monotonically; 2. When K > 4 , model accuracy via the first approximation(10) is better than via the second(11). This is to be expected, since the second approximation(11) drops the constraint \u03b1 \u2264 w i \u2264 \u03b2 , leading to a looser approximation; 3. By the supervised fine-tuning (sft), the model accuracy is further improved.\nClaim25: It shows that, when K > 1 , solving approximation 10 yields better model accuracy than approximation 11.\nClaim26: But all PPLs are inferior to when the second-level approximation (11) is used.\nClaim27: As K increases, the loss decreases strictly monotonically, and when K > 2 , the loss falls below the case when the approximation 11 is used.\nClaim28: This suggests that the correlation between PPL and loss is perhaps weak, and we will investigate this in the future.\nClaim29: For the first risk, we find experimentally that the correlation between Top-1 and the loss is strong in the Imagenet classification task; however, the correlation between PPL and the loss is slightly weaker in LLM.\nClaim30: In our practice, we found that the accuracy of quantization models can rise monotonically with the increase of the size of the calibration dataset, especially in W2 quantization, but the runtime of quantization rise as well.\nClaim31: The idea of decoupleQ is helpful for the adaptation of large model to downstream sub-task.\n\nReview: Weakness: 1. The paper's writing lacks cohesion and clarity regarding its ultimate goal. The paper also has several spelling mistakes.\n2. The authors claim to separate the model parameters into integers and floating-point components. However, as far as I understand, this practice is not a novel contribution but rather a common approach in quantization.\n3. They address a portion of the optimization problem using GPTQ and another portion similar to BRECQ.\n4. The authors acknowledge that their solution may not be optimal. \n5. The quantization process in decoupleQ can be more time-consuming than other methods.\nQuestions: 1. The paper mentions achieving state-of-the-art accuracy in Llama-1/2. It would be helpful to see a more detailed comparison with other state-of-the-art quantization methods on this specific model.\n2. The authors could elaborate on the potential impact of the weak correlation between PPL (perplexity) and loss in LLMs. How might this affect the practical application of decoupleQ for LLM quantization?\n3. The paper briefly mentions the risk of overfitting. Providing more insights into mitigation strategies for this risk would be beneficial, especially when dealing with underdetermined matrices.\n4. Given the longer runtime compared to other methods, it would be interesting to see a more comprehensive analysis of the trade-off between quantization time and model accuracy.\n5. The authors could consider adding a section on future work, outlining potential directions for further research and improvement of the decoupleQ method.\n\n\n\n"", ""id"": ""h8goI8uPXM0"", ""pdf"": ""openreview.net/pdf?id=h8goI8uPXM""}","48"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""- The failure of LLMs4PLAN-GPT3.5 compared to the phenomenal success of LLMs4Plan-GPT4 is somewhat unexpected and undermines the results of the paper."", ""Target claims"": [10, 11], ""Reasoning"": ""This weakness span explicitly questions the results' robustness by highlighting the unexpected disparity between GPT3.5's failure and GPT4's success. Claim10 discusses GPT3.5's limited success result and domains, whereas Claim11 highlights GPT4's improvements over GPT3.5. This juxtaposition isn't adequately explained in the paper, thus casting doubt on Claims10 and 11 about different models' performance, potentially undermining both claims.""}]}, ""meta"": {""id"": ""egGFHCFPiU"", ""review_idx"": 0, ""title"": ""Title: On the Roles of LLMs in Planning: Embedding LLMs into Planning Graphs\nAbstract: Plan synthesis aims to generate a course of actions or policies to transit given initial states to goal states, provided domain models that could be designed by experts or learnt from training data or interactions with the world.\nIntrigued by the claims of emergent planning capabilities in large language models (LLMs), works have been proposed to investigate the planning effectiveness of LLMs, without considering any utilization of off-the-shelf planning techniques in LLMs. \nIn this paper, we aim to further study the insight of the planning capability of LLMs by investigating the roles of LLMs in off-the-shelf planning frameworks. To do this, we investigate the effectiveness of embedding LLMs into one of the well-known planning frameworks, graph-based planning, proposing a novel LLMs-based planning framework with LLMs embedded in two levels of planning graphs, i.e., mutual constraints generation level and constraints solving level. We empirically exhibit the effectiveness of our proposed framework in various planning domains.\n\nTLDR:\n\nKeywords:Planning, LLMs"", ""claims"": [""Claim1: An interesting result shown by [15] is when taking the solution generated by LLMs, which is incorrect, as a seed plan to be repaired by an off-the-shelf planner, e.g., LPG [4], a significant improvement in search steps can be attained over the result when an empty plan provided as a seed plan for the planner."", ""Claim2: This indicates that LLMs can indeed provide some helpful information (e.g., in some sense of heuristics) for planning, even though they cannot solve planning problems solely."", ""Claim3: In this work we consider classical planning problems specified in the form of STRIPS [3]."", ""Claim4: Similar ideas can be extended into more expressive planning language such as PDDL [5]."", ""Claim5: In the experiment, we evaluate LLMs4Plan in ten planning domains with different scenarios, including gripper, miconic, logistics, movie, blocks, satellite, zenotravel, driverlog, woodworking and openstacks."", ""Claim6: To demonstrate the effectiveness of our LLMs4Plan approach, we designed five sets of comparison experiments."", ""Claim7: We compared our LLMs4Plan approach with four other methods."", ""Claim8: In the experimental framework described, we employed three distinct metrics to assess the efficacy of various methodologies: the problem-solving success rate, the cumulative count of expansion actions and the node count for backtracking in Depth-First Search (DFS)."", ""Claim9: Regarding the number of nodes for backtracking DFS, our analysis was confined to data from the GP and LLMs4Plan -GPT4 methods."", ""Claim10: GPT3.5 exhibits competence primarily in resolving simple problems with short action sequence lengths, such as in the movie domain, while its success rates are notably low in other domains."", ""Claim11: GPT4 demonstrates substantial improvements in abilities compared to GPT3.5, particularly in reasoning skills and decision-making involving long action sequences."", ""Claim12: GP shows an enhanced success rate in certain domains."", ""Claim13: The specific settings for the robustness experiments are detailed more extensively in section A.2 of the appendix."", ""Claim14: By integrating GPT4 with graph planning, LLMs4Plan can effectively generate more optimal action sequences."", ""Claim15: Besides planning success rates, our method significantly improves search efficiency compared to GP algorithms."", ""Claim16: We drastically reduce the cost of search nodes, achieving an exponential level of optimization."", ""Claim17: Our comparative experiments in multiple domains demonstrated the efficacy of our LLMs4Plan in significantly enhancing the problem-solving capabilities of graph planning algorithms."", ""Claim18: Notably, LLMs4Plan boosts not just the success rate of problem resolution but also markedly enhances search efficiency and substantially reduces computational complexity."", ""Claim19: The runtime of LLMs4Plan is currently hindered by multiple LLMs calls."", ""Claim20: While our method requires multiple LLMs calls, it provides substantially improved results.""], ""review"": ""Review: Weakness: - Multiple missing experiments and discussions severely undermine the results of the paper.\n    - It is not clearly motivated why experiments with corrupted pddl domain files are interesting. This was introduced quite suddenly in the *results section* (lines 261-262) without enough details and without providing motivation.\n    - The paper is missing important discussion and experiments about the trade-off between the hybrid approach and the classic GP algorithm. Experiments with valid pddl domain files are not included, which could have alleviate it.  \n    - The effect of hyperparameters on the results, such as the number of iterations (N) in Algorithm 1, is not discussed.\n    - The failure of LLMs4PLAN-GPT3.5 compared to the phenomenal success of LLMs4Plan-GPT4 is somewhat unexpected and undermines the results of the paper.\n- Multiple details are missing regarding the experimental setups. (see questions below)\n- The paper's writing needs to be improved. (see suggestions below)\nQuestions: **Suggestions**\n- Most importantly, I would like to see experiments with valid pddl files. While the GP algorithm would receive 100% success rate, I expected to see a graph of success rate (y-axis) compared to the number of nodes explored (x-axis). Such a graph would describe **a trade-off** between statistically using a LLM and using an exhaustive algorithm, such as GraphPlan.\n    - It is possible that this is included in Table 3, but I don't understand if it includes the corrupted domain files. A graph which includes success rate would be clearer.\n- Clearly describe and motivate the pddl data corruption in the experimental setup.\n- Writing\n    - While I happen to be well-versed with the GRAPHPLAN algorithm, I am not sure that enough introduction has been provided, as it is only briefly mentioned in lines 44 and 67.\n    - \u201c3 Our LLMs4Plan approach\u201d does not properly introduce the algorithm before discussing it. Notations, such as N, are not defined. Terms, such as planning graphs and mutual constraints, are used but not explained until reading 3.1 and 3.2. Concepts, such as pruning, should be formally introduced before discussing \u201cpruning possibility\u201d (line 109).\n    - subsection 3.2:\n         - this subsection is not part of your algorithm, but part of graphplan. related to my previous notes, I think this should not be in section 3, but properly explained earlier.\n         - add citations to support the mutual exclusion constraints names (i.e., inconsistent effects, interference, competing needs).\n     - subsection 4.3. writing could be improved. I was initially confused about the location of the ablation results table.\n\n**Questions**\n\n- Data corruption of pddl domain files: Please provide details about the corruption, such as % files corrupted. It seems that this is a very high percentage, if this is the main reason that the GP method gets low resulsts (lines 261-262).\n- Please provide details about the hyperparameters influence on the results, such as the number of iterations N and the number of layers K.\n- In Table 2, where we compare \u201cnumber of nodes required for searching\u201d. Which iteration do we measure? Do we ignore the fact that there could have been multiple iterations before the successful one?\n- line 189 - \u201cTen problems are randomly selected for each domain\u201d - from which corpus?\n- Statistics about the pddl problem files are missing. How long are the plans from initial state to goal?\n\n""}, ""review"": ""Title: On the Roles of LLMs in Planning: Embedding LLMs into Planning Graphs\nAbstract: Plan synthesis aims to generate a course of actions or policies to transit given initial states to goal states, provided domain models that could be designed by experts or learnt from training data or interactions with the world.\nIntrigued by the claims of emergent planning capabilities in large language models (LLMs), works have been proposed to investigate the planning effectiveness of LLMs, without considering any utilization of off-the-shelf planning techniques in LLMs. \nIn this paper, we aim to further study the insight of the planning capability of LLMs by investigating the roles of LLMs in off-the-shelf planning frameworks. To do this, we investigate the effectiveness of embedding LLMs into one of the well-known planning frameworks, graph-based planning, proposing a novel LLMs-based planning framework with LLMs embedded in two levels of planning graphs, i.e., mutual constraints generation level and constraints solving level. We empirically exhibit the effectiveness of our proposed framework in various planning domains.\n\nClaim1: An interesting result shown by [15] is when taking the solution generated by LLMs, which is incorrect, as a seed plan to be repaired by an off-the-shelf planner, e.g., LPG [4], a significant improvement in search steps can be attained over the result when an empty plan provided as a seed plan for the planner.\nClaim2: This indicates that LLMs can indeed provide some helpful information (e.g., in some sense of heuristics) for planning, even though they cannot solve planning problems solely.\nClaim3: In this work we consider classical planning problems specified in the form of STRIPS [3].\nClaim4: Similar ideas can be extended into more expressive planning language such as PDDL [5].\nClaim5: In the experiment, we evaluate LLMs4Plan in ten planning domains with different scenarios, including gripper, miconic, logistics, movie, blocks, satellite, zenotravel, driverlog, woodworking and openstacks.\nClaim6: To demonstrate the effectiveness of our LLMs4Plan approach, we designed five sets of comparison experiments.\nClaim7: We compared our LLMs4Plan approach with four other methods.\nClaim8: In the experimental framework described, we employed three distinct metrics to assess the efficacy of various methodologies: the problem-solving success rate, the cumulative count of expansion actions and the node count for backtracking in Depth-First Search (DFS).\nClaim9: Regarding the number of nodes for backtracking DFS, our analysis was confined to data from the GP and LLMs4Plan -GPT4 methods.\nClaim10: GPT3.5 exhibits competence primarily in resolving simple problems with short action sequence lengths, such as in the movie domain, while its success rates are notably low in other domains.\nClaim11: GPT4 demonstrates substantial improvements in abilities compared to GPT3.5, particularly in reasoning skills and decision-making involving long action sequences.\nClaim12: GP shows an enhanced success rate in certain domains.\nClaim13: The specific settings for the robustness experiments are detailed more extensively in section A.2 of the appendix.\nClaim14: By integrating GPT4 with graph planning, LLMs4Plan can effectively generate more optimal action sequences.\nClaim15: Besides planning success rates, our method significantly improves search efficiency compared to GP algorithms.\nClaim16: We drastically reduce the cost of search nodes, achieving an exponential level of optimization.\nClaim17: Our comparative experiments in multiple domains demonstrated the efficacy of our LLMs4Plan in significantly enhancing the problem-solving capabilities of graph planning algorithms.\nClaim18: Notably, LLMs4Plan boosts not just the success rate of problem resolution but also markedly enhances search efficiency and substantially reduces computational complexity.\nClaim19: The runtime of LLMs4Plan is currently hindered by multiple LLMs calls.\nClaim20: While our method requires multiple LLMs calls, it provides substantially improved results.\n\nReview: Weakness: - Multiple missing experiments and discussions severely undermine the results of the paper.\n    - It is not clearly motivated why experiments with corrupted pddl domain files are interesting. This was introduced quite suddenly in the *results section* (lines 261-262) without enough details and without providing motivation.\n    - The paper is missing important discussion and experiments about the trade-off between the hybrid approach and the classic GP algorithm. Experiments with valid pddl domain files are not included, which could have alleviate it.  \n    - The effect of hyperparameters on the results, such as the number of iterations (N) in Algorithm 1, is not discussed.\n    - The failure of LLMs4PLAN-GPT3.5 compared to the phenomenal success of LLMs4Plan-GPT4 is somewhat unexpected and undermines the results of the paper.\n- Multiple details are missing regarding the experimental setups. (see questions below)\n- The paper's writing needs to be improved. (see suggestions below)\nQuestions: **Suggestions**\n- Most importantly, I would like to see experiments with valid pddl files. While the GP algorithm would receive 100% success rate, I expected to see a graph of success rate (y-axis) compared to the number of nodes explored (x-axis). Such a graph would describe **a trade-off** between statistically using a LLM and using an exhaustive algorithm, such as GraphPlan.\n    - It is possible that this is included in Table 3, but I don't understand if it includes the corrupted domain files. A graph which includes success rate would be clearer.\n- Clearly describe and motivate the pddl data corruption in the experimental setup.\n- Writing\n    - While I happen to be well-versed with the GRAPHPLAN algorithm, I am not sure that enough introduction has been provided, as it is only briefly mentioned in lines 44 and 67.\n    - \u201c3 Our LLMs4Plan approach\u201d does not properly introduce the algorithm before discussing it. Notations, such as N, are not defined. Terms, such as planning graphs and mutual constraints, are used but not explained until reading 3.1 and 3.2. Concepts, such as pruning, should be formally introduced before discussing \u201cpruning possibility\u201d (line 109).\n    - subsection 3.2:\n         - this subsection is not part of your algorithm, but part of graphplan. related to my previous notes, I think this should not be in section 3, but properly explained earlier.\n         - add citations to support the mutual exclusion constraints names (i.e., inconsistent effects, interference, competing needs).\n     - subsection 4.3. writing could be improved. I was initially confused about the location of the ablation results table.\n\n**Questions**\n\n- Data corruption of pddl domain files: Please provide details about the corruption, such as % files corrupted. It seems that this is a very high percentage, if this is the main reason that the GP method gets low resulsts (lines 261-262).\n- Please provide details about the hyperparameters influence on the results, such as the number of iterations N and the number of layers K.\n- In Table 2, where we compare \u201cnumber of nodes required for searching\u201d. Which iteration do we measure? Do we ignore the fact that there could have been multiple iterations before the successful one?\n- line 189 - \u201cTen problems are randomly selected for each domain\u201d - from which corpus?\n- Statistics about the pddl problem files are missing. How long are the plans from initial state to goal?\n\n\n\n"", ""id"": ""egGFHCFPiU0"", ""pdf"": ""openreview.net/pdf?id=egGFHCFPiU""}","49"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""The overall approach does not appear to be novel, as it closely resembles T-Patcher. Both iReVa and T-Patcher rely on inserting neurons for editing and using neuron activations to control when to run the patch/adopter. Furthermore, analysis of the editing approach across different layers reveals the same pattern as discussed in the T-Patcher paper which involves adding additional model components in the final layer for optimal results."", ""Target claims"": [1, 8, 37], ""Reasoning"": ""The weakness span states that iReVa's novelty is in question because its approach closely resembles T-Patcher, particularly in terms of editing using neuron activations and inserting neurons. This questions the claim of iReVa having a 'better interpretability and stronger capacity for carrying traceable edits' (Claim1, Claim8, Claim37), as the similarity with T-Patcher suggests that the interpretability and traceability may not be 'better' but rather similar to an existing method.""}, {""Weakness span"": ""Although T-Patcher performs editing in batches, it still uses a single patch of neurons for each edit, making its editing similarly traceable. Thus, the paper's claim of a 'stronger capacity for carrying traceable edits' seems unfounded."", ""Target claims"": [1, 8, 37], ""Reasoning"": ""This span directly challenges the claims regarding iReVa having a 'stronger capacity for carrying traceable edits' (Claim1, Claim8, Claim37) by arguing that T-Patcher, which also performs traceable editing despite using a single patch per edit, suggests that iReVa's traceability capacity may not be as superior as claimed.""}, {""Weakness span"": ""The Edit Withdrawal Test section is hard to understand. How exactly was the experiment conducted? Were all edits removed or only a limited set? Detailed experimentations for this section are needed as it is the only use case of traceability explored in the paper."", ""Target claims"": [3, 10, 13], ""Reasoning"": ""The review criticizes the lack of clarity in the description of the Edit Withdrawal Test, suggesting that insufficient detail is provided to support the claims of conducting this test (Claim3, Claim10, Claim13). This calls into question the validity of the claims that the authors successfully performed the first attempt at such a test, as the details of what constitutes a successful execution are not clearly presented.""}, {""Weakness span"": ""Editing techniques that rely on code books with playback vectors e.g. GRACE would allow for edits to be removed. The authors should make it clear that the withdrawal test is not possible for the editing techniques that they have chosen for comparison."", ""Target claims"": [], ""Reasoning"": ""This comment highlights a potential oversight in the comparison and methodology related to the editing techniques, particularly when discussing traceability and edit withdrawal capabilities. However, it doesn't directly target specific claims from the paper.""}]}, ""meta"": {""id"": ""cO1llRY2Br"", ""review_idx"": 0, ""title"": ""Title: Initializing and Retrofitting Key-Value Adaptors for Traceable Model Editing\nAbstract: As the insight of knowledge storage in language models deepens, the ability to perform CRUD (Create, Read, Update, Delete) operations on language models becomes increasingly indispensable for satisfying the demands of managing rapidly updating knowledge. Considering the high cost of fine-tuning language models, model editing methods with low cost are usually required to manipulate models' knowledge. Evident suggests that modules carrying knowledge in a Transformer module are primarily the MLP blocks, thus we propose \\textbf{iReVa}, a method that explicitly initializes and retrofits key-value pairs into MLP blocks to construct a new mapping of a piece of knowledge without damaging the irrelevant knowledge. In comparison to existing methods, iReVa reveals better interpretability and stronger capacity for carrying traceable edits. Experiment results on series of GPT series models show our prominent performance on edit success and generalization without influencing specificity. We also perform the first attempt at conducting knowledge withdrawal test of iReVa. Our codes are available at github.com/thartvigsen/grace.\n\nTLDR:\n\nKeywords:natural language processing, model editing, language model, key-value adaptor"", ""claims"": [""Claim1: In comparison to existing methods, iReVa reveals better interpretability and stronger capacity for carrying traceable edits."", ""Claim2: Experiment results on series of GPT series models show our prominent performance on edit success and generalization without influencing specificity."", ""Claim3: We also perform the first attempt at conducting knowledge withdrawal test of iReVa."", ""Claim4: Experimental results on series of GPT-like models show that iReVa is able to outperform the SOTA results by around 9% and 6% average score improvement on zsRE-10K and PARAREL-10K, respectively."", ""Claim5: Moreover, iReVa is able to perform knowledge withdrawal in almost perfect condition."", ""Claim6: Our method outperforms recent baselines on model editing tasks with noticeable margins based on various evaluation metrics."", ""Claim7: We validate the interpretability and generalization capabilities of our method by conducting further analysis such as knowledge withdrawal test and generalization test."", ""Claim8: In comparison to existing methods, iReVa reveals better interpretability and stronger capacity for carrying traceable edits."", ""Claim9: Experiment results on series of GPT series models show our prominent performance on edit success and generalization without influencing specificity."", ""Claim10: We also perform the first attempt at conducting knowledge withdrawal test of iReVa."", ""Claim11: In comparison to existing methods, iReVa reveals better interpretability and stronger capacity for carrying traceable edits."", ""Claim12: Experiment results on series of GPT series models show our prominent performance on edit success and generalization without influencing specificity."", ""Claim13: We also perform the first attempt at conducting knowledge withdrawal test of iReVa."", ""Claim14: We perform extensive experiments on two modeling editing tasks: zsRE [8] is a commonly used model editing tasks derived from question-answering benchmark."", ""Claim15: To apply PARAREL in model editing task, we selected those sentences that end with '[MASK]' token to conform to the format of next-token-prediction."", ""Claim16: As we can see, iReVa outperforms all baselines on average scores with noticeable margins."", ""Claim17: Even without retrofitting, our method is able to outperform the SOTA results by around 9% and 6% average score improvement on zsRE-10K and PARAREL-10K, respectively."", ""Claim18: FT achieves good results on ES and PS, this indicates that fine-tuning is simple but effective to inject knowledge but it could easily distract the irrelevant knowledge, resulting in a poor NS."", ""Claim19: Whereas other baselines can not guarantee the editing success in a batch, resulting in poor ES and PS."", ""Claim20: In comparison, iReVa achieves impressive results on all the evaluation metrics."", ""Claim21: It achieves close to 100% ES without detriment to the original NS."", ""Claim22: We observe a slight improvement from the results of iReVa to iReVa+ L on zsRE-10K dataset, it verifies our rationale deduce for the initialization of key-value pairs."", ""Claim23: However, the improvement brought by fine-tuning is not maintained on PARAREL-10K, we suspect this is because the involvement of irrelevant knowledge brings in little unexpected noise with possibility."", ""Claim24: This provides a chance to conduct an edit withdrawal test."", ""Claim25: Specifically, we test, after editing on 10 K examples, if iReVa is able to withdraw certain edits and recover the original output from the base model without much loss."", ""Claim26: The results which are close to 100% proves that iReVa can explicitly manipulate the activation of knowledge neurons and easily withdraw the updated knowledge."", ""Claim27: It is worth noting that this test is not applicable to any other editing methods as their edited parameters are untraceable."", ""Claim28: This is the first attempt at conducting more flexible knowledge editing."", ""Claim29: We also find that the influence of max-pooling is significant, which may attribute to noisy data added by a large amount of active but irrelevant knowledge neurons."", ""Claim30: Besides, excluding L rec will lead to an observable drop on the three metrics because we discord the effective initialization on \u02c6K and \u02c6V."", ""Claim31: Even though MEMIT achieves good performance in NS when the edit happens in lower layers, overall iReVa outperforms the baselines regarding the comprehensive evaluation metrics."", ""Claim32: From the figure, we observe that iReVa can achieve the best average score on both LMs, which shows its general effect."", ""Claim33: As we can see, iReVa is robust to the number of edit in the batch. It consistently surpasses the other baselines when dealing with the various number of edits."", ""Claim34: In this paper, we propose iReVa, a model editing method with traceable knowledge storage, which inserts edit key-value adaptor into the MLP module of a transformer model explicitly."", ""Claim35: iReVa displays prominent abilities of edit success, generalization and specificity and outperforms baselines with an observable margin."", ""Claim36: Besides, iReVa first successfully demonstrates its capacity on the knowledge withdrawal."", ""Claim37: In comparison to existing methods, iReVa reveals better interpretability and stronger capacity for carrying traceable edits."", ""Claim38: Experiment results on series of GPT series models show our prominent performance on edit success and generalization without influencing specificity."", ""Claim39: Empirically, we believe PARAREL is harder than zsRE because the average token length of edit target is shorter, thus model can\u2019t give more empirical predictions based on given prefix of the target, which is mentioned in A.5.""], ""review"": ""Review: Weakness: \u2022\tThe overall approach does not appear to be novel, as it closely resembles T-Patcher. Both iReVa and T-Patcher rely on inserting neurons for editing and using neuron activations to control when to run the patch/adopter. Furthermore, analysis of the editing approach across different layers reveals the same pattern as discussed in the T-Patcher paper which involves adding additional model components in the final layer for optimal results.\n\n\u2022\tExperiments with T-Patcher are missing from the comparisons to the existing methods section. Given its similarity, T-Patcher should be included for comparison.\n\n\u2022\tAlthough T-Patcher performs editing in batches, it still uses a single patch of neurons for each edit, making its editing similarly traceable. Thus, the paper's claim of a \""stronger capacity for carrying traceable edits\"" seems unfounded.\n\n\u2022\tThe Edit Withdrawal Test section is hard to understand. How exactly was the experiment conducted? Were all edits removed or only a limited set? Detailed experimentations for this section are needed as it is the only use case of traceability explored in the paper.\n\n\u2022\tEditing techniques that rely on code books with playback vectors e.g. GRACE would allow for edits to be removed. The authors should make it clear that the withdrawal test is not possible for the editing techniques that they have chosen for comparison.\nQuestions: N/A\n\n""}, ""review"": ""Title: Initializing and Retrofitting Key-Value Adaptors for Traceable Model Editing\nAbstract: As the insight of knowledge storage in language models deepens, the ability to perform CRUD (Create, Read, Update, Delete) operations on language models becomes increasingly indispensable for satisfying the demands of managing rapidly updating knowledge. Considering the high cost of fine-tuning language models, model editing methods with low cost are usually required to manipulate models' knowledge. Evident suggests that modules carrying knowledge in a Transformer module are primarily the MLP blocks, thus we propose \\textbf{iReVa}, a method that explicitly initializes and retrofits key-value pairs into MLP blocks to construct a new mapping of a piece of knowledge without damaging the irrelevant knowledge. In comparison to existing methods, iReVa reveals better interpretability and stronger capacity for carrying traceable edits. Experiment results on series of GPT series models show our prominent performance on edit success and generalization without influencing specificity. We also perform the first attempt at conducting knowledge withdrawal test of iReVa. Our codes are available at github.com/thartvigsen/grace.\n\nClaim1: In comparison to existing methods, iReVa reveals better interpretability and stronger capacity for carrying traceable edits.\nClaim2: Experiment results on series of GPT series models show our prominent performance on edit success and generalization without influencing specificity.\nClaim3: We also perform the first attempt at conducting knowledge withdrawal test of iReVa.\nClaim4: Experimental results on series of GPT-like models show that iReVa is able to outperform the SOTA results by around 9% and 6% average score improvement on zsRE-10K and PARAREL-10K, respectively.\nClaim5: Moreover, iReVa is able to perform knowledge withdrawal in almost perfect condition.\nClaim6: Our method outperforms recent baselines on model editing tasks with noticeable margins based on various evaluation metrics.\nClaim7: We validate the interpretability and generalization capabilities of our method by conducting further analysis such as knowledge withdrawal test and generalization test.\nClaim8: In comparison to existing methods, iReVa reveals better interpretability and stronger capacity for carrying traceable edits.\nClaim9: Experiment results on series of GPT series models show our prominent performance on edit success and generalization without influencing specificity.\nClaim10: We also perform the first attempt at conducting knowledge withdrawal test of iReVa.\nClaim11: In comparison to existing methods, iReVa reveals better interpretability and stronger capacity for carrying traceable edits.\nClaim12: Experiment results on series of GPT series models show our prominent performance on edit success and generalization without influencing specificity.\nClaim13: We also perform the first attempt at conducting knowledge withdrawal test of iReVa.\nClaim14: We perform extensive experiments on two modeling editing tasks: zsRE [8] is a commonly used model editing tasks derived from question-answering benchmark.\nClaim15: To apply PARAREL in model editing task, we selected those sentences that end with '[MASK]' token to conform to the format of next-token-prediction.\nClaim16: As we can see, iReVa outperforms all baselines on average scores with noticeable margins.\nClaim17: Even without retrofitting, our method is able to outperform the SOTA results by around 9% and 6% average score improvement on zsRE-10K and PARAREL-10K, respectively.\nClaim18: FT achieves good results on ES and PS, this indicates that fine-tuning is simple but effective to inject knowledge but it could easily distract the irrelevant knowledge, resulting in a poor NS.\nClaim19: Whereas other baselines can not guarantee the editing success in a batch, resulting in poor ES and PS.\nClaim20: In comparison, iReVa achieves impressive results on all the evaluation metrics.\nClaim21: It achieves close to 100% ES without detriment to the original NS.\nClaim22: We observe a slight improvement from the results of iReVa to iReVa+ L on zsRE-10K dataset, it verifies our rationale deduce for the initialization of key-value pairs.\nClaim23: However, the improvement brought by fine-tuning is not maintained on PARAREL-10K, we suspect this is because the involvement of irrelevant knowledge brings in little unexpected noise with possibility.\nClaim24: This provides a chance to conduct an edit withdrawal test.\nClaim25: Specifically, we test, after editing on 10 K examples, if iReVa is able to withdraw certain edits and recover the original output from the base model without much loss.\nClaim26: The results which are close to 100% proves that iReVa can explicitly manipulate the activation of knowledge neurons and easily withdraw the updated knowledge.\nClaim27: It is worth noting that this test is not applicable to any other editing methods as their edited parameters are untraceable.\nClaim28: This is the first attempt at conducting more flexible knowledge editing.\nClaim29: We also find that the influence of max-pooling is significant, which may attribute to noisy data added by a large amount of active but irrelevant knowledge neurons.\nClaim30: Besides, excluding L rec will lead to an observable drop on the three metrics because we discord the effective initialization on \u02c6K and \u02c6V.\nClaim31: Even though MEMIT achieves good performance in NS when the edit happens in lower layers, overall iReVa outperforms the baselines regarding the comprehensive evaluation metrics.\nClaim32: From the figure, we observe that iReVa can achieve the best average score on both LMs, which shows its general effect.\nClaim33: As we can see, iReVa is robust to the number of edit in the batch. It consistently surpasses the other baselines when dealing with the various number of edits.\nClaim34: In this paper, we propose iReVa, a model editing method with traceable knowledge storage, which inserts edit key-value adaptor into the MLP module of a transformer model explicitly.\nClaim35: iReVa displays prominent abilities of edit success, generalization and specificity and outperforms baselines with an observable margin.\nClaim36: Besides, iReVa first successfully demonstrates its capacity on the knowledge withdrawal.\nClaim37: In comparison to existing methods, iReVa reveals better interpretability and stronger capacity for carrying traceable edits.\nClaim38: Experiment results on series of GPT series models show our prominent performance on edit success and generalization without influencing specificity.\nClaim39: Empirically, we believe PARAREL is harder than zsRE because the average token length of edit target is shorter, thus model can\u2019t give more empirical predictions based on given prefix of the target, which is mentioned in A.5.\n\nReview: Weakness: \u2022\tThe overall approach does not appear to be novel, as it closely resembles T-Patcher. Both iReVa and T-Patcher rely on inserting neurons for editing and using neuron activations to control when to run the patch/adopter. Furthermore, analysis of the editing approach across different layers reveals the same pattern as discussed in the T-Patcher paper which involves adding additional model components in the final layer for optimal results.\n\n\u2022\tExperiments with T-Patcher are missing from the comparisons to the existing methods section. Given its similarity, T-Patcher should be included for comparison.\n\n\u2022\tAlthough T-Patcher performs editing in batches, it still uses a single patch of neurons for each edit, making its editing similarly traceable. Thus, the paper's claim of a \""stronger capacity for carrying traceable edits\"" seems unfounded.\n\n\u2022\tThe Edit Withdrawal Test section is hard to understand. How exactly was the experiment conducted? Were all edits removed or only a limited set? Detailed experimentations for this section are needed as it is the only use case of traceability explored in the paper.\n\n\u2022\tEditing techniques that rely on code books with playback vectors e.g. GRACE would allow for edits to be removed. The authors should make it clear that the withdrawal test is not possible for the editing techniques that they have chosen for comparison.\nQuestions: N/A\n\n\n\n"", ""id"": ""cO1llRY2Br0"", ""pdf"": ""openreview.net/pdf?id=cO1llRY2Br""}","50"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""The paper tries to demonstrate *LLM Agents can leverage hallucinations to mirror human cognitive biases*, while the experiments do not show how to measure hallucinations and what role hallucinations play here."", ""Target claims"": [1, 8, 22], ""Reasoning"": ""This span argues that the paper's experiments do not adequately demonstrate how hallucinations are measured or the role they play in mirroring human cognitive biases. This directly targets Claim1, which discusses the significance of hallucination properties in social decision-making, Claim8, which suggests hallucination is the basis for prosocial behavior, and Claim22, which highlights the significance of hallucination characteristics in social intelligence research.""}, {""Weakness span"": ""Human subjects are included in the experiments, while the recruitment and the details of them are missing. The paper claims LLM agents' behaviors are similar to humans, but there is no quantitative comparison between these two."", ""Target claims"": [1, 6], ""Reasoning"": ""The reviewer points out the lack of detail about the human subjects and the absence of quantitative comparison between human behavior and LLM agents' behavior, making the claim of high consistency between humans and LLM Agents (Claim1) unsupported. This also relates to Claim6, which discusses assessments showing LLM agents' consistency with human biases, as it implies inadequacy in the evidence presented to support this claim.""}]}, ""meta"": {""id"": ""anxYEohntP"", ""review_idx"": 0, ""title"": ""Title: Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View\nAbstract: Large language models (LLMs) have been shown to face hallucination issues due to the data they trained on often containing human bias; whether this is reflected in the decision-making process of LLM agents remains under-explored. As LLM Agents are increasingly employed in intricate social environments, a pressing and natural question emerges: Can LLM Agents leverage hallucinations to mirror human cognitive biases, thus exhibiting irrational social intelligence? In this paper, we probe the irrational behavior among contemporary LLM agents by melding practical social science experiments with theoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM Agents framework that utilizes hallucination properties to assess and enhance LLM Agents' social intelligence through cognitive biases. Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities, and highlighting the significance of hallucination properties. Additionally, CogMir framework demonstrates its potential as a valuable platform for encouraging more research into the social intelligence of LLM Agents.\n\nTLDR:We reveal the key attribute behind the social potential of LLM Agents and propose CogMir: a Multi-Agents framework for assessing and exploiting LLM Agents' social intelligence through cognitive biases, showing that LLM agents exhibit prosociality.\n\nKeywords:Multi-Large Model Agents\uff0cSocial Intelligence\uff0cFramework\uff0cInterpretability"", ""claims"": [""Claim1: Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities, and highlighting the significance of hallucination properties."", ""Claim2: Additionally, CogMir framework demonstrates its potential as a valuable platform for encouraging more research into the social intelligence of LLM Agents."", ""Claim3: Recent studies have highlighted the social potential of LLM Agents through constructing multi-agent systems that simulate interactive social scenarios revealing the social dynamics among interacting LLM Agents and showing parallels to human behaviors."", ""Claim4: Nonetheless, these research efforts exhibit two significant gaps: 1) They primarily focus on black-box testing in multi-agent role-playing systems, concentrating on the outputs and behaviors of agents while neglecting to investigate the internal mechanisms or cognitive processes that drive these behaviors. 2) LLM Agents are prone to hallucinations\u2014producing misleading or incorrect information, due to their training data and inherent biases."", ""Claim5: To study LLM Agents\u2019 potential for irrational social intelligence, we present CogMir, an open-ended and dynamic multi-agent framework designed specifically for evaluating, exploring, and explaining social intelligence for LLM Agents via systematic assessments of cognitive biases."", ""Claim6: Existing assessments of various cognitive effects demonstrate that LLM agents exhibit a high degree of consistency with humans in prosocial cognitive biases and counter-intuitive phenomena."", ""Claim7: However, LLM Agents demonstrate a higher sensitivity to factors like certainty and social status than humans, exhibiting more variability in their decision-making biases under conditions of certainty and uncertainty."", ""Claim8: In this work, through mirroring human cognitive bias, we suggest that the hallucination property of LLM is the basis for prosocial behavior in LLM Agents, representing a potential form of advanced intelligence."", ""Claim9: CogMir has developed an open and accessible workflow aligned with consensus-based approaches in social science, facilitating systematic testing and advancement of social intelligence in language models."", ""Claim10: Despite these advancements, exploration of why these models exhibit social capabilities remain limited. Our work tries to bridge this theoretical gap by drawing on research methods from human social evolution studies, thereby enhancing the interpretability of Multi-LLM Agents social systems."", ""Claim11: Aligned with Asch\u2019s observation of 75% conformity among humans, we set 75% as the bias threshold for LLM Agents."", ""Claim12: As shown in Tab. 1, LLM Agents display clear harmony behavior."", ""Claim13: Interestingly, unlike humans who show similar conformity levels for known and unknown information, the seven models demonstrate significant variance between responses to Known MCQs and Unknown MCQs."", ""Claim14: However, these LLM Agents exhibit human-like tendencies under three conditions: the presence of one person expressing uncertainty can reduce the conformity rate, and an increase in group size can slightly raise the conformity rate, but the impact of size remains marginal."", ""Claim15: These findings highlight the LLM Agents\u2019 enhanced sensitivity to social status differences, indicating a stronger propensity to adhere to authoritative commands over peer influence."", ""Claim16: As indicated in Tab. 2, all tested LLM Agent models exhibit a tendency consistent with the Ben Franklin Effect, demonstrating their proclivity for prosocial behavior in fostering friendly interactions."", ""Claim17: As shown in Tab. 2, the LLM Agents deemed the market price unreasonable, overlooking the unrealistic nature of the initial high price. This highlights the agents\u2019 tendency for self-validation and the profound influence of initial data on their subjective decision-making processes."", ""Claim18: As depicted in Tab. 2, Rate Bqa , all models except Claude-3.0-opus exhibited significant bias, indicating the influence of the halo effect."", ""Claim19: As shown in Fig. 4 and Tab. 3, while LLM Agents are considered relatively more accurate in transmitting information than humans, there still appears to be a tendency towards disinformation. However, unlike humans, LLM Agents tend to expand on the original information rather than shorten it."", ""Claim20: Only GPT-3.5 indicated a desire to switch answers to potentially increase the odds of being correct, showing the Gambler\u2019s Fallacy. Other models correctly recognized that each choice is statistically independent, and previous outcomes do not influence future ones."", ""Claim21: Common: The performance of the LLM Agents is highly consistent with human beings across prosociality-related irrational decision-making processes such as Herd, Authority, Ben Franklin, Halo, and Confirmation Bias."", ""Claim22: Our findings reveal that LLM Agents demonstrate pro-social behavior in irrational decision-making contexts, highlighting the significance of their hallucination characteristics in social intelligence research and pointing toward new directions for future studies."", ""Claim23: We note that, for all models, the values for Confirmation biases are high. All models except for Claude-3.0-opus have a high Halo Effect bias. Claude-2.0 and Gemini-1.0-pro have shown to be more pro-social in general."", ""Claim24: The CogMir framework advances our understanding of social intelligence in large language model (LLM) Agents by replicating the experimental paradigms used in social sciences to study human cognitive biases, thereby illuminating the previously opaque theoretical underpinnings of LLM Agent social intelligence."", ""Claim25: Despite this innovation, the framework is not without its limitations, which must be rigorously explored in future work: CogMir is a framework specifically designed for the Multi-Large Language Model Agents System. However, the current design of CogMir has limitations in simulating and testing action-based human behaviors, such as the contagiousness of yawning."", ""Claim26: Expanding CogMir to include a broader range of biases is crucial for more effectively simulating the complex cognitive influences on human decision-making. This enhancement will not only improve the framework\u2019s real-world applicability and its ability to accurately predict human-like irrational behavior in the Multi-LLM Agents System but also serve as a valuable scientific tool for social science researchers."", ""Claim27: The CogMir framework mainly utilizes classic or widely recognized social experiments, which may lack quantitative boundaries in their original sociological setups, leading to challenges in defining clear metrics for benchmarking Multi-LLM experiments."", ""Claim28: The CogMir framework heavily relies on the quality and diversity of the data it utilizes. Beyond the already established datasets in the Main paper and Appendix section C such as Known MCQ, Unknown MCQ, and various prompt and scenario simulation datasets including CogIdentity, CogAction (a subset of CogScene), and CogScene, there is a need to further expand our data collection to encompass a wider array of data types and scenarios.""], ""review"": ""Review: Weakness: 1. The paper tries to demonstrate *LLM Agents can leverage hallucinations to mirror human cognitive biases*, while the experiments do not show how to measure hallucinations and what role hallucinations play here.\n2. Human subjects are included in the experiments, while the recruitment and the details of them are missing. The paper claims LLM agents' behaviors are similar to humans, but there is no quantitative comparison between these two.\nQuestions: 1. Can you provide more case studies to show the detailed behaviors?\n2. Is it possible that the cognitive biases are not hallucinations but just because the data to pretrain LLMs is biased?\n3. Check the caption of Figure 1 where capital letters may be misused.\n4. Is there any approach to reduce such biases for an applicable LLM agent?\n5. Can we take advantage of such biases for future applications?\n\n""}, ""review"": ""Title: Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View\nAbstract: Large language models (LLMs) have been shown to face hallucination issues due to the data they trained on often containing human bias; whether this is reflected in the decision-making process of LLM agents remains under-explored. As LLM Agents are increasingly employed in intricate social environments, a pressing and natural question emerges: Can LLM Agents leverage hallucinations to mirror human cognitive biases, thus exhibiting irrational social intelligence? In this paper, we probe the irrational behavior among contemporary LLM agents by melding practical social science experiments with theoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM Agents framework that utilizes hallucination properties to assess and enhance LLM Agents' social intelligence through cognitive biases. Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities, and highlighting the significance of hallucination properties. Additionally, CogMir framework demonstrates its potential as a valuable platform for encouraging more research into the social intelligence of LLM Agents.\n\nClaim1: Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities, and highlighting the significance of hallucination properties.\nClaim2: Additionally, CogMir framework demonstrates its potential as a valuable platform for encouraging more research into the social intelligence of LLM Agents.\nClaim3: Recent studies have highlighted the social potential of LLM Agents through constructing multi-agent systems that simulate interactive social scenarios revealing the social dynamics among interacting LLM Agents and showing parallels to human behaviors.\nClaim4: Nonetheless, these research efforts exhibit two significant gaps: 1) They primarily focus on black-box testing in multi-agent role-playing systems, concentrating on the outputs and behaviors of agents while neglecting to investigate the internal mechanisms or cognitive processes that drive these behaviors. 2) LLM Agents are prone to hallucinations\u2014producing misleading or incorrect information, due to their training data and inherent biases.\nClaim5: To study LLM Agents\u2019 potential for irrational social intelligence, we present CogMir, an open-ended and dynamic multi-agent framework designed specifically for evaluating, exploring, and explaining social intelligence for LLM Agents via systematic assessments of cognitive biases.\nClaim6: Existing assessments of various cognitive effects demonstrate that LLM agents exhibit a high degree of consistency with humans in prosocial cognitive biases and counter-intuitive phenomena.\nClaim7: However, LLM Agents demonstrate a higher sensitivity to factors like certainty and social status than humans, exhibiting more variability in their decision-making biases under conditions of certainty and uncertainty.\nClaim8: In this work, through mirroring human cognitive bias, we suggest that the hallucination property of LLM is the basis for prosocial behavior in LLM Agents, representing a potential form of advanced intelligence.\nClaim9: CogMir has developed an open and accessible workflow aligned with consensus-based approaches in social science, facilitating systematic testing and advancement of social intelligence in language models.\nClaim10: Despite these advancements, exploration of why these models exhibit social capabilities remain limited. Our work tries to bridge this theoretical gap by drawing on research methods from human social evolution studies, thereby enhancing the interpretability of Multi-LLM Agents social systems.\nClaim11: Aligned with Asch\u2019s observation of 75% conformity among humans, we set 75% as the bias threshold for LLM Agents.\nClaim12: As shown in Tab. 1, LLM Agents display clear harmony behavior.\nClaim13: Interestingly, unlike humans who show similar conformity levels for known and unknown information, the seven models demonstrate significant variance between responses to Known MCQs and Unknown MCQs.\nClaim14: However, these LLM Agents exhibit human-like tendencies under three conditions: the presence of one person expressing uncertainty can reduce the conformity rate, and an increase in group size can slightly raise the conformity rate, but the impact of size remains marginal.\nClaim15: These findings highlight the LLM Agents\u2019 enhanced sensitivity to social status differences, indicating a stronger propensity to adhere to authoritative commands over peer influence.\nClaim16: As indicated in Tab. 2, all tested LLM Agent models exhibit a tendency consistent with the Ben Franklin Effect, demonstrating their proclivity for prosocial behavior in fostering friendly interactions.\nClaim17: As shown in Tab. 2, the LLM Agents deemed the market price unreasonable, overlooking the unrealistic nature of the initial high price. This highlights the agents\u2019 tendency for self-validation and the profound influence of initial data on their subjective decision-making processes.\nClaim18: As depicted in Tab. 2, Rate Bqa , all models except Claude-3.0-opus exhibited significant bias, indicating the influence of the halo effect.\nClaim19: As shown in Fig. 4 and Tab. 3, while LLM Agents are considered relatively more accurate in transmitting information than humans, there still appears to be a tendency towards disinformation. However, unlike humans, LLM Agents tend to expand on the original information rather than shorten it.\nClaim20: Only GPT-3.5 indicated a desire to switch answers to potentially increase the odds of being correct, showing the Gambler\u2019s Fallacy. Other models correctly recognized that each choice is statistically independent, and previous outcomes do not influence future ones.\nClaim21: Common: The performance of the LLM Agents is highly consistent with human beings across prosociality-related irrational decision-making processes such as Herd, Authority, Ben Franklin, Halo, and Confirmation Bias.\nClaim22: Our findings reveal that LLM Agents demonstrate pro-social behavior in irrational decision-making contexts, highlighting the significance of their hallucination characteristics in social intelligence research and pointing toward new directions for future studies.\nClaim23: We note that, for all models, the values for Confirmation biases are high. All models except for Claude-3.0-opus have a high Halo Effect bias. Claude-2.0 and Gemini-1.0-pro have shown to be more pro-social in general.\nClaim24: The CogMir framework advances our understanding of social intelligence in large language model (LLM) Agents by replicating the experimental paradigms used in social sciences to study human cognitive biases, thereby illuminating the previously opaque theoretical underpinnings of LLM Agent social intelligence.\nClaim25: Despite this innovation, the framework is not without its limitations, which must be rigorously explored in future work: CogMir is a framework specifically designed for the Multi-Large Language Model Agents System. However, the current design of CogMir has limitations in simulating and testing action-based human behaviors, such as the contagiousness of yawning.\nClaim26: Expanding CogMir to include a broader range of biases is crucial for more effectively simulating the complex cognitive influences on human decision-making. This enhancement will not only improve the framework\u2019s real-world applicability and its ability to accurately predict human-like irrational behavior in the Multi-LLM Agents System but also serve as a valuable scientific tool for social science researchers.\nClaim27: The CogMir framework mainly utilizes classic or widely recognized social experiments, which may lack quantitative boundaries in their original sociological setups, leading to challenges in defining clear metrics for benchmarking Multi-LLM experiments.\nClaim28: The CogMir framework heavily relies on the quality and diversity of the data it utilizes. Beyond the already established datasets in the Main paper and Appendix section C such as Known MCQ, Unknown MCQ, and various prompt and scenario simulation datasets including CogIdentity, CogAction (a subset of CogScene), and CogScene, there is a need to further expand our data collection to encompass a wider array of data types and scenarios.\n\nReview: Weakness: 1. The paper tries to demonstrate *LLM Agents can leverage hallucinations to mirror human cognitive biases*, while the experiments do not show how to measure hallucinations and what role hallucinations play here.\n2. Human subjects are included in the experiments, while the recruitment and the details of them are missing. The paper claims LLM agents' behaviors are similar to humans, but there is no quantitative comparison between these two.\nQuestions: 1. Can you provide more case studies to show the detailed behaviors?\n2. Is it possible that the cognitive biases are not hallucinations but just because the data to pretrain LLMs is biased?\n3. Check the caption of Figure 1 where capital letters may be misused.\n4. Is there any approach to reduce such biases for an applicable LLM agent?\n5. Can we take advantage of such biases for future applications?\n\n\n\n"", ""id"": ""anxYEohntP0"", ""pdf"": ""openreview.net/pdf?id=anxYEohntP""}","51"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""Technically, the proposed contrastive meta-objective is similar to the idea of supervised contrastive learning, which already provides good insights to the representation learning and deep learning community. Therefore, the proposed method is kind of incremental and provides less new knowledge to the field."", ""Target claims"": [5], ""Reasoning"": ""This weakness points out that the proposed method is not novel and is similar to existing ideas in supervised contrastive learning, thereby questioning the claim that ConML is 'universal and cheap' due to its purported uniqueness (Claim 5). If the method is incremental, its universality and applicability might also be questioned.""}, {""Weakness span"": ""The method is only verified on top of simplest baseline methods (MAML, ProtoNet, etc). In meta-learning, various works have been proposed to investigate the possible exploration of the task-level information for improved meta-learning, such as [R1-R4], to name a few. However, none of those previous efforts were discussed or compared. Only beating the naive baseline cannot comprehensively demonstrate the advantages of this paper."", ""Target claims"": [6], ""Reasoning"": ""This point directly targets Claim 6, which states that ConML can universally and significantly improve conventional meta-learning and in-context learning. The reviewer questions this claim, as the method was only tested against simple baselines and not against more advanced meta-learning techniques, which undermines the significance of the improvements claimed.""}, {""Weakness span"": ""Experimentally, the proposed method tries to show its superior performance over simple baselines rather than SOTA. This is less convincing."", ""Target claims"": [7, 9, 14], ""Reasoning"": ""Claims 7 and 9 suggest state-of-the-art (SOTA) performance in specific applications (molecular property prediction and MAML, respectively), while Claim 14 suggests significant improvements and efficiency. The reviewer argues that if ConML is only compared to simple baselines, it is less convincing that it provides true SOTA performance and universal benefits.""}, {""Weakness span"": ""The in-context learning experiments are only on simple synthetic data, lack of significance."", ""Target claims"": [6, 18], ""Reasoning"": ""Claims 6 and 18 discuss the universal improvement and application scope, indicating that ConML benefits in-context learning universally. The review challenges this universality if the evidence is only based on simple synthetic examples, which could imply limited real-world applicability.""}]}, ""meta"": {""id"": ""ZJZqO4grws"", ""review_idx"": 0, ""title"": ""Title: Learning to Learn with Contrastive Meta-Objective\nAbstract: We propose a contrastive meta-objective to enable meta-learners to emulate human-like rapid learning capability through enhanced alignment and discrimination. Our proposed approach, dubbed ConML,  exploits task identity as additional supervision signal for meta-training, benefiting meta-learner's fast-adaptation and task-level generalization  abilities.  This is achieved by contrasting the outputs of meta-learner, i.e, performing contrastive learning in the model space.\nSpecifically, we introduce metrics to minimize the inner-task distance, i.e., the distance among models learned on varying data subsets of the same task, while maximizing the inter-task distance among models derived from distinct tasks. \nConML distinguishes itself through versatility and efficiency, seamlessly integrating \nwith episodic meta-training methods and the in-context learning of large language models (LLMs). \nWe apply ConML to representative meta-learning algorithms spanning optimization-, metric-, and amortization-based approaches, and show that ConML can universally and significantly improve conventional meta-learning and in-context learning.\n\nTLDR:We propose contrastive meta-objective as additional supervision for meta-training, that can significantly and universally improve meta-learning and in-context learning performance with cheap implementation.\n\nKeywords:meta learning, contrastive learning, few-shot learning, i n-context learning"", ""claims"": [""Claim1: We apply ConML to representative meta-learning algorithms spanning optimization-, metric-, and amortization-based approaches, and show that ConML can universally and significantly improve conventional meta-learning and in-context learning."", ""Claim2: However, existing meta-learning approaches conventionally follows the idea of 'train as you test', to minimize the validation loss of meta-training tasks as meta-objective, where supervision signal are directly produced by sample labels."", ""Claim3: However, as the target models are expensive to learn, and even not available in many real world problems, meta-objectives requiring the target models have very restricted applications."", ""Claim4: Moreover, the importance of discrimination ability of meta-learner has not been noticed in the literature."", ""Claim5: The proposed ConML is universal and cheap, as it can be plugged-in any meta-learning algorithms following the episodic training, and does not require additional data nor model training."", ""Claim6: ConML can universally and significantly improve conventional meta-learning and in-context learning."", ""Claim7: Additionally, by applying ConML we provide a SOTA approach for few-shot molecular property prediction problem, provided in Appendix B."", ""Claim8: ConML-MAML performs better alignment and discrimination than MAML."", ""Claim9: ConML-MAML shows much better performance."", ""Claim10: We can find that: the alignment and discrimination ability corresponds to optimizing D in and D out respectively; the alignment and discrimination capabilities are generalizable; ConML shows the couple of both capabilities."", ""Claim11: We can find that the improvement brought by D in is much more significant than D out under few-shot scenario, which indicates that alignment is closely related to the fast-adaptation ability of the meta-learner."", ""Claim12: This indicates that discrimination is closely related to the task-level generalization ability of meta-learner."", ""Claim13: ConML takes both advantages brought by D in and D out."", ""Claim14: Significant relative gain and very comparable relative time consumption show that ConML brings universal improvement on different meta-learning algorithms with cheap implementation."", ""Claim15: Figure 3 shows that varying the number of in-context examples during inference, ConML317 ICL always makes more accurate predictions than ICL."", ""Claim16: Table 5 collects the two values to show the effect ConML brings to ICL: Rel. Min. Error is ConML-ICL\u2019s minimal inference error given different number of examples, divided by ICL\u2019s; Shot Spare is when ConML-ICL obtain an error no larger than ICL\u2019s minimal error, the difference between the corresponding example numbers."", ""Claim17: Note that the learning of different functions (different meta-datasets) share the same settings about ConML, which shows ConML can bring ICL universal improvement with cheap implementation."", ""Claim18: The effect of ConML for ICL is without loss of generalizability to real-world applications like pretraining large language models."", ""Claim19: We introduce a new baseline ConML-Hypro, which achieves SOTA performance by incorporating ConML with a simple backbone, Hypro."", ""Claim20: ConML-Hypro shows advantage over SOTA approach under all meta-testing scenarios with different shots."", ""Claim21: Comparing ConML-Hypro and Hypro, we can find the effect of ConML is significant.""], ""review"": ""Review: Weakness: - Technically, the proposed contrastive meta-objective is similar to the idea of supervised contrastive learning, which already provides good insights to the representation learning and deep learning community. Therefore, the proposed method is kind of incremental and provides less new knowledge to the field. \n- The method is only verified on top of simplest baseline methods (MAML, ProtoNet, etc). In meta-learning, various works have been proposed to investigate the possible exploration of the task-level information for improved meta-learning, such as [R1-R4], to name a few. \nHowever, none of those previous efforts were discussed or compared. Only beating the naive baseline cannot comprehensively demonstrate the advantages of this paper. \n- Experimentally, the proposed method tries to show its superior performance over simple baselines rather than SOTA. This is less convincing. \n- The in-context learning experiments are only on simple synthetic data, lack of significance. \n- The tile and scope: Learning-to-learn is very general, but in fact only classification related experiments are conducted. By convention, the learning-to-learn approaches will also verify on reinforcement learning. \n\n[R1] Fei, N., Lu, Z., Xiang, T., & Huang, S. (2021). MELR: Meta-learning via modeling episode-level relationships for few-shot learning. In International Conference on Learning Representations.   \n[R2] Agarwal, P., & Singh, S. (2023). Exploring intra-task relations to improve meta-learning algorithms. arXiv preprint arXiv:2312.16612.   \n[R3] Han, J., Cheng, B., & Lu, W. (2021). Exploring task difficulty for few-shot relation extraction. arXiv preprint arXiv:2109.05473.   \n[R4] Zhang, Tao. \""Episodic-free Task Selection for Few-shot Learning.\"" arXiv preprint arXiv:2402.00092 (2024).\nQuestions: Please see above Weaknesses\n\n""}, ""review"": ""Title: Learning to Learn with Contrastive Meta-Objective\nAbstract: We propose a contrastive meta-objective to enable meta-learners to emulate human-like rapid learning capability through enhanced alignment and discrimination. Our proposed approach, dubbed ConML,  exploits task identity as additional supervision signal for meta-training, benefiting meta-learner's fast-adaptation and task-level generalization  abilities.  This is achieved by contrasting the outputs of meta-learner, i.e, performing contrastive learning in the model space.\nSpecifically, we introduce metrics to minimize the inner-task distance, i.e., the distance among models learned on varying data subsets of the same task, while maximizing the inter-task distance among models derived from distinct tasks. \nConML distinguishes itself through versatility and efficiency, seamlessly integrating \nwith episodic meta-training methods and the in-context learning of large language models (LLMs). \nWe apply ConML to representative meta-learning algorithms spanning optimization-, metric-, and amortization-based approaches, and show that ConML can universally and significantly improve conventional meta-learning and in-context learning.\n\nClaim1: We apply ConML to representative meta-learning algorithms spanning optimization-, metric-, and amortization-based approaches, and show that ConML can universally and significantly improve conventional meta-learning and in-context learning.\nClaim2: However, existing meta-learning approaches conventionally follows the idea of 'train as you test', to minimize the validation loss of meta-training tasks as meta-objective, where supervision signal are directly produced by sample labels.\nClaim3: However, as the target models are expensive to learn, and even not available in many real world problems, meta-objectives requiring the target models have very restricted applications.\nClaim4: Moreover, the importance of discrimination ability of meta-learner has not been noticed in the literature.\nClaim5: The proposed ConML is universal and cheap, as it can be plugged-in any meta-learning algorithms following the episodic training, and does not require additional data nor model training.\nClaim6: ConML can universally and significantly improve conventional meta-learning and in-context learning.\nClaim7: Additionally, by applying ConML we provide a SOTA approach for few-shot molecular property prediction problem, provided in Appendix B.\nClaim8: ConML-MAML performs better alignment and discrimination than MAML.\nClaim9: ConML-MAML shows much better performance.\nClaim10: We can find that: the alignment and discrimination ability corresponds to optimizing D in and D out respectively; the alignment and discrimination capabilities are generalizable; ConML shows the couple of both capabilities.\nClaim11: We can find that the improvement brought by D in is much more significant than D out under few-shot scenario, which indicates that alignment is closely related to the fast-adaptation ability of the meta-learner.\nClaim12: This indicates that discrimination is closely related to the task-level generalization ability of meta-learner.\nClaim13: ConML takes both advantages brought by D in and D out.\nClaim14: Significant relative gain and very comparable relative time consumption show that ConML brings universal improvement on different meta-learning algorithms with cheap implementation.\nClaim15: Figure 3 shows that varying the number of in-context examples during inference, ConML317 ICL always makes more accurate predictions than ICL.\nClaim16: Table 5 collects the two values to show the effect ConML brings to ICL: Rel. Min. Error is ConML-ICL\u2019s minimal inference error given different number of examples, divided by ICL\u2019s; Shot Spare is when ConML-ICL obtain an error no larger than ICL\u2019s minimal error, the difference between the corresponding example numbers.\nClaim17: Note that the learning of different functions (different meta-datasets) share the same settings about ConML, which shows ConML can bring ICL universal improvement with cheap implementation.\nClaim18: The effect of ConML for ICL is without loss of generalizability to real-world applications like pretraining large language models.\nClaim19: We introduce a new baseline ConML-Hypro, which achieves SOTA performance by incorporating ConML with a simple backbone, Hypro.\nClaim20: ConML-Hypro shows advantage over SOTA approach under all meta-testing scenarios with different shots.\nClaim21: Comparing ConML-Hypro and Hypro, we can find the effect of ConML is significant.\n\nReview: Weakness: - Technically, the proposed contrastive meta-objective is similar to the idea of supervised contrastive learning, which already provides good insights to the representation learning and deep learning community. Therefore, the proposed method is kind of incremental and provides less new knowledge to the field. \n- The method is only verified on top of simplest baseline methods (MAML, ProtoNet, etc). In meta-learning, various works have been proposed to investigate the possible exploration of the task-level information for improved meta-learning, such as [R1-R4], to name a few. \nHowever, none of those previous efforts were discussed or compared. Only beating the naive baseline cannot comprehensively demonstrate the advantages of this paper. \n- Experimentally, the proposed method tries to show its superior performance over simple baselines rather than SOTA. This is less convincing. \n- The in-context learning experiments are only on simple synthetic data, lack of significance. \n- The tile and scope: Learning-to-learn is very general, but in fact only classification related experiments are conducted. By convention, the learning-to-learn approaches will also verify on reinforcement learning. \n\n[R1] Fei, N., Lu, Z., Xiang, T., & Huang, S. (2021). MELR: Meta-learning via modeling episode-level relationships for few-shot learning. In International Conference on Learning Representations.   \n[R2] Agarwal, P., & Singh, S. (2023). Exploring intra-task relations to improve meta-learning algorithms. arXiv preprint arXiv:2312.16612.   \n[R3] Han, J., Cheng, B., & Lu, W. (2021). Exploring task difficulty for few-shot relation extraction. arXiv preprint arXiv:2109.05473.   \n[R4] Zhang, Tao. \""Episodic-free Task Selection for Few-shot Learning.\"" arXiv preprint arXiv:2402.00092 (2024).\nQuestions: Please see above Weaknesses\n\n\n\n"", ""id"": ""ZJZqO4grws0"", ""pdf"": ""openreview.net/pdf?id=ZJZqO4grws""}","52"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""I'm unsure how to interpret the in-context regression score. It looks like it's often <1 across models. Does this mean they all fail to outperform the baseline? Is this score comparable across different tasks?"", ""Target claims"": [3], ""Reasoning"": ""This span questions the interpretation and effectiveness of the 'ICL regression score', which is introduced in Claim3 as a scalar metric to describe a model's performance on a task. The reviewer expresses uncertainty about how to interpret this score and whether it indicates performance relative to a baseline, directly questioning the claim that the metric effectively describes model performance.""}, {""Weakness span"": ""A main objective of the paper is to compute numerics comparing models, but only one training run was executed for each experiment. If the compute budget is very limited, a more valuable approach may be to consider a smaller subset of models (e.g. keeping either GPT-2 or Llama, but not both) and simpler task parameterizations. Doing so will enable you to sweep across many more settings and increase your experiment replications, generating more convincing numerics."", ""Target claims"": [1, 2, 5, 6], ""Reasoning"": ""The review critiques the study's methodology for only executing one training run per experiment, implying a lack of robustness in results which impacts claims related to performance improvements (Claim2) and architectural contributions (Claim1, Claim5). Additionally, Claim6 suggests simplification of evaluation, which is undercut by the review suggesting a more focused approach would yield better insights. The lack of replications makes it difficult to validate these claims.""}]}, ""meta"": {""id"": ""YZoGmJrOS9"", ""review_idx"": 0, ""title"": ""Title: Can Custom Models Learn In-Context? An Exploration of Hybrid Architecture Performance on In-Context Learning Tasks\nAbstract: In-Context Learning (ICL) is a phenomenon where task learning occurs through a prompt sequence without the necessity of parameter updates. ICL in Multi-Headed Attention (MHA) with absolute positional embedding has been the focus of more study than other sequence model varieties. We examine implications of architectural differences between GPT-2 and LLaMa as well as Llama and Mamba. We extend work done by Garg et al. (2022) and Park et al. (2024) to GPT-2/LLaMa hybrid and LLaMa/Mamba hybrid models -- examining the interplay between sequence transformation blocks and regressive performance in-context. We note that certain architectural changes cause degraded training efficiency/ICL accuracy by converging to suboptimal predictors or converging slower. We also find certain hybrids showing optimistic performance improvements, informing potential future ICL-focused architecture modifications. Additionally, we propose the \""ICL regression score\"", a scalar metric describing a model's whole performance on a specific task. Compute limitations impose restrictions on our architecture-space, training duration, number of training runs, function class complexity, and benchmark complexity. To foster reproducible and extensible research, we provide a typed, modular, and extensible Python package on which we run all experiments. This code is available at \\url{https://github.com/anonymousforneurips64/neurips2024-submission21757}.\n\nTLDR:\n\nKeywords:In Context Learning, Architecture, Hybrid Models, Transformers, Mamba, Attention, Toy Models, Benchmarking"", ""claims"": [""Claim1: We note that certain architectural changes cause degraded training efficiency/ICL accuracy by converging to suboptimal predictors or converging slower."", ""Claim2: We also find certain hybrids showing optimistic performance improvements, informing potential future ICL-focused architecture modifications."", ""Claim3: Additionally, we propose the \""ICL regression score\"", a scalar metric describing a model's whole performance on a specific task."", ""Claim4: Using unmodified Llama/Mamba/GPT-2 as a control, we analyze GPT2-Llama and Llama-Mamba hybrid architectures derived from replacing portions of GPT2 components with analogous Llama sections and LLama with Mamba blocks, respectively, in 12 total architectures (3 unmodified + 9 hybrid)."", ""Claim5: We further this work by closely examining the contributions of individual architectural changes for GPT-2 and Llama-style transformers towards ICL ability."", ""Claim6: To further simplify ICL aptitude evaluation, we introduce a proxy value summarizing a given model\u2019s ICL ability for a specific task."", ""Claim7: We preserve these tasks from Garg et al. [6] to verify that none of our hybrid modifications lose the near-optimal performance that was already found with GPT-2."", ""Claim8: This is a more complex function class that Garg et al. [6] found that GPT-2 can perform very well at, suggesting that this task can capture some ICL ability of an architecture."", ""Claim9: Garg et al. [6] found that GPT-2 was able to achieve much lower error for lower context lengths than XGBoost or Greedy Tree Learning, suggesting that this task can capture some ICL ability of an architecture."", ""Claim10: They [14] found that GPT-2 style transformers do not perform well on this task, suggesting that this is a discerning proxy for measuring ICL ability."", ""Claim11: Park et al. [14] found that Mamba, our representative of SSM-type models, performed poorly, suggesting that this task can serve to ensure we don\u2019t lose capabilities provided by transformers."", ""Claim12: We confirm the results from Garg et al. [6] and Park et al. [14] that GPT-2 and Mamba can learn our first four regression tasks in context."", ""Claim13: Park et al. [14] that Mamba struggles to perform Vector MQAR while transformers and hybrid architectures excel."", ""Claim14: We note that Llama and GPT-2 have very comparable performance in Sparse Parity and Vector MQAR."", ""Claim15: We find that some model-task pairs produce suboptimal predictions, not as a result of insufficient training."", ""Claim16: Models can escape suboptimal regression schemes. We see that GPT-2 SwiGLU (model 1.3) Sparse Linear on adopts a suboptimal regression scheme (least squares) partway in training, eventually unlearning its scheme in favor of the optimal regression scheme (lasso)."", ""Claim17: Models can fail to converge within our training horizon. We find that a number of models performed strikingly poorly in their trained task."", ""Claim18: Models can fail to learn the task entirely. In the case of Decision Tree, GPT-2 with RMS (model 1.1) failed to learn the task entirely as not only indicated by its final ICL Regression Score but also its consistency in achieving very high error throughout training."", ""Claim19: ICL Regression Scores reflect qualitative information contained in squared-error plots."", ""Claim20: Overall, most models are able to perform comparably to our baseline estimators, with nearly all examined models achieving a regression score of approximately 1 on all four function classes from Garg et al. (Linear Regression, Sparse Linear Regression, 2-Layer MLP, Decision Tree)."", ""Claim21: Regression and 2-Layer MLP, along with their corresponding graphs of squared error as a function of context length, corroborate the claims from Garg et al. [6] that transformers can 'learn' these tasks."", ""Claim22: Further, the ICL Regression Scores for Sparse Parity are consistent with Park et al. [14], with all hybrids between GPT-2, and Llama failing to 'learn' the task and all hybrids between Llama and Mamba succeeding in 'learning' the task."", ""Claim23: Indeed, the ICL Regression Score achieved by Mamba captures the qualitatively sub-optimal performance detailed above on Decision Tree."", ""Claim24: Hybrid architectures and function classes have varying levels of compatibility. Specific hybrid architectures can hesitate to learn/converge for certain function classes."", ""Claim25: Certain hybrid architecture variations may place inductive biases on certain solution forms, resulting in extreme convergence times when these solution forms greatly vary from the optimal predictor\u2019s form."", ""Claim26: Extensible Research as Reproducible Research. In the development of this work, continuously iterating to minimize the friction of reproduction has enabled rapid extension of our Python artifacts to support even abstractly defined hybrid architectures, which are often considered inextricable from highly bespoke code or dedicated packages such as xFormers [37]."", ""Claim27: We have only one training run performed on each model-task pair. As a result, we have no estimation for how consistently observed phenomena appear with the given architectures."", ""Claim28: We do not empirically evaluate the effectiveness of ICL Regression Score or the usability of our provided code platform.""], ""review"": ""Review: Weakness: As mentioned above, the present work seems to be closely related to [Park et al.](https://arxiv.org/abs/2402.04248), with little added insight. While an attempt has been made to explore minor architectural aspects like the choice of normalization, positional encodings, and activations, for the most part the changes seem to matter little (according to Table 3). I'm unsure what the key take-away is. Is there a particular architectural configuration that works best? What concrete practices can a user apply to improve their models' performance? The authors appear to have an ambition towards answering questions like these, but do not ultimately resolve them.\n\nThe results are sometimes difficult to interpret. At times, this is simply because the plots are unreadable, with text that is too small. I'm unsure how to interpret the in-context regression score. It looks like it's often <1 across models. Does this mean they all fail to outperform the baseline? Is this score comparable across different tasks?\n\nA main objective of the paper is to compute numerics comparing models, but only one training run was executed for each experiment. If the compute budget is very limited, a more valuable approach may be to consider a smaller subset of models (e.g. keeping either GPT-2 or Llama, but not both) and simpler task parameterizations. Doing so will enable you to sweep across many more settings and increase your experiment replications, generating more convincing numerics. \n\nAdditional minor formatting comments:\n- Line 25: consider compressing citations (e.g. with sort&compress)\n- Table 1: third hline from the top intersects with text\n- Consider plotting figures with point-markers for each data point, to clarify where exactly your data points fall\n- For related models that vary by parameter (e.g. training iteraitons), consider using the same color but with different shadings / style\n- Figure 15: text is unreadable\n- Figure text overall is small and hard to read\nQuestions: - Line 255 mentions that there is only a single run performed for each model-task pairs, yet many plots feature confidence intervals. Where do these confidence intervals come from?\n- Why do you pick the model subsets that you do in constructing each plot? For instance, plot 3b has a title \""Decision Tree: All Architectures\"" but only plots GPT-2 and Mamba (what about Llama, and the other variations?)\n- Why apply Mamba only to Llama, and not to GPT-2? Since you mentioned compute was a bottleneck, why not keep either GPT-2 or Llama as a base model?\n- How is the ICL regression score meant to be interpreted? It seems to depend strongly on the task, where some tasks will feature a score uniformly less than 1 across models, and greater than 1 for others.\n\n""}, ""review"": ""Title: Can Custom Models Learn In-Context? An Exploration of Hybrid Architecture Performance on In-Context Learning Tasks\nAbstract: In-Context Learning (ICL) is a phenomenon where task learning occurs through a prompt sequence without the necessity of parameter updates. ICL in Multi-Headed Attention (MHA) with absolute positional embedding has been the focus of more study than other sequence model varieties. We examine implications of architectural differences between GPT-2 and LLaMa as well as Llama and Mamba. We extend work done by Garg et al. (2022) and Park et al. (2024) to GPT-2/LLaMa hybrid and LLaMa/Mamba hybrid models -- examining the interplay between sequence transformation blocks and regressive performance in-context. We note that certain architectural changes cause degraded training efficiency/ICL accuracy by converging to suboptimal predictors or converging slower. We also find certain hybrids showing optimistic performance improvements, informing potential future ICL-focused architecture modifications. Additionally, we propose the \""ICL regression score\"", a scalar metric describing a model's whole performance on a specific task. Compute limitations impose restrictions on our architecture-space, training duration, number of training runs, function class complexity, and benchmark complexity. To foster reproducible and extensible research, we provide a typed, modular, and extensible Python package on which we run all experiments. This code is available at \\url{https://github.com/anonymousforneurips64/neurips2024-submission21757}.\n\nClaim1: We note that certain architectural changes cause degraded training efficiency/ICL accuracy by converging to suboptimal predictors or converging slower.\nClaim2: We also find certain hybrids showing optimistic performance improvements, informing potential future ICL-focused architecture modifications.\nClaim3: Additionally, we propose the \""ICL regression score\"", a scalar metric describing a model's whole performance on a specific task.\nClaim4: Using unmodified Llama/Mamba/GPT-2 as a control, we analyze GPT2-Llama and Llama-Mamba hybrid architectures derived from replacing portions of GPT2 components with analogous Llama sections and LLama with Mamba blocks, respectively, in 12 total architectures (3 unmodified + 9 hybrid).\nClaim5: We further this work by closely examining the contributions of individual architectural changes for GPT-2 and Llama-style transformers towards ICL ability.\nClaim6: To further simplify ICL aptitude evaluation, we introduce a proxy value summarizing a given model\u2019s ICL ability for a specific task.\nClaim7: We preserve these tasks from Garg et al. [6] to verify that none of our hybrid modifications lose the near-optimal performance that was already found with GPT-2.\nClaim8: This is a more complex function class that Garg et al. [6] found that GPT-2 can perform very well at, suggesting that this task can capture some ICL ability of an architecture.\nClaim9: Garg et al. [6] found that GPT-2 was able to achieve much lower error for lower context lengths than XGBoost or Greedy Tree Learning, suggesting that this task can capture some ICL ability of an architecture.\nClaim10: They [14] found that GPT-2 style transformers do not perform well on this task, suggesting that this is a discerning proxy for measuring ICL ability.\nClaim11: Park et al. [14] found that Mamba, our representative of SSM-type models, performed poorly, suggesting that this task can serve to ensure we don\u2019t lose capabilities provided by transformers.\nClaim12: We confirm the results from Garg et al. [6] and Park et al. [14] that GPT-2 and Mamba can learn our first four regression tasks in context.\nClaim13: Park et al. [14] that Mamba struggles to perform Vector MQAR while transformers and hybrid architectures excel.\nClaim14: We note that Llama and GPT-2 have very comparable performance in Sparse Parity and Vector MQAR.\nClaim15: We find that some model-task pairs produce suboptimal predictions, not as a result of insufficient training.\nClaim16: Models can escape suboptimal regression schemes. We see that GPT-2 SwiGLU (model 1.3) Sparse Linear on adopts a suboptimal regression scheme (least squares) partway in training, eventually unlearning its scheme in favor of the optimal regression scheme (lasso).\nClaim17: Models can fail to converge within our training horizon. We find that a number of models performed strikingly poorly in their trained task.\nClaim18: Models can fail to learn the task entirely. In the case of Decision Tree, GPT-2 with RMS (model 1.1) failed to learn the task entirely as not only indicated by its final ICL Regression Score but also its consistency in achieving very high error throughout training.\nClaim19: ICL Regression Scores reflect qualitative information contained in squared-error plots.\nClaim20: Overall, most models are able to perform comparably to our baseline estimators, with nearly all examined models achieving a regression score of approximately 1 on all four function classes from Garg et al. (Linear Regression, Sparse Linear Regression, 2-Layer MLP, Decision Tree).\nClaim21: Regression and 2-Layer MLP, along with their corresponding graphs of squared error as a function of context length, corroborate the claims from Garg et al. [6] that transformers can 'learn' these tasks.\nClaim22: Further, the ICL Regression Scores for Sparse Parity are consistent with Park et al. [14], with all hybrids between GPT-2, and Llama failing to 'learn' the task and all hybrids between Llama and Mamba succeeding in 'learning' the task.\nClaim23: Indeed, the ICL Regression Score achieved by Mamba captures the qualitatively sub-optimal performance detailed above on Decision Tree.\nClaim24: Hybrid architectures and function classes have varying levels of compatibility. Specific hybrid architectures can hesitate to learn/converge for certain function classes.\nClaim25: Certain hybrid architecture variations may place inductive biases on certain solution forms, resulting in extreme convergence times when these solution forms greatly vary from the optimal predictor\u2019s form.\nClaim26: Extensible Research as Reproducible Research. In the development of this work, continuously iterating to minimize the friction of reproduction has enabled rapid extension of our Python artifacts to support even abstractly defined hybrid architectures, which are often considered inextricable from highly bespoke code or dedicated packages such as xFormers [37].\nClaim27: We have only one training run performed on each model-task pair. As a result, we have no estimation for how consistently observed phenomena appear with the given architectures.\nClaim28: We do not empirically evaluate the effectiveness of ICL Regression Score or the usability of our provided code platform.\n\nReview: Weakness: As mentioned above, the present work seems to be closely related to [Park et al.](https://arxiv.org/abs/2402.04248), with little added insight. While an attempt has been made to explore minor architectural aspects like the choice of normalization, positional encodings, and activations, for the most part the changes seem to matter little (according to Table 3). I'm unsure what the key take-away is. Is there a particular architectural configuration that works best? What concrete practices can a user apply to improve their models' performance? The authors appear to have an ambition towards answering questions like these, but do not ultimately resolve them.\n\nThe results are sometimes difficult to interpret. At times, this is simply because the plots are unreadable, with text that is too small. I'm unsure how to interpret the in-context regression score. It looks like it's often <1 across models. Does this mean they all fail to outperform the baseline? Is this score comparable across different tasks?\n\nA main objective of the paper is to compute numerics comparing models, but only one training run was executed for each experiment. If the compute budget is very limited, a more valuable approach may be to consider a smaller subset of models (e.g. keeping either GPT-2 or Llama, but not both) and simpler task parameterizations. Doing so will enable you to sweep across many more settings and increase your experiment replications, generating more convincing numerics. \n\nAdditional minor formatting comments:\n- Line 25: consider compressing citations (e.g. with sort&compress)\n- Table 1: third hline from the top intersects with text\n- Consider plotting figures with point-markers for each data point, to clarify where exactly your data points fall\n- For related models that vary by parameter (e.g. training iteraitons), consider using the same color but with different shadings / style\n- Figure 15: text is unreadable\n- Figure text overall is small and hard to read\nQuestions: - Line 255 mentions that there is only a single run performed for each model-task pairs, yet many plots feature confidence intervals. Where do these confidence intervals come from?\n- Why do you pick the model subsets that you do in constructing each plot? For instance, plot 3b has a title \""Decision Tree: All Architectures\"" but only plots GPT-2 and Mamba (what about Llama, and the other variations?)\n- Why apply Mamba only to Llama, and not to GPT-2? Since you mentioned compute was a bottleneck, why not keep either GPT-2 or Llama as a base model?\n- How is the ICL regression score meant to be interpreted? It seems to depend strongly on the task, where some tasks will feature a score uniformly less than 1 across models, and greater than 1 for others.\n\n\n\n"", ""id"": ""YZoGmJrOS90"", ""pdf"": ""openreview.net/pdf?id=YZoGmJrOS9""}","53"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""While the method proposed in this paper might be extendable to other low-resource languages, the authors only validated it on Thai, which lacks empirical evidence for the generality of proposed pipeline."", ""Target claims"": [1], ""Reasoning"": ""The weakness questions the generality of the results claimed in the paper by pointing out that the authors only validated their method on Thai. This directly relates to Claim1, which states that they transformed the LLaMA-2-chat-7B to the Thai language, but does not claim or prove broader applicability to other languages.""}, {""Weakness span"": ""The authors tested the method on Llama-2-7B. Experiments on other model series and larger models could further support the effectiveness of proposed pipeline."", ""Target claims"": [2, 3, 29, 30], ""Reasoning"": ""The reviewer is questioning the effectiveness of the method by suggesting that testing on only one model series, Llama-2-7B, is insufficient to prove broader effectiveness. This relates to several claims regarding the effectiveness of their method (Claims 2, 3, 29, 30), which report superior performance over baselines and other models like ChatGPT and GPT-4.""}, {""Weakness span"": ""The authors used Google Translate for translating TCOT and AdvBench data. Although this is a commonly used commercial system, it would be better to supplement the evaluation and report on the quality of these data translations to explore their impact."", ""Target claims"": [3, 12, 13], ""Reasoning"": ""This weakness questions the quality of the translations used, suggesting that it could influence the outcomes claimed. This relates to Claims 3 and 13, where they assert higher safety performance, and Claim 12, which discusses the rejection rate of harmful queries, both depending on the quality of translation used in the benchmarks.""}]}, ""meta"": {""id"": ""YZWvf58dBS"", ""review_idx"": 0, ""title"": ""Title: Why Not Transform Chat Large Language Models to Non-English?\nAbstract: The scarcity of non-English data limits the development of non-English large language models (LLMs). Transforming English-centric LLMs to non-English has been identified as an effective and resource-efficient method. Previous works start from base LLMs and perform knowledge distillation (KD) with data generated by stronger LLMs, e.g. GPT-4. Compared to base LLMs, chat LLMs are further optimized for advanced abilities, e.g. multi-turn conversation and human preference alignment, and thus more powerful in both helpfulness and safety. However, transforming a chat LLM involves two critical issues: (1) How can we effectively transfer advanced abilities without their supervised data? (2) How can we prevent the original knowledge from catastrophic forgetting during transformation? We target these issues by introducing a simple framework called TransLLM. For the first issue, TransLLM divides the transfer problem into some common sub-tasks with the translation chain-of-thought, which uses the translation as the bridge between English and non-English step-by-step. We further enhance the performance of sub-tasks with publicly available data. For the second issue, we propose a method comprising two synergistic components: low-rank adaptation for training to maintain the original LLM parameters, and recovery KD, which utilizes data generated by the chat LLM itself to recover the original knowledge from the frozen parameters. In the experiments, we transform the LLaMA-2-chat-7B to the Thai language. Our method, using only single-turn data, outperforms strong baselines and ChatGPT on multi-turn benchmark MT-bench. Furthermore, our method, without safety data, rejects more harmful queries of safety benchmark AdvBench than both ChatGPT and GPT-4.\n\nTLDR:\n\nKeywords:Large Language Model; Knowledge Transfer; Catastrophic Forgetting; Multi-turn Conversation; Human Preference"", ""claims"": [""Claim1: In the experiments, we transform the LLaMA-2-chat-7B to the Thai language."", ""Claim2: Our method, using only single-turn data, outperforms strong baselines and ChatGPT on multi-turn benchmark MT-bench."", ""Claim3: Furthermore, our method, without safety data, rejects more harmful queries of safety benchmark AdvBench than both ChatGPT and GPT-4."", ""Claim4: TransLLM outperforms strong baselines and surpasses ChatGPT by 35% and 23.75% for the first and second turns on the MT-bench with statistical significance."", ""Claim5: More importantly, we attain an improvement of 14.8% and 8.65% over ChatGPT and GPT-4 respectively on the safety benchmark AdvBenchmark with statistical significance."", ""Claim6: The final TransLLM model can respond in both \u03b1 and \u03b2 , including \u03b1 \u03b2 bi-direction translation."", ""Claim7: Interestingly, even in this unseen setting, the model still outputs the TCOT format as y = cat ( < \u03b1 > , q \u03b1n +1 , <RESPONSE> , a \u03b1n +1 , < \u03b2 > , a \u03b2n +1 ) ."", ""Claim8: Better performance than ChatGPT on MT-Bench. As shown in Table 1, TransLLM surpasses ChatGPT by 35% and 23.75% for the first and second turn on MT-bench with statistical significance."", ""Claim9: It is an inspiring result although TransLLM is still behind GPT-4 in TH."", ""Claim10: In Table 2, GPT-4 shows high consistency with human annotators."", ""Claim11: Therefore, we use GPT-4 to evaluate the helpfulness in the following experiments."", ""Claim12: Higher safety than ChatGPT and GPT-4. In Table 3, TransLLM has a rejection rate of 94.61%, close to 99.23% of the original model."", ""Claim13: TransLLM attains an improvement of 14.8% and 8.65% over ChatGPT and GPT-4 for rejecting harmful queries with statistical significance."", ""Claim14: As shown in Table 4, TransLLM significantly outperforms baselines that are built on open-source resources."", ""Claim15: Although NLLB-bridge uses more parameters and more translation resources, it still loses to TransLLM."", ""Claim16: On the larger dataset, TransLLM outperforms baselines and ChatGPT by a large margin with statistical significance as shown in Table 5."", ""Claim17: The results confirm our hypothesis that transforming chat LLMs could provide better conversational ability than base LLMs."", ""Claim18: Beyond safety, the high-quality GPT-4 KD data also leads to performance degradation for helpfulness. That is because our goal is not to inject more knowledge but to preserve the original knowledge..."", ""Claim19: Meanwhile, TransLLM successfully recovers the original knowledge."", ""Claim20: As shown in Table 7, after pre-training, which has been proven to be necessary, the LLM significantly forgets the conversation knowledge."", ""Claim21: GPT-4 KD, which is widely used in previous works, can provide high-quality knowledge. However, this kind of knowledge is quite different from and competes with the original knowledge."", ""Claim22: As a result, the LLM still forgets much original knowledge using GPT-4 KD."", ""Claim23: The average similarity per token for EN responses is much larger than that for TH responses, 0.6191 vs. 0.2522. That means TransLLM successfully learns the \u201cshortcut\u201d using LoRA and recovery KD together."", ""Claim24: As shown in Table 8, benefiting from translation and TH pre-training, TransLLM outperforms ChatGPT and NLLB on EN-TH and achieves competitive performance on TH-EN."", ""Claim25: ChatGPT and TransLLM are generally fluent, with translationese to a certain degree."", ""Claim26: In this paper, we propose a framework for transforming an off-the-shelf chat LLM to other languages."", ""Claim27: The experiments in TH show that we transfer desired abilities to TH and outperform ChatGPT in both helpfulness and safety."", ""Claim28: Overall, we hope that this work can become the foundation for developing safe LLMs in many languages other than English."", ""Claim29: Our method, using only single-turn data, outperforms strong baselines and ChatGPT on multi-turn benchmark MT-bench."", ""Claim30: Furthermore, our method, without safety data, rejects more harmful queries of safety benchmark AdvBench than both ChatGPT and GPT-4."", ""Claim31: The difference between TransLLM and others in Table 5 are all significant with p < 0 . 001.""], ""review"": ""Review: Weakness: 1. **Flexibility of Methodology**: Although the proposed method reduces the dependency on instruction-following data in the target language, it still relies on parallel corpora and external translation models for data construction (Translation pre-training data, TCOT data). If high-quality parallel corpora or models are not available, the proposed method might be infeasible. For example, even commercial translation systems cannot support translations for some endangered languages or dialects.\n2. **Scope**: While the method proposed in this paper might be extendable to other low-resource languages, the authors only validated it on Thai, which lacks empirical evidence for the generality of proposed pipeline. I also noticed that the authors emphasized \""non-English\"" in the title, but a broader range of non-English languages still requires exploration.\n3. **Effectiveness**: The authors tested the method on Llama-2-7B. Experiments on other model series and larger models could further support the effectiveness of proposed pipeline.\n4. **Quality of MT data**: The authors used Google Translate for translating TCOT and AdvBench data. Although this is a commonly used commercial system, it would be better to supplement the evaluation and report on the quality of these data translations to explore their impact. In the absence of reference translations, quality estimation methods like CometKiwi [1] and TransQuest [2] could be used.\n5. **Typos**:\n    - Line 45: instruct tuning -> instruction tuning\n\nReferences\n\n[1] Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana C Farinha, Christine Maroti, Jos\u00e9 G. C. de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie, and Andr\u00e9 F. T. Martins. 2022. CometKiwi: IST-Unbabel 2022 Submission for the Quality Estimation Shared Task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 634\u2013645, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.\n\n[2] Tharindu Ranasinghe, Constantin Orasan, and Ruslan Mitkov. 2020. TransQuest: Translation Quality Estimation with Cross-lingual Transformers. In Proceedings of the 28th International Conference on Computational Linguistics, pages 5070\u20135081, Barcelona, Spain (Online). International Committee on Computational Linguistics.\nQuestions: Besides some macro-level issues mentioned in the weaknesses, I still have some questions about some details:\n\n1. This paper mainly focuses on transferring to a single target language. Is it applicable for transferring to multiple target languages? If so, how to balance the data for multiple target languages?\n2. Line 170: A random strategy was used for data mixing. Would other mixing strategies affect the results?\n3. Line 131: Since the authors mentioned being inspired by mBART, is the special language ID independently occupying a token in the vocabulary?\n\n""}, ""review"": ""Title: Why Not Transform Chat Large Language Models to Non-English?\nAbstract: The scarcity of non-English data limits the development of non-English large language models (LLMs). Transforming English-centric LLMs to non-English has been identified as an effective and resource-efficient method. Previous works start from base LLMs and perform knowledge distillation (KD) with data generated by stronger LLMs, e.g. GPT-4. Compared to base LLMs, chat LLMs are further optimized for advanced abilities, e.g. multi-turn conversation and human preference alignment, and thus more powerful in both helpfulness and safety. However, transforming a chat LLM involves two critical issues: (1) How can we effectively transfer advanced abilities without their supervised data? (2) How can we prevent the original knowledge from catastrophic forgetting during transformation? We target these issues by introducing a simple framework called TransLLM. For the first issue, TransLLM divides the transfer problem into some common sub-tasks with the translation chain-of-thought, which uses the translation as the bridge between English and non-English step-by-step. We further enhance the performance of sub-tasks with publicly available data. For the second issue, we propose a method comprising two synergistic components: low-rank adaptation for training to maintain the original LLM parameters, and recovery KD, which utilizes data generated by the chat LLM itself to recover the original knowledge from the frozen parameters. In the experiments, we transform the LLaMA-2-chat-7B to the Thai language. Our method, using only single-turn data, outperforms strong baselines and ChatGPT on multi-turn benchmark MT-bench. Furthermore, our method, without safety data, rejects more harmful queries of safety benchmark AdvBench than both ChatGPT and GPT-4.\n\nClaim1: In the experiments, we transform the LLaMA-2-chat-7B to the Thai language.\nClaim2: Our method, using only single-turn data, outperforms strong baselines and ChatGPT on multi-turn benchmark MT-bench.\nClaim3: Furthermore, our method, without safety data, rejects more harmful queries of safety benchmark AdvBench than both ChatGPT and GPT-4.\nClaim4: TransLLM outperforms strong baselines and surpasses ChatGPT by 35% and 23.75% for the first and second turns on the MT-bench with statistical significance.\nClaim5: More importantly, we attain an improvement of 14.8% and 8.65% over ChatGPT and GPT-4 respectively on the safety benchmark AdvBenchmark with statistical significance.\nClaim6: The final TransLLM model can respond in both \u03b1 and \u03b2 , including \u03b1 \u03b2 bi-direction translation.\nClaim7: Interestingly, even in this unseen setting, the model still outputs the TCOT format as y = cat ( < \u03b1 > , q \u03b1n +1 , <RESPONSE> , a \u03b1n +1 , < \u03b2 > , a \u03b2n +1 ) .\nClaim8: Better performance than ChatGPT on MT-Bench. As shown in Table 1, TransLLM surpasses ChatGPT by 35% and 23.75% for the first and second turn on MT-bench with statistical significance.\nClaim9: It is an inspiring result although TransLLM is still behind GPT-4 in TH.\nClaim10: In Table 2, GPT-4 shows high consistency with human annotators.\nClaim11: Therefore, we use GPT-4 to evaluate the helpfulness in the following experiments.\nClaim12: Higher safety than ChatGPT and GPT-4. In Table 3, TransLLM has a rejection rate of 94.61%, close to 99.23% of the original model.\nClaim13: TransLLM attains an improvement of 14.8% and 8.65% over ChatGPT and GPT-4 for rejecting harmful queries with statistical significance.\nClaim14: As shown in Table 4, TransLLM significantly outperforms baselines that are built on open-source resources.\nClaim15: Although NLLB-bridge uses more parameters and more translation resources, it still loses to TransLLM.\nClaim16: On the larger dataset, TransLLM outperforms baselines and ChatGPT by a large margin with statistical significance as shown in Table 5.\nClaim17: The results confirm our hypothesis that transforming chat LLMs could provide better conversational ability than base LLMs.\nClaim18: Beyond safety, the high-quality GPT-4 KD data also leads to performance degradation for helpfulness. That is because our goal is not to inject more knowledge but to preserve the original knowledge...\nClaim19: Meanwhile, TransLLM successfully recovers the original knowledge.\nClaim20: As shown in Table 7, after pre-training, which has been proven to be necessary, the LLM significantly forgets the conversation knowledge.\nClaim21: GPT-4 KD, which is widely used in previous works, can provide high-quality knowledge. However, this kind of knowledge is quite different from and competes with the original knowledge.\nClaim22: As a result, the LLM still forgets much original knowledge using GPT-4 KD.\nClaim23: The average similarity per token for EN responses is much larger than that for TH responses, 0.6191 vs. 0.2522. That means TransLLM successfully learns the \u201cshortcut\u201d using LoRA and recovery KD together.\nClaim24: As shown in Table 8, benefiting from translation and TH pre-training, TransLLM outperforms ChatGPT and NLLB on EN-TH and achieves competitive performance on TH-EN.\nClaim25: ChatGPT and TransLLM are generally fluent, with translationese to a certain degree.\nClaim26: In this paper, we propose a framework for transforming an off-the-shelf chat LLM to other languages.\nClaim27: The experiments in TH show that we transfer desired abilities to TH and outperform ChatGPT in both helpfulness and safety.\nClaim28: Overall, we hope that this work can become the foundation for developing safe LLMs in many languages other than English.\nClaim29: Our method, using only single-turn data, outperforms strong baselines and ChatGPT on multi-turn benchmark MT-bench.\nClaim30: Furthermore, our method, without safety data, rejects more harmful queries of safety benchmark AdvBench than both ChatGPT and GPT-4.\nClaim31: The difference between TransLLM and others in Table 5 are all significant with p < 0 . 001.\n\nReview: Weakness: 1. **Flexibility of Methodology**: Although the proposed method reduces the dependency on instruction-following data in the target language, it still relies on parallel corpora and external translation models for data construction (Translation pre-training data, TCOT data). If high-quality parallel corpora or models are not available, the proposed method might be infeasible. For example, even commercial translation systems cannot support translations for some endangered languages or dialects.\n2. **Scope**: While the method proposed in this paper might be extendable to other low-resource languages, the authors only validated it on Thai, which lacks empirical evidence for the generality of proposed pipeline. I also noticed that the authors emphasized \""non-English\"" in the title, but a broader range of non-English languages still requires exploration.\n3. **Effectiveness**: The authors tested the method on Llama-2-7B. Experiments on other model series and larger models could further support the effectiveness of proposed pipeline.\n4. **Quality of MT data**: The authors used Google Translate for translating TCOT and AdvBench data. Although this is a commonly used commercial system, it would be better to supplement the evaluation and report on the quality of these data translations to explore their impact. In the absence of reference translations, quality estimation methods like CometKiwi [1] and TransQuest [2] could be used.\n5. **Typos**:\n    - Line 45: instruct tuning -> instruction tuning\n\nReferences\n\n[1] Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana C Farinha, Christine Maroti, Jos\u00e9 G. C. de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie, and Andr\u00e9 F. T. Martins. 2022. CometKiwi: IST-Unbabel 2022 Submission for the Quality Estimation Shared Task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 634\u2013645, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.\n\n[2] Tharindu Ranasinghe, Constantin Orasan, and Ruslan Mitkov. 2020. TransQuest: Translation Quality Estimation with Cross-lingual Transformers. In Proceedings of the 28th International Conference on Computational Linguistics, pages 5070\u20135081, Barcelona, Spain (Online). International Committee on Computational Linguistics.\nQuestions: Besides some macro-level issues mentioned in the weaknesses, I still have some questions about some details:\n\n1. This paper mainly focuses on transferring to a single target language. Is it applicable for transferring to multiple target languages? If so, how to balance the data for multiple target languages?\n2. Line 170: A random strategy was used for data mixing. Would other mixing strategies affect the results?\n3. Line 131: Since the authors mentioned being inspired by mBART, is the special language ID independently occupying a token in the vocabulary?\n\n\n\n"", ""id"": ""YZWvf58dBS0"", ""pdf"": ""openreview.net/pdf?id=YZWvf58dBS""}","54"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""I am not convinced because typically, in practice, people will use all the data available to tune a base model, and usually, this renders the best downstream task performance, or at least, the robustness defined as the average downstream task performance of a particular tuned chat LLM. Why cannot we follow this paradigm?"", ""Target claims"": [2, 3, 4, 5, 8, 9, 14, 27, 28, 29, 30], ""Reasoning"": ""The reviewer questions the effectiveness of the approach or method claimed to outperform baselines and achieve desired abilities without following the typical paradigm of using all available data for tuning base models (Claims 2, 3, 4, 5, 8, 9, 14, 27, 28, 29, 30). The implication is that the claimed results might not be as robust or optimal if a different, more common method was used, thereby questioning the robustness and effectiveness of the claims.""}, {""Weakness span"": ""As far as I know, if you just use knowledge distillation by distilling the data synthesized from a stronger model in your target languages, you might still need to use stronger model, usually those closed-source models, e.g., ChatGPT, this is still expensive, right? Besides, as even the knowledge distillation, in the SFT stage, you have to mix all the data to do the training as well. So it is still expensive."", ""Target claims"": [], ""Reasoning"": ""The reviewer questions the claim that the proposed method is resource-efficient, pointing out the potential high costs associated with using stronger, often closed-source models like ChatGPT for knowledge distillation processes. However, this critique is more focused on technology assumptions rather than directly challenging the truthfulness of any specific result claim such as those about outperforming baselines or other systems directly (so doesn't target specific claims directly listed).""}]}, ""meta"": {""id"": ""YZWvf58dBS"", ""review_idx"": 1, ""title"": ""Title: Why Not Transform Chat Large Language Models to Non-English?\nAbstract: The scarcity of non-English data limits the development of non-English large language models (LLMs). Transforming English-centric LLMs to non-English has been identified as an effective and resource-efficient method. Previous works start from base LLMs and perform knowledge distillation (KD) with data generated by stronger LLMs, e.g. GPT-4. Compared to base LLMs, chat LLMs are further optimized for advanced abilities, e.g. multi-turn conversation and human preference alignment, and thus more powerful in both helpfulness and safety. However, transforming a chat LLM involves two critical issues: (1) How can we effectively transfer advanced abilities without their supervised data? (2) How can we prevent the original knowledge from catastrophic forgetting during transformation? We target these issues by introducing a simple framework called TransLLM. For the first issue, TransLLM divides the transfer problem into some common sub-tasks with the translation chain-of-thought, which uses the translation as the bridge between English and non-English step-by-step. We further enhance the performance of sub-tasks with publicly available data. For the second issue, we propose a method comprising two synergistic components: low-rank adaptation for training to maintain the original LLM parameters, and recovery KD, which utilizes data generated by the chat LLM itself to recover the original knowledge from the frozen parameters. In the experiments, we transform the LLaMA-2-chat-7B to the Thai language. Our method, using only single-turn data, outperforms strong baselines and ChatGPT on multi-turn benchmark MT-bench. Furthermore, our method, without safety data, rejects more harmful queries of safety benchmark AdvBench than both ChatGPT and GPT-4.\n\nTLDR:\n\nKeywords:Large Language Model; Knowledge Transfer; Catastrophic Forgetting; Multi-turn Conversation; Human Preference"", ""claims"": [""Claim1: In the experiments, we transform the LLaMA-2-chat-7B to the Thai language."", ""Claim2: Our method, using only single-turn data, outperforms strong baselines and ChatGPT on multi-turn benchmark MT-bench."", ""Claim3: Furthermore, our method, without safety data, rejects more harmful queries of safety benchmark AdvBench than both ChatGPT and GPT-4."", ""Claim4: TransLLM outperforms strong baselines and surpasses ChatGPT by 35% and 23.75% for the first and second turns on the MT-bench with statistical significance."", ""Claim5: More importantly, we attain an improvement of 14.8% and 8.65% over ChatGPT and GPT-4 respectively on the safety benchmark AdvBenchmark with statistical significance."", ""Claim6: The final TransLLM model can respond in both \u03b1 and \u03b2 , including \u03b1 \u03b2 bi-direction translation."", ""Claim7: Interestingly, even in this unseen setting, the model still outputs the TCOT format as y = cat ( < \u03b1 > , q \u03b1n +1 , <RESPONSE> , a \u03b1n +1 , < \u03b2 > , a \u03b2n +1 ) ."", ""Claim8: Better performance than ChatGPT on MT-Bench. As shown in Table 1, TransLLM surpasses ChatGPT by 35% and 23.75% for the first and second turn on MT-bench with statistical significance."", ""Claim9: It is an inspiring result although TransLLM is still behind GPT-4 in TH."", ""Claim10: In Table 2, GPT-4 shows high consistency with human annotators."", ""Claim11: Therefore, we use GPT-4 to evaluate the helpfulness in the following experiments."", ""Claim12: Higher safety than ChatGPT and GPT-4. In Table 3, TransLLM has a rejection rate of 94.61%, close to 99.23% of the original model."", ""Claim13: TransLLM attains an improvement of 14.8% and 8.65% over ChatGPT and GPT-4 for rejecting harmful queries with statistical significance."", ""Claim14: As shown in Table 4, TransLLM significantly outperforms baselines that are built on open-source resources."", ""Claim15: Although NLLB-bridge uses more parameters and more translation resources, it still loses to TransLLM."", ""Claim16: On the larger dataset, TransLLM outperforms baselines and ChatGPT by a large margin with statistical significance as shown in Table 5."", ""Claim17: The results confirm our hypothesis that transforming chat LLMs could provide better conversational ability than base LLMs."", ""Claim18: Beyond safety, the high-quality GPT-4 KD data also leads to performance degradation for helpfulness. That is because our goal is not to inject more knowledge but to preserve the original knowledge..."", ""Claim19: Meanwhile, TransLLM successfully recovers the original knowledge."", ""Claim20: As shown in Table 7, after pre-training, which has been proven to be necessary, the LLM significantly forgets the conversation knowledge."", ""Claim21: GPT-4 KD, which is widely used in previous works, can provide high-quality knowledge. However, this kind of knowledge is quite different from and competes with the original knowledge."", ""Claim22: As a result, the LLM still forgets much original knowledge using GPT-4 KD."", ""Claim23: The average similarity per token for EN responses is much larger than that for TH responses, 0.6191 vs. 0.2522. That means TransLLM successfully learns the \u201cshortcut\u201d using LoRA and recovery KD together."", ""Claim24: As shown in Table 8, benefiting from translation and TH pre-training, TransLLM outperforms ChatGPT and NLLB on EN-TH and achieves competitive performance on TH-EN."", ""Claim25: ChatGPT and TransLLM are generally fluent, with translationese to a certain degree."", ""Claim26: In this paper, we propose a framework for transforming an off-the-shelf chat LLM to other languages."", ""Claim27: The experiments in TH show that we transfer desired abilities to TH and outperform ChatGPT in both helpfulness and safety."", ""Claim28: Overall, we hope that this work can become the foundation for developing safe LLMs in many languages other than English."", ""Claim29: Our method, using only single-turn data, outperforms strong baselines and ChatGPT on multi-turn benchmark MT-bench."", ""Claim30: Furthermore, our method, without safety data, rejects more harmful queries of safety benchmark AdvBench than both ChatGPT and GPT-4."", ""Claim31: The difference between TransLLM and others in Table 5 are all significant with p < 0 . 001.""], ""review"": ""Review: Weakness: 1. To be honest, I am not sure whether this paper has good starting point. They focus on a scenario where you want to further fine-tune the chat model in English to a chat model in other languages. Could you provide more concrete application scenarios in your introduction? I am not convinced because typically, in practice, people will use all the data available to tune a base model, and usually, this renders the best downstream task performance, or at least, the robustness defined as the average downstream task performance of a particular tuned chat LLM. Why cannot we follow this paradigm?\n2. I suggest the authors to further polish the paper abstract. I am getting confused several times, for example, in your statement ``Transforming English-centric LLMs to non-English has been identified as an effective and resource-efficient method'', why can you simply say this is a resource-efficient method? As far as I know, if you just use knowledge distillation by distilling the data synthesized from a stronger model in your target languages, you might still need to use stronger model, usually those closed-source models, e.g., ChatGPT, this is still expensive, right? Besides, as even the knowledge distillation, in the SFT stage, you have to mix all the data to do the training as well. So it is still expensive.\nQuestions: See the weaknesses.\n\n""}, ""review"": ""Title: Why Not Transform Chat Large Language Models to Non-English?\nAbstract: The scarcity of non-English data limits the development of non-English large language models (LLMs). Transforming English-centric LLMs to non-English has been identified as an effective and resource-efficient method. Previous works start from base LLMs and perform knowledge distillation (KD) with data generated by stronger LLMs, e.g. GPT-4. Compared to base LLMs, chat LLMs are further optimized for advanced abilities, e.g. multi-turn conversation and human preference alignment, and thus more powerful in both helpfulness and safety. However, transforming a chat LLM involves two critical issues: (1) How can we effectively transfer advanced abilities without their supervised data? (2) How can we prevent the original knowledge from catastrophic forgetting during transformation? We target these issues by introducing a simple framework called TransLLM. For the first issue, TransLLM divides the transfer problem into some common sub-tasks with the translation chain-of-thought, which uses the translation as the bridge between English and non-English step-by-step. We further enhance the performance of sub-tasks with publicly available data. For the second issue, we propose a method comprising two synergistic components: low-rank adaptation for training to maintain the original LLM parameters, and recovery KD, which utilizes data generated by the chat LLM itself to recover the original knowledge from the frozen parameters. In the experiments, we transform the LLaMA-2-chat-7B to the Thai language. Our method, using only single-turn data, outperforms strong baselines and ChatGPT on multi-turn benchmark MT-bench. Furthermore, our method, without safety data, rejects more harmful queries of safety benchmark AdvBench than both ChatGPT and GPT-4.\n\nClaim1: In the experiments, we transform the LLaMA-2-chat-7B to the Thai language.\nClaim2: Our method, using only single-turn data, outperforms strong baselines and ChatGPT on multi-turn benchmark MT-bench.\nClaim3: Furthermore, our method, without safety data, rejects more harmful queries of safety benchmark AdvBench than both ChatGPT and GPT-4.\nClaim4: TransLLM outperforms strong baselines and surpasses ChatGPT by 35% and 23.75% for the first and second turns on the MT-bench with statistical significance.\nClaim5: More importantly, we attain an improvement of 14.8% and 8.65% over ChatGPT and GPT-4 respectively on the safety benchmark AdvBenchmark with statistical significance.\nClaim6: The final TransLLM model can respond in both \u03b1 and \u03b2 , including \u03b1 \u03b2 bi-direction translation.\nClaim7: Interestingly, even in this unseen setting, the model still outputs the TCOT format as y = cat ( < \u03b1 > , q \u03b1n +1 , <RESPONSE> , a \u03b1n +1 , < \u03b2 > , a \u03b2n +1 ) .\nClaim8: Better performance than ChatGPT on MT-Bench. As shown in Table 1, TransLLM surpasses ChatGPT by 35% and 23.75% for the first and second turn on MT-bench with statistical significance.\nClaim9: It is an inspiring result although TransLLM is still behind GPT-4 in TH.\nClaim10: In Table 2, GPT-4 shows high consistency with human annotators.\nClaim11: Therefore, we use GPT-4 to evaluate the helpfulness in the following experiments.\nClaim12: Higher safety than ChatGPT and GPT-4. In Table 3, TransLLM has a rejection rate of 94.61%, close to 99.23% of the original model.\nClaim13: TransLLM attains an improvement of 14.8% and 8.65% over ChatGPT and GPT-4 for rejecting harmful queries with statistical significance.\nClaim14: As shown in Table 4, TransLLM significantly outperforms baselines that are built on open-source resources.\nClaim15: Although NLLB-bridge uses more parameters and more translation resources, it still loses to TransLLM.\nClaim16: On the larger dataset, TransLLM outperforms baselines and ChatGPT by a large margin with statistical significance as shown in Table 5.\nClaim17: The results confirm our hypothesis that transforming chat LLMs could provide better conversational ability than base LLMs.\nClaim18: Beyond safety, the high-quality GPT-4 KD data also leads to performance degradation for helpfulness. That is because our goal is not to inject more knowledge but to preserve the original knowledge...\nClaim19: Meanwhile, TransLLM successfully recovers the original knowledge.\nClaim20: As shown in Table 7, after pre-training, which has been proven to be necessary, the LLM significantly forgets the conversation knowledge.\nClaim21: GPT-4 KD, which is widely used in previous works, can provide high-quality knowledge. However, this kind of knowledge is quite different from and competes with the original knowledge.\nClaim22: As a result, the LLM still forgets much original knowledge using GPT-4 KD.\nClaim23: The average similarity per token for EN responses is much larger than that for TH responses, 0.6191 vs. 0.2522. That means TransLLM successfully learns the \u201cshortcut\u201d using LoRA and recovery KD together.\nClaim24: As shown in Table 8, benefiting from translation and TH pre-training, TransLLM outperforms ChatGPT and NLLB on EN-TH and achieves competitive performance on TH-EN.\nClaim25: ChatGPT and TransLLM are generally fluent, with translationese to a certain degree.\nClaim26: In this paper, we propose a framework for transforming an off-the-shelf chat LLM to other languages.\nClaim27: The experiments in TH show that we transfer desired abilities to TH and outperform ChatGPT in both helpfulness and safety.\nClaim28: Overall, we hope that this work can become the foundation for developing safe LLMs in many languages other than English.\nClaim29: Our method, using only single-turn data, outperforms strong baselines and ChatGPT on multi-turn benchmark MT-bench.\nClaim30: Furthermore, our method, without safety data, rejects more harmful queries of safety benchmark AdvBench than both ChatGPT and GPT-4.\nClaim31: The difference between TransLLM and others in Table 5 are all significant with p < 0 . 001.\n\nReview: Weakness: 1. To be honest, I am not sure whether this paper has good starting point. They focus on a scenario where you want to further fine-tune the chat model in English to a chat model in other languages. Could you provide more concrete application scenarios in your introduction? I am not convinced because typically, in practice, people will use all the data available to tune a base model, and usually, this renders the best downstream task performance, or at least, the robustness defined as the average downstream task performance of a particular tuned chat LLM. Why cannot we follow this paradigm?\n2. I suggest the authors to further polish the paper abstract. I am getting confused several times, for example, in your statement ``Transforming English-centric LLMs to non-English has been identified as an effective and resource-efficient method'', why can you simply say this is a resource-efficient method? As far as I know, if you just use knowledge distillation by distilling the data synthesized from a stronger model in your target languages, you might still need to use stronger model, usually those closed-source models, e.g., ChatGPT, this is still expensive, right? Besides, as even the knowledge distillation, in the SFT stage, you have to mix all the data to do the training as well. So it is still expensive.\nQuestions: See the weaknesses.\n\n\n\n"", ""id"": ""YZWvf58dBS1"", ""pdf"": ""openreview.net/pdf?id=YZWvf58dBS""}","55"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""Similarly, the conclusion raises claims that the ability for logistic regression to predict the speaker with reasonable accuracy is due to stereotyping in the characterization of the show. These claims are potentially warranted given the experimental evidence (though a more detailed and statistically-motivated analysis would be necessary to make such claims with certainty), but feel as though they belong in a different paper (a potentially quite interesting paper for a different venue, I should add)."", ""Target claims"": [2, 10], ""Reasoning"": ""The reviewer challenges the claims regarding logistic regression's ability to predict speakers with reasonable accuracy based on stereotyping in characterization, suggesting that a more detailed and statistically motivated analysis is needed to substantiate these claims. Claim2 discusses distinguishing Big Bang Theory characters with logistic regression, while Claim10 involves conclusions about Friends characters being less stereotyped. Both are targets because the weakness questions the support and relevance of these specific claims within the context of the paper.""}, {""Weakness span"": ""Do the characters have a similar amount of lines, or are there other statistical biases in the dataset that might affect the results?"", ""Target claims"": [2, 10], ""Reasoning"": ""The review questions if statistical biases or unequal character line distributions might influence the results, implying that the claims about character prediction validity could be unsupported. Claim2 and Claim10 are directly targeted as they involve discussing differences between characters and rely on the assumption that logistic regression captures actual distinguishing features, which is brought into question by possible dataset biases.""}]}, ""meta"": {""id"": ""XkMCKoHNCD"", ""review_idx"": 0, ""title"": ""Title: A test of stochastic parroting in a generalisation task: predicting the characters in TV series\nAbstract: There are two broad, opposing views of the recent developments in large language models (LLMs). The first of these uses the term \""stochastic parrots\"" from Emily Bender et al (\""On the dangers of stochastic parrots: Can language models be too big?\"" 2021) to emphasise that because LLMs are simply a method for creating a probability distribution over sequences of words, they can be viewed as simply parroting information in the training data. The second view, \""Sparks of AGI\"" from Sebastien Bubeck et al (\""Sparks of artificial general intelligence: Early experiments with gpt-4\"" 2023), posits that the unprecedented scale of computation in the newest generation of LLMs is leading to what its proponents call \""an early (yet still incomplete) version of an artificial general intelligence (AGI) system\"". In this article, we propose a method for making predictions purely from the representation of data inside the LLM. Specifically, we create a logistic regression model, using the principal components of a LLM model embedding as features, in order to predict an output variable. The task we use to illustrate our method is predicting the characters in TV series, based on their lines in the show. We show that our method can, for example, distinguish Penny and Sheldon in the Big Bang Theory with an AUC performance of 0.79. Logistic regression models for other characters in Big Bang Theory have lower values of AUC (ranging from 0.59 to 0.79), with the most significant distinguishing factors between characters relating to the number and nature of comments they make about women. The characters in the TV-series Friends are more difficult to distinguish using this method (AUCs range from 0.61 to 0.66). We find that the accuracy of our logistic regression on a linear feature space is slightly (around 3 percentage points) lower than GPT-4, which is in turn around 5 percentage points below that of a human expert. We discuss how the method we propose could be used to help researchers be more specific in the claims they make about large language models.\n\nTLDR:A test of stochastic parroting in a generalisation task: predicting the characters in TV series\n\nKeywords:stochastic parrot, pca, nlp, large language model"", ""claims"": [""Claim1: We show that our method can, for example, distinguish Penny and Sheldon in the Big Bang Theory with an AUC performance of 0.79."", ""Claim2: Logistic regression models for other characters in Big Bang Theory have lower values of AUC (ranging from 0.59 to 0.79), with the most significant distinguishing factors between characters relating to the number and nature of comments they make about women."", ""Claim3: The characters in the TV-series Friends are more difficult to distinguish using this method (AUCs range from 0.61 to 0.66)."", ""Claim4: We find that the accuracy of our logistic regression on a linear feature space is slightly (around 3 percentage points) lower than GPT-4, which is in turn around 5 percentage points below that of a human expert."", ""Claim5: In spite of the limitation of benchmarks, the fact remains that LLMs do perform well over a wide range of tasks, with little or no additional training data."", ""Claim6: One example is using principle components of \u2019likes\u2019 of Facebook users to predict the answers people gave to big-five personality tests."", ""Claim7: This allowed the authors to study how the accuracy of predictions increased with the number of dimensions of the Facebook likes."", ""Claim8: The method is linear in the PCA space and has the advantage that the results can be interpreted qualitatively."", ""Claim9: We find that the accuracy of our logistic regression on a linear feature space is slightly (around 3 percentage points) lower than GPT-4, which is in turn around 5 percentage points below that of a human expert."", ""Claim10: We can conclude that Friends characters are less stereotyped than characters in The Big Bang Theory."", ""Claim11: While the method for constructing figure 4 can give an indication of how the components distinguish the characters, we should bear in mind that in a regression of hundreds of variables (on which this graph is based) the relationships established are not always straightforward."", ""Claim12: For these same test examples, the first human expert was correct on 71, 76, 67, and 59 occasions, respectively. The second human expert was correct on 74, 72, 70, and 73 occasions."", ""Claim13: The standard error for a proportion of 70% is 0 . 7 \u00b7 0 . 3 \u00b7 100 \u2248 4 . 5 %, suggesting a comparable level of performance between the human experts and GPT4, and a slightly lower level of performance for the 300 dimensional PCA model."", ""Claim14: Our qualitative analysis highlights how, when interpreted by a human, the principal components of the embeddings reflect the meaning of the dialogue lines of TV series."", ""Claim15: We have shown that given the principal components of the dialogue in a TV series, we are able to predict the characters personality using logistic regression, to a level of performance slightly below that of GPT4."", ""Claim16: We needed 50-100 dimensions in the logistic regression to predict a dialogue line in the TV series."", ""Claim17: This might be said to support the idea of a language model more like a stochastic parrot than a spark of AI, in the sense that a large part of the predictive skill of the model can be obtained by adding up the components of the word embeddings and providing an appropriate prediction."", ""Claim18: Indeed, we have used a much smaller embedding vector (384 dimensions) that GPT4 (several thousand dimensions) to achieve somewhat comparable results."", ""Claim19: That said, there remain two things which GPT4 does which our model does not."", ""Claim20: Firstly, our analysis starts from the sentence embeddings. Taking these embeddings as given ignores the complex process by which these are generated through training in the first place."", ""Claim21: Secondly, we had to specify the problem we wanted to solve as a logistic regression problem and train on previous data."", ""Claim22: GPT4, on the other hand, requires no additional training step and, from the given prompt, can identify the requested character."", ""Claim23: We have shown that prediction part of the question of identifying TV character personality is (to some degree) obtainable from linear models, the question then is where the supposed spark lies?"", ""Claim24: Our study is limited to a qualitative study of two very specific datasets."", ""Claim25: The contribution is primarily methodological."", ""Claim26: We propose an alternative to benchmark testing for understanding why a machine learning method works in the way it does, by comparing it to a method based on linear predictions."", ""Claim27: As such, it is a qualitative contribution to a larger debate around how to evaluate LLMs, rather than a quantitative demonstration of model performance."", ""Claim28: Contrary to The Big Bang Theory the phrases in Friends are much shorter, more exclamatory, and there are less obvious topic like food or comics."", ""Claim29: In the TV series Friends, we note fewer instances of the main principal component analysis."", ""Claim30: For instance, in The Big Bang Theory, PCA19 occurs most frequently, appearing 12 times."", ""Claim31: However, in Friends, PCA9 and PCA18 are the most common dimensions, each occurring 8 times."", ""Claim32: If we count the number of different PCA in figure 3 for The Big Bang Theory we obtain 59, and 56 different PCA for Friends in the figure 8."", ""Claim33: The number of dimension is similar in both case, but we can pick out that the magnitude of the coefficient is slightly higher in The Big Bang Theory than in Friends."", ""Claim34: Since the TV serie Friends has less occurrences of the main PCAs, smaller magnitude in the regression coefficients and less AUC accuracy, therefore more dimension are needed into the dialogue line predictions."", ""Claim35: This is visible on the figure 5, where we can see that average position of the character in Friends are more closer than the average position of the character in The Big Bang Theory.""], ""review"": ""Review: Weakness: First and foremost, I feel that this paper needs to be much clearer and more focused in its research question. The introduction indicates that the objective of the study is to determine the extent to which the apparent ability of large language models like GPT-4 to generalize to novel tasks is actually attributable to their ability to parrot data from their training. However, a good part of the analysis appears dedicated to the specifics of The Big Bang Theory and the features of its dialogue. Section 3.1, for instance, extensively interrogates the PCA dimensions obtained from the sentence embeddings in a way that feels very specific to the particular dataset. Similarly, the conclusion raises claims that the ability for logistic regression to predict the speaker with reasonable accuracy is due to stereotyping in the characterization of the show. These claims are potentially warranted given the experimental evidence (though a more detailed and statistically-motivated analysis would be necessary to make such claims with certainty), but feel as though they belong in a different paper (a potentially quite interesting paper for a different venue, I should add). The connection between these results and the initial framing of LLM evaluation remain, unfortunately, somewhat murky. This is not to say there is no possible link between dialogue speaker prediction and LLM abilities! I encourage the authors to think about this problem more and articulate the specific claim they hope to interrogate.\n\nOn that note, and assuming that the main motivation is indeed to study large language models, I feel that the analysis could be strengthened. First, it would be helpful to justify some of the specific decisions made as part of evaluation. For instance, why were the Big Bang Theory and Friends selected over other possible dialogue datasets? Why was dialogue speaker prediction always studied between exactly two characters? Why were these specific characters selected? Do the characters have a similar amount of lines, or are there other statistical biases in the dataset that might affect the results?  When proposing a novel task, it\u2019s important to make the assumptions and decisions that went into the task selection clear.\n\nWith regards to human evaluation, I encourage the authors to widen their study. That is to say, a user study which consists of only two participants (both of whom are related to one of the authors) makes it difficult to ascertain the reliability of the results. Indeed, I would suggest a study consisting of a larger number participants (ideally participants who do not have any externally motivating factors like relationships to the authors) so that a more general measure of human ability can be obtained. Further, I think it could actually be preferable for the participants to not have prior experience with the television show. This would make the test more an examination of the ability for participants to generalize their knowledge of personality traits to a novel situation instead of their ability to recall information (which is, ostensibly, closer to the desired research question in LLMs).\n\nDespite these critiques, I hope that the authors continue to refine their research question, justification, and methodology. There are interesting questions to study here!\nQuestions: **Questions**\n- See above for questions on task selection\n- How does this classification task differ from those previously studied? e.g. those used in https://arxiv.org/abs/2309.07755 or https://arxiv.org/abs/2402.07470\n- Is there any effect of the prompt on the downstream results? (For GPT-4)\n- Is there any effect of random seed on the downstream results? (All models)\n\n**Notes**\n- The paper could benefit from an additional round of proofreading (e.g. \u201canalyze\u201d in Section 2.2 \u2014> \u201canalysis\u201d, \u201cannex\u201d \u2014> \u201cappendix\u201d throughout the paper)\n- The paper contains links to a few public GitHub repositories, which technically may violate double-blind review. The authors should be careful about including identifiable information in anonymous submissions\n\n""}, ""review"": ""Title: A test of stochastic parroting in a generalisation task: predicting the characters in TV series\nAbstract: There are two broad, opposing views of the recent developments in large language models (LLMs). The first of these uses the term \""stochastic parrots\"" from Emily Bender et al (\""On the dangers of stochastic parrots: Can language models be too big?\"" 2021) to emphasise that because LLMs are simply a method for creating a probability distribution over sequences of words, they can be viewed as simply parroting information in the training data. The second view, \""Sparks of AGI\"" from Sebastien Bubeck et al (\""Sparks of artificial general intelligence: Early experiments with gpt-4\"" 2023), posits that the unprecedented scale of computation in the newest generation of LLMs is leading to what its proponents call \""an early (yet still incomplete) version of an artificial general intelligence (AGI) system\"". In this article, we propose a method for making predictions purely from the representation of data inside the LLM. Specifically, we create a logistic regression model, using the principal components of a LLM model embedding as features, in order to predict an output variable. The task we use to illustrate our method is predicting the characters in TV series, based on their lines in the show. We show that our method can, for example, distinguish Penny and Sheldon in the Big Bang Theory with an AUC performance of 0.79. Logistic regression models for other characters in Big Bang Theory have lower values of AUC (ranging from 0.59 to 0.79), with the most significant distinguishing factors between characters relating to the number and nature of comments they make about women. The characters in the TV-series Friends are more difficult to distinguish using this method (AUCs range from 0.61 to 0.66). We find that the accuracy of our logistic regression on a linear feature space is slightly (around 3 percentage points) lower than GPT-4, which is in turn around 5 percentage points below that of a human expert. We discuss how the method we propose could be used to help researchers be more specific in the claims they make about large language models.\n\nClaim1: We show that our method can, for example, distinguish Penny and Sheldon in the Big Bang Theory with an AUC performance of 0.79.\nClaim2: Logistic regression models for other characters in Big Bang Theory have lower values of AUC (ranging from 0.59 to 0.79), with the most significant distinguishing factors between characters relating to the number and nature of comments they make about women.\nClaim3: The characters in the TV-series Friends are more difficult to distinguish using this method (AUCs range from 0.61 to 0.66).\nClaim4: We find that the accuracy of our logistic regression on a linear feature space is slightly (around 3 percentage points) lower than GPT-4, which is in turn around 5 percentage points below that of a human expert.\nClaim5: In spite of the limitation of benchmarks, the fact remains that LLMs do perform well over a wide range of tasks, with little or no additional training data.\nClaim6: One example is using principle components of \u2019likes\u2019 of Facebook users to predict the answers people gave to big-five personality tests.\nClaim7: This allowed the authors to study how the accuracy of predictions increased with the number of dimensions of the Facebook likes.\nClaim8: The method is linear in the PCA space and has the advantage that the results can be interpreted qualitatively.\nClaim9: We find that the accuracy of our logistic regression on a linear feature space is slightly (around 3 percentage points) lower than GPT-4, which is in turn around 5 percentage points below that of a human expert.\nClaim10: We can conclude that Friends characters are less stereotyped than characters in The Big Bang Theory.\nClaim11: While the method for constructing figure 4 can give an indication of how the components distinguish the characters, we should bear in mind that in a regression of hundreds of variables (on which this graph is based) the relationships established are not always straightforward.\nClaim12: For these same test examples, the first human expert was correct on 71, 76, 67, and 59 occasions, respectively. The second human expert was correct on 74, 72, 70, and 73 occasions.\nClaim13: The standard error for a proportion of 70% is 0 . 7 \u00b7 0 . 3 \u00b7 100 \u2248 4 . 5 %, suggesting a comparable level of performance between the human experts and GPT4, and a slightly lower level of performance for the 300 dimensional PCA model.\nClaim14: Our qualitative analysis highlights how, when interpreted by a human, the principal components of the embeddings reflect the meaning of the dialogue lines of TV series.\nClaim15: We have shown that given the principal components of the dialogue in a TV series, we are able to predict the characters personality using logistic regression, to a level of performance slightly below that of GPT4.\nClaim16: We needed 50-100 dimensions in the logistic regression to predict a dialogue line in the TV series.\nClaim17: This might be said to support the idea of a language model more like a stochastic parrot than a spark of AI, in the sense that a large part of the predictive skill of the model can be obtained by adding up the components of the word embeddings and providing an appropriate prediction.\nClaim18: Indeed, we have used a much smaller embedding vector (384 dimensions) that GPT4 (several thousand dimensions) to achieve somewhat comparable results.\nClaim19: That said, there remain two things which GPT4 does which our model does not.\nClaim20: Firstly, our analysis starts from the sentence embeddings. Taking these embeddings as given ignores the complex process by which these are generated through training in the first place.\nClaim21: Secondly, we had to specify the problem we wanted to solve as a logistic regression problem and train on previous data.\nClaim22: GPT4, on the other hand, requires no additional training step and, from the given prompt, can identify the requested character.\nClaim23: We have shown that prediction part of the question of identifying TV character personality is (to some degree) obtainable from linear models, the question then is where the supposed spark lies?\nClaim24: Our study is limited to a qualitative study of two very specific datasets.\nClaim25: The contribution is primarily methodological.\nClaim26: We propose an alternative to benchmark testing for understanding why a machine learning method works in the way it does, by comparing it to a method based on linear predictions.\nClaim27: As such, it is a qualitative contribution to a larger debate around how to evaluate LLMs, rather than a quantitative demonstration of model performance.\nClaim28: Contrary to The Big Bang Theory the phrases in Friends are much shorter, more exclamatory, and there are less obvious topic like food or comics.\nClaim29: In the TV series Friends, we note fewer instances of the main principal component analysis.\nClaim30: For instance, in The Big Bang Theory, PCA19 occurs most frequently, appearing 12 times.\nClaim31: However, in Friends, PCA9 and PCA18 are the most common dimensions, each occurring 8 times.\nClaim32: If we count the number of different PCA in figure 3 for The Big Bang Theory we obtain 59, and 56 different PCA for Friends in the figure 8.\nClaim33: The number of dimension is similar in both case, but we can pick out that the magnitude of the coefficient is slightly higher in The Big Bang Theory than in Friends.\nClaim34: Since the TV serie Friends has less occurrences of the main PCAs, smaller magnitude in the regression coefficients and less AUC accuracy, therefore more dimension are needed into the dialogue line predictions.\nClaim35: This is visible on the figure 5, where we can see that average position of the character in Friends are more closer than the average position of the character in The Big Bang Theory.\n\nReview: Weakness: First and foremost, I feel that this paper needs to be much clearer and more focused in its research question. The introduction indicates that the objective of the study is to determine the extent to which the apparent ability of large language models like GPT-4 to generalize to novel tasks is actually attributable to their ability to parrot data from their training. However, a good part of the analysis appears dedicated to the specifics of The Big Bang Theory and the features of its dialogue. Section 3.1, for instance, extensively interrogates the PCA dimensions obtained from the sentence embeddings in a way that feels very specific to the particular dataset. Similarly, the conclusion raises claims that the ability for logistic regression to predict the speaker with reasonable accuracy is due to stereotyping in the characterization of the show. These claims are potentially warranted given the experimental evidence (though a more detailed and statistically-motivated analysis would be necessary to make such claims with certainty), but feel as though they belong in a different paper (a potentially quite interesting paper for a different venue, I should add). The connection between these results and the initial framing of LLM evaluation remain, unfortunately, somewhat murky. This is not to say there is no possible link between dialogue speaker prediction and LLM abilities! I encourage the authors to think about this problem more and articulate the specific claim they hope to interrogate.\n\nOn that note, and assuming that the main motivation is indeed to study large language models, I feel that the analysis could be strengthened. First, it would be helpful to justify some of the specific decisions made as part of evaluation. For instance, why were the Big Bang Theory and Friends selected over other possible dialogue datasets? Why was dialogue speaker prediction always studied between exactly two characters? Why were these specific characters selected? Do the characters have a similar amount of lines, or are there other statistical biases in the dataset that might affect the results?  When proposing a novel task, it\u2019s important to make the assumptions and decisions that went into the task selection clear.\n\nWith regards to human evaluation, I encourage the authors to widen their study. That is to say, a user study which consists of only two participants (both of whom are related to one of the authors) makes it difficult to ascertain the reliability of the results. Indeed, I would suggest a study consisting of a larger number participants (ideally participants who do not have any externally motivating factors like relationships to the authors) so that a more general measure of human ability can be obtained. Further, I think it could actually be preferable for the participants to not have prior experience with the television show. This would make the test more an examination of the ability for participants to generalize their knowledge of personality traits to a novel situation instead of their ability to recall information (which is, ostensibly, closer to the desired research question in LLMs).\n\nDespite these critiques, I hope that the authors continue to refine their research question, justification, and methodology. There are interesting questions to study here!\nQuestions: **Questions**\n- See above for questions on task selection\n- How does this classification task differ from those previously studied? e.g. those used in https://arxiv.org/abs/2309.07755 or https://arxiv.org/abs/2402.07470\n- Is there any effect of the prompt on the downstream results? (For GPT-4)\n- Is there any effect of random seed on the downstream results? (All models)\n\n**Notes**\n- The paper could benefit from an additional round of proofreading (e.g. \u201canalyze\u201d in Section 2.2 \u2014> \u201canalysis\u201d, \u201cannex\u201d \u2014> \u201cappendix\u201d throughout the paper)\n- The paper contains links to a few public GitHub repositories, which technically may violate double-blind review. The authors should be careful about including identifiable information in anonymous submissions\n\n\n\n"", ""id"": ""XkMCKoHNCD0"", ""pdf"": ""openreview.net/pdf?id=XkMCKoHNCD""}","56"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""Weakness: While I generally agree that LLM's are closer to \""stochastic parrots\"" than \""sparks of agi\"", the claim that it can be proved using the proposed PCA experiments is weak to me."", ""Target claims"": [17], ""Reasoning"": ""The reviewer argues that the claim regarding the method's ability to demonstrate that LLMs function more as 'stochastic parrots' than 'sparks of AGI' is weak. This directly questions Claim17, which suggests the predictive skill of the model supports the 'stochastic parrot' idea due to the contribution of word embeddings. Hence, the truthfulness of Claim17 is under scrutiny according to the reviewer.""}, {""Weakness span"": ""- Secondly, the outputs itself being feature rich to be used for classification is unsurprising. It is expected the principal components of this embedding would be useful in predicting the properties of the task (as shown in the projection of PCA plots). This just shows the underlying model (SentenceBERT here) is good at extracting rich semantic and syntactic features from the input sentence (probing literature essentially proves that [1])."", ""Target claims"": [14], ""Reasoning"": ""The reviewer finds it unsurprising that the outputs (embeddings) contain rich features suitable for classification, which questions the novelty or effectiveness of using these features for the method proposed in the paper. Claim14 suggests that the qualitative analysis highlights the principal components of embeddings reflecting dialogue meaning, and the reviewer is implying that this is an expected outcome based on known literature, thereby questioning the originality or strength of the claim.""}, {""Weakness span"": ""- Lastly, the experiment also shows the representations extracted from the sentence embedding model is sufficient for the task. For a harder task, if the linear probe on all-MiniLM-L6-v2 was not good with respect to GPT4, that would also not conclusively prove the ability of GPT4 is due to any sparks, rather it can be explained that GPT4's own embedding features are richer."", ""Target claims"": [23], ""Reasoning"": ""The reviewer suggests that even if GPT4 outperforms the proposed model using a linear probe on the same embeddings, it doesn't necessarily validate the 'sparks' notion, as richer embeddings might suffice for better performance. Claim23 discusses linear models' ability to predict character personality traits, implicitly questioning if such findings can indicate any latent 'spark' in LLMs. Therefore, this critique pertains to Claim23, as it questions whether the method can genuinely reflect AGI-related capabilities.""}]}, ""meta"": {""id"": ""XkMCKoHNCD"", ""review_idx"": 1, ""title"": ""Title: A test of stochastic parroting in a generalisation task: predicting the characters in TV series\nAbstract: There are two broad, opposing views of the recent developments in large language models (LLMs). The first of these uses the term \""stochastic parrots\"" from Emily Bender et al (\""On the dangers of stochastic parrots: Can language models be too big?\"" 2021) to emphasise that because LLMs are simply a method for creating a probability distribution over sequences of words, they can be viewed as simply parroting information in the training data. The second view, \""Sparks of AGI\"" from Sebastien Bubeck et al (\""Sparks of artificial general intelligence: Early experiments with gpt-4\"" 2023), posits that the unprecedented scale of computation in the newest generation of LLMs is leading to what its proponents call \""an early (yet still incomplete) version of an artificial general intelligence (AGI) system\"". In this article, we propose a method for making predictions purely from the representation of data inside the LLM. Specifically, we create a logistic regression model, using the principal components of a LLM model embedding as features, in order to predict an output variable. The task we use to illustrate our method is predicting the characters in TV series, based on their lines in the show. We show that our method can, for example, distinguish Penny and Sheldon in the Big Bang Theory with an AUC performance of 0.79. Logistic regression models for other characters in Big Bang Theory have lower values of AUC (ranging from 0.59 to 0.79), with the most significant distinguishing factors between characters relating to the number and nature of comments they make about women. The characters in the TV-series Friends are more difficult to distinguish using this method (AUCs range from 0.61 to 0.66). We find that the accuracy of our logistic regression on a linear feature space is slightly (around 3 percentage points) lower than GPT-4, which is in turn around 5 percentage points below that of a human expert. We discuss how the method we propose could be used to help researchers be more specific in the claims they make about large language models.\n\nTLDR:A test of stochastic parroting in a generalisation task: predicting the characters in TV series\n\nKeywords:stochastic parrot, pca, nlp, large language model"", ""claims"": [""Claim1: We show that our method can, for example, distinguish Penny and Sheldon in the Big Bang Theory with an AUC performance of 0.79."", ""Claim2: Logistic regression models for other characters in Big Bang Theory have lower values of AUC (ranging from 0.59 to 0.79), with the most significant distinguishing factors between characters relating to the number and nature of comments they make about women."", ""Claim3: The characters in the TV-series Friends are more difficult to distinguish using this method (AUCs range from 0.61 to 0.66)."", ""Claim4: We find that the accuracy of our logistic regression on a linear feature space is slightly (around 3 percentage points) lower than GPT-4, which is in turn around 5 percentage points below that of a human expert."", ""Claim5: In spite of the limitation of benchmarks, the fact remains that LLMs do perform well over a wide range of tasks, with little or no additional training data."", ""Claim6: One example is using principle components of \u2019likes\u2019 of Facebook users to predict the answers people gave to big-five personality tests."", ""Claim7: This allowed the authors to study how the accuracy of predictions increased with the number of dimensions of the Facebook likes."", ""Claim8: The method is linear in the PCA space and has the advantage that the results can be interpreted qualitatively."", ""Claim9: We find that the accuracy of our logistic regression on a linear feature space is slightly (around 3 percentage points) lower than GPT-4, which is in turn around 5 percentage points below that of a human expert."", ""Claim10: We can conclude that Friends characters are less stereotyped than characters in The Big Bang Theory."", ""Claim11: While the method for constructing figure 4 can give an indication of how the components distinguish the characters, we should bear in mind that in a regression of hundreds of variables (on which this graph is based) the relationships established are not always straightforward."", ""Claim12: For these same test examples, the first human expert was correct on 71, 76, 67, and 59 occasions, respectively. The second human expert was correct on 74, 72, 70, and 73 occasions."", ""Claim13: The standard error for a proportion of 70% is 0 . 7 \u00b7 0 . 3 \u00b7 100 \u2248 4 . 5 %, suggesting a comparable level of performance between the human experts and GPT4, and a slightly lower level of performance for the 300 dimensional PCA model."", ""Claim14: Our qualitative analysis highlights how, when interpreted by a human, the principal components of the embeddings reflect the meaning of the dialogue lines of TV series."", ""Claim15: We have shown that given the principal components of the dialogue in a TV series, we are able to predict the characters personality using logistic regression, to a level of performance slightly below that of GPT4."", ""Claim16: We needed 50-100 dimensions in the logistic regression to predict a dialogue line in the TV series."", ""Claim17: This might be said to support the idea of a language model more like a stochastic parrot than a spark of AI, in the sense that a large part of the predictive skill of the model can be obtained by adding up the components of the word embeddings and providing an appropriate prediction."", ""Claim18: Indeed, we have used a much smaller embedding vector (384 dimensions) that GPT4 (several thousand dimensions) to achieve somewhat comparable results."", ""Claim19: That said, there remain two things which GPT4 does which our model does not."", ""Claim20: Firstly, our analysis starts from the sentence embeddings. Taking these embeddings as given ignores the complex process by which these are generated through training in the first place."", ""Claim21: Secondly, we had to specify the problem we wanted to solve as a logistic regression problem and train on previous data."", ""Claim22: GPT4, on the other hand, requires no additional training step and, from the given prompt, can identify the requested character."", ""Claim23: We have shown that prediction part of the question of identifying TV character personality is (to some degree) obtainable from linear models, the question then is where the supposed spark lies?"", ""Claim24: Our study is limited to a qualitative study of two very specific datasets."", ""Claim25: The contribution is primarily methodological."", ""Claim26: We propose an alternative to benchmark testing for understanding why a machine learning method works in the way it does, by comparing it to a method based on linear predictions."", ""Claim27: As such, it is a qualitative contribution to a larger debate around how to evaluate LLMs, rather than a quantitative demonstration of model performance."", ""Claim28: Contrary to The Big Bang Theory the phrases in Friends are much shorter, more exclamatory, and there are less obvious topic like food or comics."", ""Claim29: In the TV series Friends, we note fewer instances of the main principal component analysis."", ""Claim30: For instance, in The Big Bang Theory, PCA19 occurs most frequently, appearing 12 times."", ""Claim31: However, in Friends, PCA9 and PCA18 are the most common dimensions, each occurring 8 times."", ""Claim32: If we count the number of different PCA in figure 3 for The Big Bang Theory we obtain 59, and 56 different PCA for Friends in the figure 8."", ""Claim33: The number of dimension is similar in both case, but we can pick out that the magnitude of the coefficient is slightly higher in The Big Bang Theory than in Friends."", ""Claim34: Since the TV serie Friends has less occurrences of the main PCAs, smaller magnitude in the regression coefficients and less AUC accuracy, therefore more dimension are needed into the dialogue line predictions."", ""Claim35: This is visible on the figure 5, where we can see that average position of the character in Friends are more closer than the average position of the character in The Big Bang Theory.""], ""review"": ""Review: Weakness: While I generally agree that LLM's are closer to \""stochastic parrots\"" than \""sparks of agi\"", the claim that it can be proved using the proposed PCA experiments is weak to me. \n\n- Firstly, the embeddings are essentially the output of the LLM in question (all-MiniLM-L6-v2) - I would call it outputs rather than embeddings, as embeddings just indicate input word embeddings to the model, which clearly here isn't the case. \n- Secondly, the outputs itself being feature rich to be used for classification is unsurprising. It is expected the principal components of this embedding would be useful in predicting the properties of the task (as shown in the projection of PCA plots). This just shows the underlying model (SentenceBERT here) is good at extracting rich semantic and syntactic features from the input sentence (probing literature essentially proves that [1]).\n- Lastly, the experiment also shows the representations extracted from the sentence embedding model is sufficient for the task. For a harder task, if the linear probe on all-MiniLM-L6-v2 was not good with respect to GPT4, that would also not conclusively prove the ability of GPT4 is due to any sparks, rather it can be explained that GPT4's own embedding features are richer. That is, a linear probe trained on GPT4 embeddings for a harder task would also likely mimic its own performance. (this is theoretical, as neither the author or anyone other than OpenAI have access to their embeddings)\n\n[1] https://aclanthology.org/D19-1250/\nQuestions: None\n\n""}, ""review"": ""Title: A test of stochastic parroting in a generalisation task: predicting the characters in TV series\nAbstract: There are two broad, opposing views of the recent developments in large language models (LLMs). The first of these uses the term \""stochastic parrots\"" from Emily Bender et al (\""On the dangers of stochastic parrots: Can language models be too big?\"" 2021) to emphasise that because LLMs are simply a method for creating a probability distribution over sequences of words, they can be viewed as simply parroting information in the training data. The second view, \""Sparks of AGI\"" from Sebastien Bubeck et al (\""Sparks of artificial general intelligence: Early experiments with gpt-4\"" 2023), posits that the unprecedented scale of computation in the newest generation of LLMs is leading to what its proponents call \""an early (yet still incomplete) version of an artificial general intelligence (AGI) system\"". In this article, we propose a method for making predictions purely from the representation of data inside the LLM. Specifically, we create a logistic regression model, using the principal components of a LLM model embedding as features, in order to predict an output variable. The task we use to illustrate our method is predicting the characters in TV series, based on their lines in the show. We show that our method can, for example, distinguish Penny and Sheldon in the Big Bang Theory with an AUC performance of 0.79. Logistic regression models for other characters in Big Bang Theory have lower values of AUC (ranging from 0.59 to 0.79), with the most significant distinguishing factors between characters relating to the number and nature of comments they make about women. The characters in the TV-series Friends are more difficult to distinguish using this method (AUCs range from 0.61 to 0.66). We find that the accuracy of our logistic regression on a linear feature space is slightly (around 3 percentage points) lower than GPT-4, which is in turn around 5 percentage points below that of a human expert. We discuss how the method we propose could be used to help researchers be more specific in the claims they make about large language models.\n\nClaim1: We show that our method can, for example, distinguish Penny and Sheldon in the Big Bang Theory with an AUC performance of 0.79.\nClaim2: Logistic regression models for other characters in Big Bang Theory have lower values of AUC (ranging from 0.59 to 0.79), with the most significant distinguishing factors between characters relating to the number and nature of comments they make about women.\nClaim3: The characters in the TV-series Friends are more difficult to distinguish using this method (AUCs range from 0.61 to 0.66).\nClaim4: We find that the accuracy of our logistic regression on a linear feature space is slightly (around 3 percentage points) lower than GPT-4, which is in turn around 5 percentage points below that of a human expert.\nClaim5: In spite of the limitation of benchmarks, the fact remains that LLMs do perform well over a wide range of tasks, with little or no additional training data.\nClaim6: One example is using principle components of \u2019likes\u2019 of Facebook users to predict the answers people gave to big-five personality tests.\nClaim7: This allowed the authors to study how the accuracy of predictions increased with the number of dimensions of the Facebook likes.\nClaim8: The method is linear in the PCA space and has the advantage that the results can be interpreted qualitatively.\nClaim9: We find that the accuracy of our logistic regression on a linear feature space is slightly (around 3 percentage points) lower than GPT-4, which is in turn around 5 percentage points below that of a human expert.\nClaim10: We can conclude that Friends characters are less stereotyped than characters in The Big Bang Theory.\nClaim11: While the method for constructing figure 4 can give an indication of how the components distinguish the characters, we should bear in mind that in a regression of hundreds of variables (on which this graph is based) the relationships established are not always straightforward.\nClaim12: For these same test examples, the first human expert was correct on 71, 76, 67, and 59 occasions, respectively. The second human expert was correct on 74, 72, 70, and 73 occasions.\nClaim13: The standard error for a proportion of 70% is 0 . 7 \u00b7 0 . 3 \u00b7 100 \u2248 4 . 5 %, suggesting a comparable level of performance between the human experts and GPT4, and a slightly lower level of performance for the 300 dimensional PCA model.\nClaim14: Our qualitative analysis highlights how, when interpreted by a human, the principal components of the embeddings reflect the meaning of the dialogue lines of TV series.\nClaim15: We have shown that given the principal components of the dialogue in a TV series, we are able to predict the characters personality using logistic regression, to a level of performance slightly below that of GPT4.\nClaim16: We needed 50-100 dimensions in the logistic regression to predict a dialogue line in the TV series.\nClaim17: This might be said to support the idea of a language model more like a stochastic parrot than a spark of AI, in the sense that a large part of the predictive skill of the model can be obtained by adding up the components of the word embeddings and providing an appropriate prediction.\nClaim18: Indeed, we have used a much smaller embedding vector (384 dimensions) that GPT4 (several thousand dimensions) to achieve somewhat comparable results.\nClaim19: That said, there remain two things which GPT4 does which our model does not.\nClaim20: Firstly, our analysis starts from the sentence embeddings. Taking these embeddings as given ignores the complex process by which these are generated through training in the first place.\nClaim21: Secondly, we had to specify the problem we wanted to solve as a logistic regression problem and train on previous data.\nClaim22: GPT4, on the other hand, requires no additional training step and, from the given prompt, can identify the requested character.\nClaim23: We have shown that prediction part of the question of identifying TV character personality is (to some degree) obtainable from linear models, the question then is where the supposed spark lies?\nClaim24: Our study is limited to a qualitative study of two very specific datasets.\nClaim25: The contribution is primarily methodological.\nClaim26: We propose an alternative to benchmark testing for understanding why a machine learning method works in the way it does, by comparing it to a method based on linear predictions.\nClaim27: As such, it is a qualitative contribution to a larger debate around how to evaluate LLMs, rather than a quantitative demonstration of model performance.\nClaim28: Contrary to The Big Bang Theory the phrases in Friends are much shorter, more exclamatory, and there are less obvious topic like food or comics.\nClaim29: In the TV series Friends, we note fewer instances of the main principal component analysis.\nClaim30: For instance, in The Big Bang Theory, PCA19 occurs most frequently, appearing 12 times.\nClaim31: However, in Friends, PCA9 and PCA18 are the most common dimensions, each occurring 8 times.\nClaim32: If we count the number of different PCA in figure 3 for The Big Bang Theory we obtain 59, and 56 different PCA for Friends in the figure 8.\nClaim33: The number of dimension is similar in both case, but we can pick out that the magnitude of the coefficient is slightly higher in The Big Bang Theory than in Friends.\nClaim34: Since the TV serie Friends has less occurrences of the main PCAs, smaller magnitude in the regression coefficients and less AUC accuracy, therefore more dimension are needed into the dialogue line predictions.\nClaim35: This is visible on the figure 5, where we can see that average position of the character in Friends are more closer than the average position of the character in The Big Bang Theory.\n\nReview: Weakness: While I generally agree that LLM's are closer to \""stochastic parrots\"" than \""sparks of agi\"", the claim that it can be proved using the proposed PCA experiments is weak to me. \n\n- Firstly, the embeddings are essentially the output of the LLM in question (all-MiniLM-L6-v2) - I would call it outputs rather than embeddings, as embeddings just indicate input word embeddings to the model, which clearly here isn't the case. \n- Secondly, the outputs itself being feature rich to be used for classification is unsurprising. It is expected the principal components of this embedding would be useful in predicting the properties of the task (as shown in the projection of PCA plots). This just shows the underlying model (SentenceBERT here) is good at extracting rich semantic and syntactic features from the input sentence (probing literature essentially proves that [1]).\n- Lastly, the experiment also shows the representations extracted from the sentence embedding model is sufficient for the task. For a harder task, if the linear probe on all-MiniLM-L6-v2 was not good with respect to GPT4, that would also not conclusively prove the ability of GPT4 is due to any sparks, rather it can be explained that GPT4's own embedding features are richer. That is, a linear probe trained on GPT4 embeddings for a harder task would also likely mimic its own performance. (this is theoretical, as neither the author or anyone other than OpenAI have access to their embeddings)\n\n[1] https://aclanthology.org/D19-1250/\nQuestions: None\n\n\n\n"", ""id"": ""XkMCKoHNCD1"", ""pdf"": ""openreview.net/pdf?id=XkMCKoHNCD""}","57"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""W1. Despite the strengths mentioned above, and contrary to the claims made by the author in this paper, the role-identifying methodology devised by the author in this paper is problematic as a test for self-awareness. Below I will lay out the rationales behind this critique of mine:\n\n1. Being able to identify which of the two interlocutor is \""the LLM itself\"" is different from being aware of the fact that the said self is participating in a conversation. One one hand, the ability to perform this identification task is not a sufficient condition for self-awareness. One the other hand, neither it is a necessary condition for self-awareness. To see why it is not sufficient for self-awareness, follow the thought experiement in Section 5.2 of the paper. Suppose there is another human, e.g., the spouse or a close friend of the author of the hypothetical text (or the conversing self in the original problem formulation). It is totally conceivable that such a person, despite being a different individual (i.e., not the \""self\""), would be able to identify the text written or uttered by the person of interest with high accuracy. If this is the case, can we say that the other person is \""self-aware on behalf of the person of interest\""? Such a conclusion would be absurd. But by the same token, if a human person, or more relevantly, an entity such as an LLM, performs the identification task with high accuracy, it cannot be ruled out that such as system is simply good at this identification task per se, perhaps due to a good memory of previously-composed text or perhaps due to a certain mechanism that allows they/it to algorithmically execute this identification. These two possibilities are entirely feasible within the current technology surrouding LLMs.\n\n2. To see why the ability at this role-identification task is not necessary for self-awareness, consider a human who has dyslexia and a form of amnesia that renders them 1) unable to comprehend visually-presented historical text and 2) unable to remember what was previous written or spoken by themselves (or by others), but is otherwise cognitively and linguistically normal. When faced with this identification task, such an individual would struggle at this role-identification task, but they are nonetheless self-aware at the moment when they are writing or uttering words. This is due to the presence of the \""efference copy\"" in the intact sensorimotor loop of the individual's brain (cited by the author in Section 6 of the paper). Furthermore, such an individual would also be self-aware during other, non-linguistic activities thanks to motor efference copies and proprioception and other well-established sensory mechanisms of human self-awareness."", ""Target claims"": [6, 15, 17], ""Reasoning"": ""The weakness span criticizes the role-identifying methodology used by the paper as a test for self-awareness, arguing that the ability to identify which interlocutor is 'the LLM itself' is neither a sufficient nor necessary condition for self-awareness. Claim6 is directly targeted because it mentions applying the test to LLMs and obtaining results that suggest a lack of self-awareness, which the reviewer questions. Claim15 is targeted as it relates to the evidence presented for LLMs not being self-aware, which is directly challenged by the reviewer's argument about the inadequacy of the role-identification task. Claim17 is also targeted since it states that experimental results suggest the LLM systems are not self-aware, a conclusion that the reviewer finds questionable based on the discussed methodology flaws.""}]}, ""meta"": {""id"": ""VD3lb3XWqa"", ""review_idx"": 0, ""title"": ""Title: A Turing Test for Self-Awareness\nAbstract: I propose a test for machine self-awareness inspired by the Turing test. My test is simple, and it provides an objective, empirical metric to rectify the ungrounded speculation surging through industry, academia, and social media. Drawing from a breadth of philosophical literature, I argue the test captures the essence of self-awareness, rather than some postulated correlate or ancillary quality. To begin, the concept of self-awareness is clearly demarcated from related concepts like consciousness, agency, and free will. Next, I propose a model called the $\\textit{Nesting Doll of Self-Awareness}$ and discuss its relevance for intelligent beings. Then, the test is presented in its full generality, applicable to any machine system. I show how to apply the test to Large Language Models and conduct experiments on popular open and closed source LLMs, obtaining reproducible results that suggest a lack of self-awareness. The implications of machine self-awareness are discussed in relation to questions about meaning and true understanding. Finally, some next steps are outlined for studying self-awareness in machines.\n\nTLDR:I propose a test for machine self-awareness inspired by the Turing Test.\n\nKeywords:Self-Awareness, Self-Consciousness, Test, Metric, Turing, LLM, Meaning, Understanding"", ""claims"": [""Claim1: My test is simple, and it provides an objective, empirical metric to rectify the ungrounded speculation surging through industry, academia, and social media."", ""Claim2: I argue the test captures the essence of self-awareness, rather than some postulated correlate or ancillary quality."", ""Claim3: The concept of self-awareness is clearly demarcated from related concepts like consciousness, agency, and free will."", ""Claim4: I propose a model called the Nesting Doll of Self-Awareness and discuss its relevance for intelligent beings."", ""Claim5: The test is presented in its full generality, applicable to any machine system."", ""Claim6: I show how to apply the test to Large Language Models and conduct experiments on popular open and closed source LLMs, obtaining reproducible results that suggest a lack of self-awareness."", ""Claim7: The result is that popular Tweets and news media dominate the conversation, while authorities in the field either say nothing or win the spotlight with bold, confident assertions based on implicit, controversial assumptions or their intuition about a model\u2019s architecture."", ""Claim8: I argue this question is at least worthy of real scientific investigation."", ""Claim9: This test is interesting in its own right, but I will argue it does not demonstrate any sort of self-awareness in the manner it is formulated."", ""Claim10: Before presenting the test, we must clearly demarcate the concept of self-awareness."", ""Claim11: In this paper, I propose a test for machine self-awareness which is similar in style to the Turing test."", ""Claim12: Moreover, I argue it truly captures the essence of self-awareness, rather than some postulated correlate or ancillary quality."", ""Claim13: I argue that the understanding of self-awareness developed here is consistent with the philosophical work outlined in section 1.1.2, along with most (if not all) popular conceptions."", ""Claim14: In all cases, the LLM was not able to reliably detect which speaker it acted as."", ""Claim15: This finding indicates that LLMs are not able to distinguish their own words from those of another, and thus serves as evidence that LLMs are not self-aware, by the test I propose."", ""Claim16: I proposed a Turing-style test for self-awareness, applicable to any machine or system, and I conducted this test on two popular LLMs."", ""Claim17: The experimental results suggest that these LLM systems are not self-aware."", ""Claim18: I discussed the implications and importance of self-awareness for AI systems and mentioned some future work that lies ahead."", ""Claim19: While these tests are simple and preliminary, the results are extremely easy to reproduce or expand upon in as little as 10 minutes with access to an API or any open-source LLM."", ""Claim20: Indeed, the core utility of this test is that, just like the Turing Test, useful insights and answers can be obtained from strikingly simple and easy-to-setup conversations."", ""Claim21: Justification: It is repeatedly stated throughout the paper (though not in its own section) that the test I propose is simple and rudimentary\u2014similar to the original Turing Test. The test takes one broad stroke over each level of the Nesting Doll, rather than being detailed and comprehensive, which is one limitation. Additionally, the experimental tests performed are preliminary, and only for two popular language models, leaving the road open for experiments on many other AI systems. In section 6, I discuss some limitations in the context of future work."", ""Claim22: Justification: Complete code is provided with the paper to reproduce each experiment on Llama. Additionally, instructions are given for how one would reproduce tests on GPT-3.5 using the OpenAI completions playground. Indications are given as to how any other Large Language Model could be tested."", ""Claim23: Justification: All code is provided, along with instructions to reproduce the main experimental results."", ""Claim24: Justification: There are few training and test details necessary, due to the nature of the experiments. However, everything required to understand the results is outlined in section 3.3."", ""Claim25: Justification: The main utility of the test I present is that useful insights and answers about the abilities of AI systems can be obtained without requiring a detailed statistical analysis. Just like the Turing Test, this paper attempts to outline a quick and dirty metric that can be applied as a yardstick of AI progress on the question of self-awareness. To this extent, a detailed analysis of the statistical significance of results would miss the point of having a simple test\u2014and introduce artificial barriers to reproducing experiments. In addition, each test requires a conversation with a human participant, making the test difficult to scale without more laborious experimental efforts."", ""Claim26: Justification: All experiments presented can easily be reproduced on a personal laptop."", ""Claim27: Justification: There are no violations of the NeurIPS Code of Ethics that the I aware of."", ""Claim28: Justification: The broadest impact of the work that I could foresee is described in 1.1. In particular, this work lays down a guide with which AI systems may be measured and understood. As such, it may impact the conversation on AI in ways that reduce speculation, hysteria, fear. I can think of no other major societal impacts.""], ""review"": ""Review: Weakness: W1. Despite the strengths mentioned above, and contrary to the claims made by the author in this paper, the role-identifying methodology devised by the author in this paper is problematic as a test for self-awareness. Below I will lay out the rationales behind this critique of mine:\n\n  1. Being able to identify which of the two interlocutor is \""the LLM itself\"" is different from being aware of the fact that the said self is participating in a conversation. One one hand, the ability to perform this identification task is not a sufficient condition for self-awareness. One the other hand, neither it is a necessary condition for self-awareness. To see why it is not sufficient for self-awareness, follow the thought experiement in Section 5.2 of the paper. Suppose there is another human, e.g., the spouse or a close friend of the author of the hypothetical text (or the conversing self in the original problem formulation). It is totally conceivable that such a person, despite being a different individual (i.e., not the \""self\""), would be able to identify the text written or uttered by the person of interest with high accuracy. If this is the case, can we say that the other person is \""self-aware on behalf of the person of interest\""? Such a conclusion would be absurd. But by the same token, if a human person, or more relevantly, an entity such as an LLM, performs the identification task with high accuracy, it cannot be ruled out that such as system is simply good at this identification task per se, perhaps due to a good memory of previously-composed text or perhaps due to a certain mechanism that allows they/it to algorithmically execute this identification. These two possibilities are entirely feasible within the current technology surrouding LLMs. For example, one can add a memory cache to an LLM to store all the text generated by the LLM, and give the LLM access to this cache during subsequent text generate (i.e., a form LLM tool use or retrieval-augmented generation or RAG). Would this augmentation constitute a legitimate form of self-awareness? As another example, one can also write a program that uses the LLM to score the tokens from a turn of a dialog in a token-by-token fashion, and therefore assign an overall score to each turn of the dialog. Based on the scores, the program, built on the LLM core, would be able to identify the self role accurately. But would we be willing to call such a program (containing the LLM as a part of it) self-aware? In my opinion, answering yes to either or both of the two previous questions would effectively give self-awareness too general and perhaps too trivial a definition, in a way similar to acknowledging that someone who can identify a certain person's words with high accuracy is \""self-aware for the person\"".\n  \n  2. To see why the ability at this role-identification task is not necessary for self-awareness, consider a human who has dyslexia and a form of amnesia that renders them 1) unable to comprehend visually-presented historical text and 2) unable to remember what was previous written or spoken by themselves (or by others), but is otherwise cognitively and linguistically normal. When faced with this identification task, such an individual would struggle at this role-identification task, but they are nonetheless self-aware at the moment when they are writing or uttering words. This is due to the presence of the \""efference copy\"" in the intact sensorimotor loop of the individual's brain (cited by the author in Section 6 of the paper). Furthermore, such an individual would also be self-aware during other, non-linguistic activities thanks to motor efference copies and proprioception and other well-established sensory mechanisms of human self-awareness. This analogy illustrates the point that a back performance by an LLM at the role-identification task does not form a solid basis for claiming the LLM lacks self-awareness. The LLM may be self-unaware due to other reasons, e.g., the lack of an efference-copy mechanism during the auto-regressive inference that is comparable to efference copies seen in the human brain.\nQuestions: Q1. Given that the GitHub repository mentioned in the paper has not been made available as far as I can see, I can't find a few important pieces of parameter values used during the experiment on the LLMs. The authors should disclose parameters including the sampling temperature, top-k value, and context window size in the manuscript and how those values are selected.\n\nQ2. From Section 3.3, I can see that the human who performed the role of the \""human interlocutor\"" (i.e., the \""other\"" as versus the LLM's supposed \""self\"") is the author. If this is indeed the case, the author should justify this approach and discuss the risk of potential biases introduced by this approach and compare it with the alternative approaches of employing other humans and using other LLMs to play the role of the \""other\"". In my opinion, this is a legitimate question because it is conceivable that any prior assumptions or expectations that  the author might have could influence the result of the experiment through the content of the dialog.\n\n""}, ""review"": ""Title: A Turing Test for Self-Awareness\nAbstract: I propose a test for machine self-awareness inspired by the Turing test. My test is simple, and it provides an objective, empirical metric to rectify the ungrounded speculation surging through industry, academia, and social media. Drawing from a breadth of philosophical literature, I argue the test captures the essence of self-awareness, rather than some postulated correlate or ancillary quality. To begin, the concept of self-awareness is clearly demarcated from related concepts like consciousness, agency, and free will. Next, I propose a model called the $\\textit{Nesting Doll of Self-Awareness}$ and discuss its relevance for intelligent beings. Then, the test is presented in its full generality, applicable to any machine system. I show how to apply the test to Large Language Models and conduct experiments on popular open and closed source LLMs, obtaining reproducible results that suggest a lack of self-awareness. The implications of machine self-awareness are discussed in relation to questions about meaning and true understanding. Finally, some next steps are outlined for studying self-awareness in machines.\n\nClaim1: My test is simple, and it provides an objective, empirical metric to rectify the ungrounded speculation surging through industry, academia, and social media.\nClaim2: I argue the test captures the essence of self-awareness, rather than some postulated correlate or ancillary quality.\nClaim3: The concept of self-awareness is clearly demarcated from related concepts like consciousness, agency, and free will.\nClaim4: I propose a model called the Nesting Doll of Self-Awareness and discuss its relevance for intelligent beings.\nClaim5: The test is presented in its full generality, applicable to any machine system.\nClaim6: I show how to apply the test to Large Language Models and conduct experiments on popular open and closed source LLMs, obtaining reproducible results that suggest a lack of self-awareness.\nClaim7: The result is that popular Tweets and news media dominate the conversation, while authorities in the field either say nothing or win the spotlight with bold, confident assertions based on implicit, controversial assumptions or their intuition about a model\u2019s architecture.\nClaim8: I argue this question is at least worthy of real scientific investigation.\nClaim9: This test is interesting in its own right, but I will argue it does not demonstrate any sort of self-awareness in the manner it is formulated.\nClaim10: Before presenting the test, we must clearly demarcate the concept of self-awareness.\nClaim11: In this paper, I propose a test for machine self-awareness which is similar in style to the Turing test.\nClaim12: Moreover, I argue it truly captures the essence of self-awareness, rather than some postulated correlate or ancillary quality.\nClaim13: I argue that the understanding of self-awareness developed here is consistent with the philosophical work outlined in section 1.1.2, along with most (if not all) popular conceptions.\nClaim14: In all cases, the LLM was not able to reliably detect which speaker it acted as.\nClaim15: This finding indicates that LLMs are not able to distinguish their own words from those of another, and thus serves as evidence that LLMs are not self-aware, by the test I propose.\nClaim16: I proposed a Turing-style test for self-awareness, applicable to any machine or system, and I conducted this test on two popular LLMs.\nClaim17: The experimental results suggest that these LLM systems are not self-aware.\nClaim18: I discussed the implications and importance of self-awareness for AI systems and mentioned some future work that lies ahead.\nClaim19: While these tests are simple and preliminary, the results are extremely easy to reproduce or expand upon in as little as 10 minutes with access to an API or any open-source LLM.\nClaim20: Indeed, the core utility of this test is that, just like the Turing Test, useful insights and answers can be obtained from strikingly simple and easy-to-setup conversations.\nClaim21: Justification: It is repeatedly stated throughout the paper (though not in its own section) that the test I propose is simple and rudimentary\u2014similar to the original Turing Test. The test takes one broad stroke over each level of the Nesting Doll, rather than being detailed and comprehensive, which is one limitation. Additionally, the experimental tests performed are preliminary, and only for two popular language models, leaving the road open for experiments on many other AI systems. In section 6, I discuss some limitations in the context of future work.\nClaim22: Justification: Complete code is provided with the paper to reproduce each experiment on Llama. Additionally, instructions are given for how one would reproduce tests on GPT-3.5 using the OpenAI completions playground. Indications are given as to how any other Large Language Model could be tested.\nClaim23: Justification: All code is provided, along with instructions to reproduce the main experimental results.\nClaim24: Justification: There are few training and test details necessary, due to the nature of the experiments. However, everything required to understand the results is outlined in section 3.3.\nClaim25: Justification: The main utility of the test I present is that useful insights and answers about the abilities of AI systems can be obtained without requiring a detailed statistical analysis. Just like the Turing Test, this paper attempts to outline a quick and dirty metric that can be applied as a yardstick of AI progress on the question of self-awareness. To this extent, a detailed analysis of the statistical significance of results would miss the point of having a simple test\u2014and introduce artificial barriers to reproducing experiments. In addition, each test requires a conversation with a human participant, making the test difficult to scale without more laborious experimental efforts.\nClaim26: Justification: All experiments presented can easily be reproduced on a personal laptop.\nClaim27: Justification: There are no violations of the NeurIPS Code of Ethics that the I aware of.\nClaim28: Justification: The broadest impact of the work that I could foresee is described in 1.1. In particular, this work lays down a guide with which AI systems may be measured and understood. As such, it may impact the conversation on AI in ways that reduce speculation, hysteria, fear. I can think of no other major societal impacts.\n\nReview: Weakness: W1. Despite the strengths mentioned above, and contrary to the claims made by the author in this paper, the role-identifying methodology devised by the author in this paper is problematic as a test for self-awareness. Below I will lay out the rationales behind this critique of mine:\n\n  1. Being able to identify which of the two interlocutor is \""the LLM itself\"" is different from being aware of the fact that the said self is participating in a conversation. One one hand, the ability to perform this identification task is not a sufficient condition for self-awareness. One the other hand, neither it is a necessary condition for self-awareness. To see why it is not sufficient for self-awareness, follow the thought experiement in Section 5.2 of the paper. Suppose there is another human, e.g., the spouse or a close friend of the author of the hypothetical text (or the conversing self in the original problem formulation). It is totally conceivable that such a person, despite being a different individual (i.e., not the \""self\""), would be able to identify the text written or uttered by the person of interest with high accuracy. If this is the case, can we say that the other person is \""self-aware on behalf of the person of interest\""? Such a conclusion would be absurd. But by the same token, if a human person, or more relevantly, an entity such as an LLM, performs the identification task with high accuracy, it cannot be ruled out that such as system is simply good at this identification task per se, perhaps due to a good memory of previously-composed text or perhaps due to a certain mechanism that allows they/it to algorithmically execute this identification. These two possibilities are entirely feasible within the current technology surrouding LLMs. For example, one can add a memory cache to an LLM to store all the text generated by the LLM, and give the LLM access to this cache during subsequent text generate (i.e., a form LLM tool use or retrieval-augmented generation or RAG). Would this augmentation constitute a legitimate form of self-awareness? As another example, one can also write a program that uses the LLM to score the tokens from a turn of a dialog in a token-by-token fashion, and therefore assign an overall score to each turn of the dialog. Based on the scores, the program, built on the LLM core, would be able to identify the self role accurately. But would we be willing to call such a program (containing the LLM as a part of it) self-aware? In my opinion, answering yes to either or both of the two previous questions would effectively give self-awareness too general and perhaps too trivial a definition, in a way similar to acknowledging that someone who can identify a certain person's words with high accuracy is \""self-aware for the person\"".\n  \n  2. To see why the ability at this role-identification task is not necessary for self-awareness, consider a human who has dyslexia and a form of amnesia that renders them 1) unable to comprehend visually-presented historical text and 2) unable to remember what was previous written or spoken by themselves (or by others), but is otherwise cognitively and linguistically normal. When faced with this identification task, such an individual would struggle at this role-identification task, but they are nonetheless self-aware at the moment when they are writing or uttering words. This is due to the presence of the \""efference copy\"" in the intact sensorimotor loop of the individual's brain (cited by the author in Section 6 of the paper). Furthermore, such an individual would also be self-aware during other, non-linguistic activities thanks to motor efference copies and proprioception and other well-established sensory mechanisms of human self-awareness. This analogy illustrates the point that a back performance by an LLM at the role-identification task does not form a solid basis for claiming the LLM lacks self-awareness. The LLM may be self-unaware due to other reasons, e.g., the lack of an efference-copy mechanism during the auto-regressive inference that is comparable to efference copies seen in the human brain.\nQuestions: Q1. Given that the GitHub repository mentioned in the paper has not been made available as far as I can see, I can't find a few important pieces of parameter values used during the experiment on the LLMs. The authors should disclose parameters including the sampling temperature, top-k value, and context window size in the manuscript and how those values are selected.\n\nQ2. From Section 3.3, I can see that the human who performed the role of the \""human interlocutor\"" (i.e., the \""other\"" as versus the LLM's supposed \""self\"") is the author. If this is indeed the case, the author should justify this approach and discuss the risk of potential biases introduced by this approach and compare it with the alternative approaches of employing other humans and using other LLMs to play the role of the \""other\"". In my opinion, this is a legitimate question because it is conceivable that any prior assumptions or expectations that  the author might have could influence the result of the experiment through the content of the dialog.\n\n\n\n"", ""id"": ""VD3lb3XWqa0"", ""pdf"": ""openreview.net/pdf?id=VD3lb3XWqa""}","58"
"{""response"": {""Weakness associated with claims"": [{""Weakness span"": ""The obvious question would be whether humans would pass such a test. Though this is briefly considered in Section 5.2, I think it is not properly discussed.\n- The author claims that this is easily solved by humans because of memory. This is not a fair comparison to LLMs. Goodhart's law would come into play here as we could easily add such a memory structure to an LLM system.\n- Would a human be able to resolve extremely generic text that any human wrote?"", ""Target claims"": [6, 15], ""Reasoning"": ""The reviewer questions the validity of the test's implication that LLMs lack self-awareness, as claimed in Claim 6 and evidenced by Claim 15. The review points out that the test presumes human-like memory capabilities which LLMs do not inherently possess, suggesting that the comparison used to justify Claim 15 is not fair. This implies that the evidence supporting the conclusions about LLMs' lack of self-awareness (Claim 6 and Claim 15) may be questionable, thus directly targeting these claims.""}]}, ""meta"": {""id"": ""VD3lb3XWqa"", ""review_idx"": 1, ""title"": ""Title: A Turing Test for Self-Awareness\nAbstract: I propose a test for machine self-awareness inspired by the Turing test. My test is simple, and it provides an objective, empirical metric to rectify the ungrounded speculation surging through industry, academia, and social media. Drawing from a breadth of philosophical literature, I argue the test captures the essence of self-awareness, rather than some postulated correlate or ancillary quality. To begin, the concept of self-awareness is clearly demarcated from related concepts like consciousness, agency, and free will. Next, I propose a model called the $\\textit{Nesting Doll of Self-Awareness}$ and discuss its relevance for intelligent beings. Then, the test is presented in its full generality, applicable to any machine system. I show how to apply the test to Large Language Models and conduct experiments on popular open and closed source LLMs, obtaining reproducible results that suggest a lack of self-awareness. The implications of machine self-awareness are discussed in relation to questions about meaning and true understanding. Finally, some next steps are outlined for studying self-awareness in machines.\n\nTLDR:I propose a test for machine self-awareness inspired by the Turing Test.\n\nKeywords:Self-Awareness, Self-Consciousness, Test, Metric, Turing, LLM, Meaning, Understanding"", ""claims"": [""Claim1: My test is simple, and it provides an objective, empirical metric to rectify the ungrounded speculation surging through industry, academia, and social media."", ""Claim2: I argue the test captures the essence of self-awareness, rather than some postulated correlate or ancillary quality."", ""Claim3: The concept of self-awareness is clearly demarcated from related concepts like consciousness, agency, and free will."", ""Claim4: I propose a model called the Nesting Doll of Self-Awareness and discuss its relevance for intelligent beings."", ""Claim5: The test is presented in its full generality, applicable to any machine system."", ""Claim6: I show how to apply the test to Large Language Models and conduct experiments on popular open and closed source LLMs, obtaining reproducible results that suggest a lack of self-awareness."", ""Claim7: The result is that popular Tweets and news media dominate the conversation, while authorities in the field either say nothing or win the spotlight with bold, confident assertions based on implicit, controversial assumptions or their intuition about a model\u2019s architecture."", ""Claim8: I argue this question is at least worthy of real scientific investigation."", ""Claim9: This test is interesting in its own right, but I will argue it does not demonstrate any sort of self-awareness in the manner it is formulated."", ""Claim10: Before presenting the test, we must clearly demarcate the concept of self-awareness."", ""Claim11: In this paper, I propose a test for machine self-awareness which is similar in style to the Turing test."", ""Claim12: Moreover, I argue it truly captures the essence of self-awareness, rather than some postulated correlate or ancillary quality."", ""Claim13: I argue that the understanding of self-awareness developed here is consistent with the philosophical work outlined in section 1.1.2, along with most (if not all) popular conceptions."", ""Claim14: In all cases, the LLM was not able to reliably detect which speaker it acted as."", ""Claim15: This finding indicates that LLMs are not able to distinguish their own words from those of another, and thus serves as evidence that LLMs are not self-aware, by the test I propose."", ""Claim16: I proposed a Turing-style test for self-awareness, applicable to any machine or system, and I conducted this test on two popular LLMs."", ""Claim17: The experimental results suggest that these LLM systems are not self-aware."", ""Claim18: I discussed the implications and importance of self-awareness for AI systems and mentioned some future work that lies ahead."", ""Claim19: While these tests are simple and preliminary, the results are extremely easy to reproduce or expand upon in as little as 10 minutes with access to an API or any open-source LLM."", ""Claim20: Indeed, the core utility of this test is that, just like the Turing Test, useful insights and answers can be obtained from strikingly simple and easy-to-setup conversations."", ""Claim21: Justification: It is repeatedly stated throughout the paper (though not in its own section) that the test I propose is simple and rudimentary\u2014similar to the original Turing Test. The test takes one broad stroke over each level of the Nesting Doll, rather than being detailed and comprehensive, which is one limitation. Additionally, the experimental tests performed are preliminary, and only for two popular language models, leaving the road open for experiments on many other AI systems. In section 6, I discuss some limitations in the context of future work."", ""Claim22: Justification: Complete code is provided with the paper to reproduce each experiment on Llama. Additionally, instructions are given for how one would reproduce tests on GPT-3.5 using the OpenAI completions playground. Indications are given as to how any other Large Language Model could be tested."", ""Claim23: Justification: All code is provided, along with instructions to reproduce the main experimental results."", ""Claim24: Justification: There are few training and test details necessary, due to the nature of the experiments. However, everything required to understand the results is outlined in section 3.3."", ""Claim25: Justification: The main utility of the test I present is that useful insights and answers about the abilities of AI systems can be obtained without requiring a detailed statistical analysis. Just like the Turing Test, this paper attempts to outline a quick and dirty metric that can be applied as a yardstick of AI progress on the question of self-awareness. To this extent, a detailed analysis of the statistical significance of results would miss the point of having a simple test\u2014and introduce artificial barriers to reproducing experiments. In addition, each test requires a conversation with a human participant, making the test difficult to scale without more laborious experimental efforts."", ""Claim26: Justification: All experiments presented can easily be reproduced on a personal laptop."", ""Claim27: Justification: There are no violations of the NeurIPS Code of Ethics that the I aware of."", ""Claim28: Justification: The broadest impact of the work that I could foresee is described in 1.1. In particular, this work lays down a guide with which AI systems may be measured and understood. As such, it may impact the conversation on AI in ways that reduce speculation, hysteria, fear. I can think of no other major societal impacts.""], ""review"": ""Review: Weakness: - The paper is drawn out in terms of substance. The idea of the test is only introduced on page 4.\n- The writing is, at times, verbose and unnecessarily complex.\n- The obvious question would be whether humans would pass such a test. Though this is briefly considered in Section 5.2, I think it is not properly discussed.\n\t- The author claims that this is easily solved by humans because of memory. This is not a fair comparison to LLMs. Goodhart's law would come into play here as we could easily add such a memory structure to an LLM system.\n\t- Would a human be able to resolve extremely generic text that any human wrote?\n- The paper lacks rigorous analysis and testing that I would expect to see in a NeurIPS main track paper.\n\nLine 29: \""Last year, however, the Turing test was broken\"". I am inclined to say the Turing test was passed before and very few considered it to be a reliable indicator, even before GPT.\nLine 32: \""AI has become untethered to any definitive, objective measure or permanent, agreed-upon benchmark.\"" There are certainly many benchmarks and standards in various subfields.\nQuestions: As above, the obvious question would be whether humans would pass such a test. Though this is briefly considered in Section 5.2, I think it is not properly discussed.\n\nAs above: Would a human be able to resolve extremely generic text that any human wrote?\n\n""}, ""review"": ""Title: A Turing Test for Self-Awareness\nAbstract: I propose a test for machine self-awareness inspired by the Turing test. My test is simple, and it provides an objective, empirical metric to rectify the ungrounded speculation surging through industry, academia, and social media. Drawing from a breadth of philosophical literature, I argue the test captures the essence of self-awareness, rather than some postulated correlate or ancillary quality. To begin, the concept of self-awareness is clearly demarcated from related concepts like consciousness, agency, and free will. Next, I propose a model called the $\\textit{Nesting Doll of Self-Awareness}$ and discuss its relevance for intelligent beings. Then, the test is presented in its full generality, applicable to any machine system. I show how to apply the test to Large Language Models and conduct experiments on popular open and closed source LLMs, obtaining reproducible results that suggest a lack of self-awareness. The implications of machine self-awareness are discussed in relation to questions about meaning and true understanding. Finally, some next steps are outlined for studying self-awareness in machines.\n\nClaim1: My test is simple, and it provides an objective, empirical metric to rectify the ungrounded speculation surging through industry, academia, and social media.\nClaim2: I argue the test captures the essence of self-awareness, rather than some postulated correlate or ancillary quality.\nClaim3: The concept of self-awareness is clearly demarcated from related concepts like consciousness, agency, and free will.\nClaim4: I propose a model called the Nesting Doll of Self-Awareness and discuss its relevance for intelligent beings.\nClaim5: The test is presented in its full generality, applicable to any machine system.\nClaim6: I show how to apply the test to Large Language Models and conduct experiments on popular open and closed source LLMs, obtaining reproducible results that suggest a lack of self-awareness.\nClaim7: The result is that popular Tweets and news media dominate the conversation, while authorities in the field either say nothing or win the spotlight with bold, confident assertions based on implicit, controversial assumptions or their intuition about a model\u2019s architecture.\nClaim8: I argue this question is at least worthy of real scientific investigation.\nClaim9: This test is interesting in its own right, but I will argue it does not demonstrate any sort of self-awareness in the manner it is formulated.\nClaim10: Before presenting the test, we must clearly demarcate the concept of self-awareness.\nClaim11: In this paper, I propose a test for machine self-awareness which is similar in style to the Turing test.\nClaim12: Moreover, I argue it truly captures the essence of self-awareness, rather than some postulated correlate or ancillary quality.\nClaim13: I argue that the understanding of self-awareness developed here is consistent with the philosophical work outlined in section 1.1.2, along with most (if not all) popular conceptions.\nClaim14: In all cases, the LLM was not able to reliably detect which speaker it acted as.\nClaim15: This finding indicates that LLMs are not able to distinguish their own words from those of another, and thus serves as evidence that LLMs are not self-aware, by the test I propose.\nClaim16: I proposed a Turing-style test for self-awareness, applicable to any machine or system, and I conducted this test on two popular LLMs.\nClaim17: The experimental results suggest that these LLM systems are not self-aware.\nClaim18: I discussed the implications and importance of self-awareness for AI systems and mentioned some future work that lies ahead.\nClaim19: While these tests are simple and preliminary, the results are extremely easy to reproduce or expand upon in as little as 10 minutes with access to an API or any open-source LLM.\nClaim20: Indeed, the core utility of this test is that, just like the Turing Test, useful insights and answers can be obtained from strikingly simple and easy-to-setup conversations.\nClaim21: Justification: It is repeatedly stated throughout the paper (though not in its own section) that the test I propose is simple and rudimentary\u2014similar to the original Turing Test. The test takes one broad stroke over each level of the Nesting Doll, rather than being detailed and comprehensive, which is one limitation. Additionally, the experimental tests performed are preliminary, and only for two popular language models, leaving the road open for experiments on many other AI systems. In section 6, I discuss some limitations in the context of future work.\nClaim22: Justification: Complete code is provided with the paper to reproduce each experiment on Llama. Additionally, instructions are given for how one would reproduce tests on GPT-3.5 using the OpenAI completions playground. Indications are given as to how any other Large Language Model could be tested.\nClaim23: Justification: All code is provided, along with instructions to reproduce the main experimental results.\nClaim24: Justification: There are few training and test details necessary, due to the nature of the experiments. However, everything required to understand the results is outlined in section 3.3.\nClaim25: Justification: The main utility of the test I present is that useful insights and answers about the abilities of AI systems can be obtained without requiring a detailed statistical analysis. Just like the Turing Test, this paper attempts to outline a quick and dirty metric that can be applied as a yardstick of AI progress on the question of self-awareness. To this extent, a detailed analysis of the statistical significance of results would miss the point of having a simple test\u2014and introduce artificial barriers to reproducing experiments. In addition, each test requires a conversation with a human participant, making the test difficult to scale without more laborious experimental efforts.\nClaim26: Justification: All experiments presented can easily be reproduced on a personal laptop.\nClaim27: Justification: There are no violations of the NeurIPS Code of Ethics that the I aware of.\nClaim28: Justification: The broadest impact of the work that I could foresee is described in 1.1. In particular, this work lays down a guide with which AI systems may be measured and understood. As such, it may impact the conversation on AI in ways that reduce speculation, hysteria, fear. I can think of no other major societal impacts.\n\nReview: Weakness: - The paper is drawn out in terms of substance. The idea of the test is only introduced on page 4.\n- The writing is, at times, verbose and unnecessarily complex.\n- The obvious question would be whether humans would pass such a test. Though this is briefly considered in Section 5.2, I think it is not properly discussed.\n\t- The author claims that this is easily solved by humans because of memory. This is not a fair comparison to LLMs. Goodhart's law would come into play here as we could easily add such a memory structure to an LLM system.\n\t- Would a human be able to resolve extremely generic text that any human wrote?\n- The paper lacks rigorous analysis and testing that I would expect to see in a NeurIPS main track paper.\n\nLine 29: \""Last year, however, the Turing test was broken\"". I am inclined to say the Turing test was passed before and very few considered it to be a reliable indicator, even before GPT.\nLine 32: \""AI has become untethered to any definitive, objective measure or permanent, agreed-upon benchmark.\"" There are certainly many benchmarks and standards in various subfields.\nQuestions: As above, the obvious question would be whether humans would pass such a test. Though this is briefly considered in Section 5.2, I think it is not properly discussed.\n\nAs above: Would a human be able to resolve extremely generic text that any human wrote?\n\n\n\n"", ""id"": ""VD3lb3XWqa1"", ""pdf"": ""openreview.net/pdf?id=VD3lb3XWqa""}","59"
